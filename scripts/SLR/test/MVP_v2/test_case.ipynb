{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, for_git=True):\n",
    "        self.gap_too_large_threshold = 1000\n",
    "        self.savetime_on_fulltext = False   # If True, operations on fulltext will be kept to a minimum\n",
    "\n",
    "    def get_output_path(self):\n",
    "        return ''\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "def time_function(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        appendix = \"\"\n",
    "        # if instances in args:\n",
    "        if \"instances\" in kwargs:\n",
    "            # append len of instances\n",
    "            appendix = f\"({len(kwargs['instances'])} instances\"\n",
    "        if \"papers\" in kwargs:\n",
    "            if appendix:\n",
    "                appendix += \", \"\n",
    "            appendix += f\"{len(kwargs['papers'])} papers\"\n",
    "        if appendix:\n",
    "            appendix += \")\"\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{func.__name__} executed in {end_time - start_time} seconds\" + appendix)\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(input_string, delimiters = [\" \", \"-\", \"_\"]):\n",
    "    for delimiter in delimiters:\n",
    "        input_string = \" \".join(input_string.split(delimiter))\n",
    "    return input_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: find occurrences of instances in full text of papers\n",
    "import sys\n",
    "from bisect import bisect_left\n",
    "from sortedcontainers import SortedSet\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "class PosInPaper:\n",
    "    def __init__(self):\n",
    "        # List of paper identifiers\n",
    "        self.papers = []\n",
    "        # List of literals\n",
    "        self.literals = []\n",
    "        # Dict of unique words across all literals\n",
    "        self.words = {}\n",
    "        self.word_len = []\n",
    "        # List of unique combinations of words across all literals\n",
    "        self.word_combinations = {}\n",
    "        self.word_combination_lists = []\n",
    "        # 2D list mapping pairs of literals to their word combination index\n",
    "        self.word_combination_index_literal_literal = []\n",
    "        # 2D list of SortedSets, each containing the positions of a word in a paper\n",
    "        self.word_occurrences_in_papers = []\n",
    "        # 3D list containing the minimum distances between word combinations in each paper\n",
    "        self.min_distances = []\n",
    "\n",
    "    @time_function\n",
    "    def populate(self, config: Config, papers: list, literals: list[str], paper_full_text, optimize=True):\n",
    "        \"\"\"\n",
    "        Populates the internal data structures with occurrences and distances of literals in papers.\n",
    "\n",
    "        Parameters:\n",
    "        - config (Config): Configuration object containing settings.\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - literals (list[str]): List of literals to process.\n",
    "        - paper_full_text (dict): Mapping from paper identifiers to their full text file paths.\n",
    "        - optimize (bool): Flag to optimize data structures after population.\n",
    "        \"\"\"\n",
    "        self.initialize_variables(papers, literals)\n",
    "        self.process_literals(literals)\n",
    "        self.process_literal_combinations(literals)\n",
    "        self.setup_data_structures(papers)\n",
    "        self.find_occurrences_in_texts(papers, paper_full_text)\n",
    "        if optimize:\n",
    "            self.optimize_data()\n",
    "\n",
    "    @time_function\n",
    "    def initialize_variables(self, papers, literals):\n",
    "        \"\"\"\n",
    "        Initializes basic variables for the class instance.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        self.papers = papers\n",
    "        self.literals = literals\n",
    "        self.word_combination_index_literal = {lit: None for lit in literals}\n",
    "\n",
    "    @time_function\n",
    "    def process_literals(self, literals):\n",
    "        \"\"\"\n",
    "        Processes each literal to extract and store unique words and word combinations.\n",
    "\n",
    "        Parameters:\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        for lit in literals:\n",
    "            word_list = split_string(lit)\n",
    "            self.add_words(word_list)\n",
    "            self.add_if_word_combination(word_list, lit)\n",
    "\n",
    "    def add_words(self, word_list):\n",
    "        \"\"\"\n",
    "        Adds unique words from a list to the internal list of words.\n",
    "\n",
    "        Parameters:\n",
    "        - word_list (list): List of words to add.\n",
    "        \"\"\"\n",
    "        for word in word_list:\n",
    "            if word not in self.words:\n",
    "                self.words[word] = len(self.words)\n",
    "                self.word_len.append(len(word))\n",
    "\n",
    "    def add_if_word_combination(self, word_list, lit):\n",
    "        \"\"\"\n",
    "        Adds a unique combination of words from a list to the internal list of word combinations.\n",
    "\n",
    "        Parameters:\n",
    "        - word_list (list): List of words forming a combination.\n",
    "        - lit (str): The literal corresponding to the word combination.\n",
    "        \"\"\"\n",
    "        if len(word_list) > 1:\n",
    "            pos = self.word_combination_index_literal.get(lit, -1)\n",
    "            if pos == -1 or pos == None:\n",
    "                froz = frozenset(word_list)\n",
    "                pos = len(self.word_combinations)\n",
    "                self.add_word_combination(froz, pos)\n",
    "            self.word_combination_index_literal[lit] = pos\n",
    "    \n",
    "    def add_word_combination(self, froz, pos):\n",
    "        self.word_combinations[froz] = pos\n",
    "        self.word_combination_lists.append([self.words[word] for word in sorted(froz, key=len, reverse=True)])\n",
    "\n",
    "    @time_function\n",
    "    def process_literal_combinations(self, literals):\n",
    "        \"\"\"\n",
    "        Processes combinations of literals to store their indices in the internal data structure.\n",
    "\n",
    "        Parameters:\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        self.word_combination_index_literal_literal = [[None] * len(literals) for _ in range(len(literals))]\n",
    "        # Use a dictionary for quick lookup and storage\n",
    "        combination_index = len(self.word_combinations)\n",
    "\n",
    "        for id1, literal1 in enumerate(literals):\n",
    "            for id2 in range(id1 + 1, len(literals)):\n",
    "                literal2 = literals[id2]\n",
    "                # Use a sorted tuple for consistent ordering\n",
    "                froz = frozenset(split_string(literal1) + split_string(literal2))\n",
    "                # Check if the combination is already in the dictionary\n",
    "                pos = self.word_combinations.get(froz, -1)\n",
    "                if pos == -1:\n",
    "                    pos = combination_index\n",
    "                    combination_index += 1\n",
    "\n",
    "                    self.add_word_combination(froz, pos)\n",
    "\n",
    "                # Update the matrix with the index of the combination\n",
    "                self.word_combination_index_literal_literal[id1][id2] = pos\n",
    "                self.word_combination_index_literal_literal[id2][id1] = pos\n",
    "\n",
    "    @time_function\n",
    "    def setup_data_structures(self, papers):\n",
    "        \"\"\"\n",
    "        Initializes the data structures for storing word occurrences and minimum distances.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        \"\"\"\n",
    "        self.word_occurrences_in_papers = [[SortedSet() for _ in self.words] for _ in papers]\n",
    "        self.min_distances = np.full((len(papers), len(self.word_combinations)), -2, dtype=int)\n",
    "\n",
    "    @time_function\n",
    "    def find_occurrences_in_texts(self, papers, paper_full_text):\n",
    "        \"\"\"\n",
    "        Finds and stores the occurrences of each word in the full text of each paper.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - paper_full_text (dict): Mapping from paper identifiers to their full text file paths.\n",
    "        \"\"\"\n",
    "        for paperID, paper in enumerate(papers):\n",
    "            if paper in paper_full_text:\n",
    "                with open(paper_full_text[paper], 'r', encoding=\"utf8\") as f:\n",
    "                    text = f.read().lower()\n",
    "                    for wordID, word in enumerate(self.words):\n",
    "                        self.find_and_add_word_occurrences(paperID, wordID, word, text)\n",
    "            else:\n",
    "                print(f\"Paper {paper} has no full text available.\")\n",
    "\n",
    "    def find_and_add_word_occurrences(self, paperID, wordID, word, text):\n",
    "        \"\"\"\n",
    "        Finds and adds the occurrences of a word in a paper's text to the internal data structure.\n",
    "\n",
    "        Parameters:\n",
    "        - paperID (int): The index of the paper in the internal list.\n",
    "        - wordID (int): The index of the word in the internal list.\n",
    "        - word (str): The word to find occurrences of.\n",
    "        - text (str): The full text of the paper.\n",
    "        \"\"\"\n",
    "        pos = text.find(word)\n",
    "        while pos != -1:\n",
    "            self.word_occurrences_in_papers[paperID][wordID].add(pos)\n",
    "            pos = text.find(word, pos + 1)\n",
    "\n",
    "    @time_function\n",
    "    def optimize_data(self):\n",
    "        \"\"\"\n",
    "        Optimizes the internal data structures for faster access and smaller memory footprint.\n",
    "        \"\"\"\n",
    "        # self.word_combination_index_literal_literal = np.array(self.word_combination_index_literal_literal, dtype=int)\n",
    "        for paperID in range(len(self.papers)):\n",
    "            for wordID in range(len(self.words)):\n",
    "                # self.word_occurrences_in_papers[paperID][wordID] = SortedSet(self.word_occurrences_in_papers[paperID][wordID])\n",
    "                self.word_occurrences_in_papers[paperID][wordID] = [(x, wordID) for x in self.word_occurrences_in_papers[paperID][wordID]]\n",
    "\n",
    "    @time_function\n",
    "    def save_to_file(self, config, path=None, name = \"pos_in_paper\", check_size=False, min_distances=False):\n",
    "        \"\"\"\n",
    "        Saves the internal data structures to files for persistence.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str, optional): The base path for the output files. Defaults to \"pos_in_paper\".\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = config.get_output_path()\n",
    "        filepath = os.path.join(path, name + '.json')\n",
    "        \n",
    "        data = {\n",
    "            \"papers\": self.papers,\n",
    "            \"literals\": self.literals,\n",
    "            \"words\": self.words,\n",
    "            \"word_len\": self.word_len,\n",
    "            \"word_combinations\": {\"_\".join(key): value for key, value in self.word_combinations.items()},\n",
    "            \"word_combination_lists\": self.word_combination_lists,\n",
    "            \"word_combination_index_literal_literal\": self.word_combination_index_literal_literal,\n",
    "            # Convert SortedSets to lists for JSON serialization\n",
    "            \"word_occurrences_in_papers\": [[list(occurrences) for occurrences in paper] for paper in self.word_occurrences_in_papers],\n",
    "        }\n",
    "        if min_distances:\n",
    "            data[\"min_distances\"] = self.min_distances.tolist()\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "        if check_size:\n",
    "            for key, value in data.items():\n",
    "                # Construct the file name for each sub-dictionary\n",
    "                filepath = os.path.join(path, f\"{name}_{key}.json\")\n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(value, f, ensure_ascii=False)\n",
    "\n",
    "    @time_function\n",
    "    def load_from_file(self, config, path=None, name=\"pos_in_paper\"):\n",
    "        \"\"\"\n",
    "        Loads the internal data structures from files.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str, optional): The base path for the input files. Defaults to \"pos_in_paper\".\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = config.get_output_path()\n",
    "        filepath = os.path.join(path, name + '.json')\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.papers = data[\"papers\"]\n",
    "        self.literals = data[\"literals\"]\n",
    "        self.words = data[\"words\"]\n",
    "        self.word_len = data[\"word_len\"]\n",
    "        self.word_combinations = {frozenset(split_string(key)): i for i, key in enumerate(data[\"word_combinations\"])}\n",
    "        self.word_combination_lists = data[\"word_combination_lists\"]\n",
    "        self.word_combination_index_literal_literal = data[\"word_combination_index_literal_literal\"]\n",
    "        if \"min_distances\" in data:\n",
    "            self.min_distances = np.array(data[\"min_distances\"])\n",
    "        else:\n",
    "            self.setup_data_structures(self.papers)\n",
    "        self.word_occurrences_in_papers = data[\"word_occurrences_in_papers\"]\n",
    "        \n",
    "    # def set_min_distance(self, paper, literals, distance):\n",
    "    def set_min_distance(self, paperID, word_combination_id, distance):\n",
    "        \"\"\"\n",
    "        Sets the minimum distance between occurrences of literals in a paper.\n",
    "\n",
    "        Parameters:\n",
    "        - paper (str): The identifier for the paper.\n",
    "        - literals (list): A list of literals for which the distance is calculated.\n",
    "        - distance (int): The calculated minimum distance.\n",
    "        \"\"\"\n",
    "        # key = frozenset(literals)\n",
    "        # if paper not in self.min_distances:\n",
    "        #     self.min_distances[paper] = {}\n",
    "        self.min_distances[paperID][word_combination_id] = distance\n",
    "\n",
    "    @time_function\n",
    "    def calculate_all_possible(self, start_at=0, stop_at=None):\n",
    "        \"\"\"\n",
    "        Calculates the minimum distances between all possible combinations of literals in all papers.\n",
    "        \"\"\"\n",
    "        for p in range(len(self.papers)):\n",
    "            if stop_at is not None and p >= stop_at:\n",
    "                break\n",
    "            if p < start_at:\n",
    "                continue\n",
    "            for i in range(len(self.literals)):\n",
    "                for j in range(i + 1, len(self.literals)):\n",
    "                    # get word_combination_index_literal_literal\n",
    "                    self.find_min_distance_by_id(p, self.word_combination_index_literal_literal[i][j])\n",
    "\n",
    "    def find_min_distance_by_id(self, paperID, wcID, allow_call=True):\n",
    "        \"\"\"\n",
    "        Finds the minimum distance between occurrences of literals in a paper.\n",
    "\n",
    "        Parameters:\n",
    "        - paper (str): The identifier for the paper.\n",
    "        - literals (list): A list of literals for which the distance is to be found.\n",
    "        - allow_call (bool): Flag to allow recursive call to get_min_distance.\n",
    "\n",
    "        Returns:\n",
    "        - int: The minimum distance between occurrences of the literals.\n",
    "        \"\"\"\n",
    "        distance = self.min_distances[paperID][wcID]\n",
    "\n",
    "        if distance == -1:\n",
    "            # word combination not found in paper\n",
    "            return -1\n",
    "        if distance == -2:\n",
    "            # calculate distance\n",
    "            pass\n",
    "        else:\n",
    "            return distance\n",
    "        \n",
    "        list_ids = self.word_combination_lists[wcID]\n",
    "        list_ids_map = {list_ids[i]: i for i in range(len(list_ids))}\n",
    "        lit_len = [self.word_len[i] for i in list_ids]\n",
    "        # literals = [list(self.words)[i] for i in list_ids]\n",
    "        \n",
    "        for i in list_ids:\n",
    "            if not self.word_occurrences_in_papers[paperID][i]:\n",
    "                self.set_min_distance(paperID, wcID, -1)\n",
    "                return -1\n",
    "        # Outsourced to optimize\n",
    "        # inputs = [[(x, i) for x in self.word_occurrences_in_papers[paperID][wordID]] for i, wordID in enumerate(list_ids)]\n",
    "        inputs = [self.word_occurrences_in_papers[paperID][wordID] for wordID in list_ids]\n",
    "\n",
    "        indices = [lst[0][0] for lst in inputs]\n",
    "        best = float('inf')\n",
    "        self.set_min_distance(paperID, wcID, 15)\n",
    "\n",
    "        for item in sorted(sum(inputs, [])):\n",
    "            indices[list_ids_map[item[1]]] = item[0]\n",
    "            arr_min = min(indices)\n",
    "            best = min(max(indices) - arr_min - lit_len[indices.index(arr_min)], best)\n",
    "        self.set_min_distance(paperID, wcID, best)\n",
    "        return best\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewPosInPaper(PosInPaper):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Initialize any additional attributes for EnhancedPosInPaper here\n",
    "\n",
    "    # # Example of redefining a specific function from PosInPaper\n",
    "    @time_function\n",
    "    def find_min_distance_by_id(self, paperID, wcID, allow_call=True):\n",
    "        raise NotImplementedError(\"This function has not been implemented yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_in_paper = PosInPaper()\n",
    "pos_in_paper.load_from_file(config)\n",
    "\n",
    "instances = []\n",
    "with open(\"instances.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    instances = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "start_at=0\n",
    "stop_at=10\n",
    "stop_at=None\n",
    "\n",
    "# instances = [\n",
    "#     'knowledge based engineering',\n",
    "#     'engine analysis'\n",
    "# ]\n",
    "\n",
    "print(\"Now testing new vs. old\")\n",
    "\n",
    "old_version = PosInPaper()\n",
    "old_version.load_from_file(config)\n",
    "\n",
    "new_version = NewPosInPaper()\n",
    "new_version.load_from_file(config)\n",
    "\n",
    "old_version.calculate_all_possible(start_at=start_at, stop_at=stop_at)\n",
    "new_version.calculate_all_possible(start_at=start_at, stop_at=stop_at)\n",
    "error_count = 0\n",
    "for paperID in range(len(pos_in_paper.papers)):\n",
    "    for wcID in range(len(pos_in_paper.word_combinations)):\n",
    "        if old_version.min_distances[paperID][wcID] != new_version.min_distances[paperID][wcID]:\n",
    "            error_count += 1\n",
    "            if error_count < 5:\n",
    "                print(f\"Paper {paperID}, word combination {wcID}: {old_version.min_distances[paperID][wcID]} vs. {new_version.min_distances[paperID][wcID]}\")\n",
    "if error_count:\n",
    "    raise Exception(f\"{error_count} errors found!\")\n",
    "else:\n",
    "    print(\"No errors found!\")\n",
    "    \n",
    "print(\"Now testing only old\")\n",
    "old_version = PosInPaper()\n",
    "old_version.load_from_file(config)\n",
    "old_version.calculate_all_possible(start_at=start_at, stop_at=stop_at)\n",
    "\n",
    "print(\"Now testing the new version\")\n",
    "new_version = NewPosInPaper()\n",
    "new_version.load_from_file(config)\n",
    "new_version.calculate_all_possible(start_at=start_at, stop_at=stop_at)\n",
    "\n",
    "# Debug barrier\n",
    "raise Exception(\"This is the end of the script. The following code is not executed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
