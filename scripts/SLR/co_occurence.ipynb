{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Instance occurrence in Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnw_tools.SLR.config import Config\n",
    "\n",
    "config = {\n",
    "    \"for_git\": True,\n",
    "    \"visualize\": False,\n",
    "    \"csv_separator\": \",\",\n",
    "    \"csv_decimal\": \".\",\n",
    "    ## Should only accepted papers be used for the analysis?\n",
    "    \"only_included_papers\": True,\n",
    "    ## Which instance columns actually indicate properties?\n",
    "    \"properties\": [\"source\"],\n",
    "    \"proximity_mode\": \"sqrt\",\n",
    "    ## Paths\n",
    "    \"base_path\": \"data/\",\n",
    "    \"subset_path\": \"data_subset/\",\n",
    "    \"visualization_path\": \"visualization/\",\n",
    "    \"ontology_path\": \"ontology/\",\n",
    "    \"orkg_path\": \"ORKG/\",\n",
    "    \"folder_path\": \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR\",\n",
    "    \"papers_path\": \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/02_nlp\",\n",
    "    \"review_path\": \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/03_notes\",\n",
    "    \"csv_file\": \"C:/workspace/borgnetzwerk/tools/scripts/SLR/data.csv\",\n",
    "    \"obsidian_path\": \"ontology/obsidian/\",\n",
    "    ## Position in Paper settings\n",
    "    \"gap_too_large_threshold\": 1000,\n",
    "    \"savetime_on_fulltext\": False,\n",
    "    \"try_to_save_time\": False,\n",
    "    \"recalculate_pos_in_paper\": False,\n",
    "    \"debug\": True,\n",
    "    ## Wikidata settings\n",
    "    \"wikidata_query_limit\": 20,\n",
    "    ## Graph settings\n",
    "    \"proximity_seed\": 17,\n",
    "    \"proximity_k_spring\": 18,\n",
    "    \"proximity_min_value\": 0.1,\n",
    "}\n",
    "\n",
    "\n",
    "config = Config(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnw_tools.SLR.builder import *\n",
    "\n",
    "director = PaperInstanceDirector(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column data validation is empty\n",
      "column inference is empty\n"
     ]
    }
   ],
   "source": [
    "# director.get_instances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no new Zotero export at G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR:\n",
      "There should be a folder called 'files'\n",
      "We now have 1035 PDFs stored at G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR\\00_PDFs\n",
      "1024 out of 1028 paper instances have metadata.\n",
      "get_metadata executed in 21.115363121032715 seconds\n"
     ]
    }
   ],
   "source": [
    "director.get_papers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce to Reviewed papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "director.reduce_to_reviewed_papers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status: Instances and Papers setup\n",
    "\n",
    "## Next: Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bnw_tools.SLR.wikidata import *\n",
    "\n",
    "# wikidata = WikiData(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Obsidian folder:\n",
      "number of tempaltes: 2 (Class, Instance)\n",
      "number of classes: 5\n",
      "number of instances: 570\n",
      "Instance .p21 will be newly added to instances.\n",
      "Instance 2d drawings will be newly added to instances.\n",
      "Instance 2d engineering drawings will be newly added to instances.\n",
      "Instance 3d models will be newly added to instances.\n",
      "Instance 3d pdf will be newly added to instances.\n",
      "Instance 3d solid model will be newly added to instances.\n",
      "Instance aaa will be newly added to instances.\n",
      "Instance access product design data will be newly added to instances.\n",
      "Instance access to knowledge and expertise will be newly added to instances.\n",
      "Instance access will be newly added to instances.\n",
      "Instance accessory manufacture will be newly added to instances.\n",
      "Instance aceess tooling cad models will be newly added to instances.\n",
      "Instance acis will be newly added to instances.\n",
      "Instance acquisition will be newly added to instances.\n",
      "Instance activity profiling will be newly added to instances.\n",
      "Instance ad-drw will be newly added to instances.\n",
      "Instance adaptive neuro-fuzzy inference system (anfis) will be newly added to instances.\n",
      "Instance add will be newly added to instances.\n",
      "Instance addam will be newly added to instances.\n",
      "Instance ad_asm will be newly added to instances.\n",
      "Instance ad_prt will be newly added to instances.\n",
      "Instance aerospace ontology will be newly added to instances.\n",
      "Instance agile development framework (adf) will be newly added to instances.\n",
      "Instance airfraem manufacturing will be newly added to instances.\n",
      "Instance airspace system ontology will be newly added to instances.\n",
      "Instance airworthiness test will be newly added to instances.\n",
      "Instance alm will be newly added to instances.\n",
      "Instance altova’s umodel will be newly added to instances.\n",
      "Instance analysing information content (data mining techniques) will be newly added to instances.\n",
      "Instance analysis will be newly added to instances.\n",
      "Instance analyzing will be newly added to instances.\n",
      "Instance animation will be newly added to instances.\n",
      "Instance animations will be newly added to instances.\n",
      "Instance annotating the semantic contents of a document will be newly added to instances.\n",
      "Instance api will be newly added to instances.\n",
      "Instance architecture design space graph (adsg) will be newly added to instances.\n",
      "Instance archive will be newly added to instances.\n",
      "Instance assemblies will be newly added to instances.\n",
      "Instance audio will be newly added to instances.\n",
      "Instance authoring will be newly added to instances.\n",
      "Instance automated reasoning will be newly added to instances.\n",
      "Instance avionics analytics ontology (aao)  will be newly added to instances.\n",
      "Instance babelnet will be newly added to instances.\n",
      "Instance bfo will be newly added to instances.\n",
      "Instance bom will be newly added to instances.\n",
      "Instance bond graphs will be newly added to instances.\n",
      "Instance brics technology will be newly added to instances.\n",
      "Instance brics will be newly added to instances.\n",
      "Instance business logic will be newly added to instances.\n",
      "Instance business objects & processes will be newly added to instances.\n",
      "Instance business process integration will be newly added to instances.\n",
      "Instance c application will be newly added to instances.\n",
      "Instance c will be newly added to instances.\n",
      "Instance cad programming interface will be newly added to instances.\n",
      "Instance cad representation vdsm will be newly added to instances.\n",
      "Instance cad representation will be newly added to instances.\n",
      "Instance cad will be newly added to instances.\n",
      "Instance cad2fem will be newly added to instances.\n",
      "Instance cadds5(cv) will be newly added to instances.\n",
      "Instance calculating will be newly added to instances.\n",
      "Instance calculation report will be newly added to instances.\n",
      "Instance cameo system modeler (csm) will be newly added to instances.\n",
      "Instance capp will be newly added to instances.\n",
      "Instance capture and reuse engineering experience will be newly added to instances.\n",
      "Instance catia will be newly added to instances.\n",
      "Instance caturing contextual information form information users (profiling techniques) will be newly added to instances.\n",
      "Instance cco will be newly added to instances.\n",
      "Instance ceasiom will be newly added to instances.\n",
      "Instance certification will be newly added to instances.\n",
      "Instance cfd code will be newly added to instances.\n",
      "Instance cfd simulation data will be newly added to instances.\n",
      "Instance  cfd simulations of the european hypersonic database will be newly added to instances.\n",
      "Instance cfview will be newly added to instances.\n",
      "Instance check will be newly added to instances.\n",
      "Instance classification and terminology development will be newly added to instances.\n",
      "Instance cloud visualization will be newly added to instances.\n",
      "Instance cmdows will be newly added to instances.\n",
      "Instance codex will be newly added to instances.\n",
      "Instance collection and sharing of design information through a single source of truth (ssot) will be newly added to instances.\n",
      "Instance common logic interchange format (clif) will be newly added to instances.\n",
      "Instance common mdao workflow schema (cmdows) will be newly added to instances.\n",
      "Instance common mdo workflow schema (cmdows) will be newly added to instances.\n",
      "Instance common parametric aircraft configuration schema (cpacs) will be newly added to instances.\n",
      "Instance common parametric aircraft schema (cpacs) will be newly added to instances.\n",
      "Instance computational analysis programming interface (capri) will be newly added to instances.\n",
      "Instance concept design will be newly added to instances.\n",
      "Instance concept, design & development will be newly added to instances.\n",
      "Instance conceptual modelling will be newly added to instances.\n",
      "Instance configuration management will be newly added to instances.\n",
      "Instance context inference will be newly added to instances.\n",
      "Instance control & stability will be newly added to instances.\n",
      "Instance convert file formats will be newly added to instances.\n",
      "Instance coom10 will be newly added to instances.\n",
      "Instance cost analysis tool for manufacturing components  will be newly added to instances.\n",
      "Instance cost analysis tool for manufacturing of aircraft components (catmac) will be newly added to instances.\n",
      "Instance cpacs will be newly added to instances.\n",
      "Instance cpacspy library will be newly added to instances.\n",
      "Instance crawling will be newly added to instances.\n",
      "Instance creation will be newly added to instances.\n",
      "Instance csv will be newly added to instances.\n",
      "Instance customer service will be newly added to instances.\n",
      "Instance cypher will be newly added to instances.\n",
      "Instance d1 will be newly added to instances.\n",
      "Instance d11 will be newly added to instances.\n",
      "Instance daedalus will be newly added to instances.\n",
      "Instance darpa agent markup language (daml) will be newly added to instances.\n",
      "Instance data integration/fusion will be newly added to instances.\n",
      "Instance data mining will be newly added to instances.\n",
      "Instance data processing:analysis will be newly added to instances.\n",
      "Instance data processing:collection will be newly added to instances.\n",
      "Instance data processing:organization will be newly added to instances.\n",
      "Instance data processing:recall will be newly added to instances.\n",
      "Instance data processing:storage will be newly added to instances.\n",
      "Instance data processing:visualization will be newly added to instances.\n",
      "Instance data search, access, and retrieval will be newly added to instances.\n",
      "Instance data storage will be newly added to instances.\n",
      "Instance data validation will be newly added to instances.\n",
      "Instance data visualization will be newly added to instances.\n",
      "Instance database management system servers (dbms) will be newly added to instances.\n",
      "Instance database will be newly added to instances.\n",
      "Instance davetools will be newly added to instances.\n",
      "Instance decision support will be newly added to instances.\n",
      "Instance definition of the optimization problem will be newly added to instances.\n",
      "Instance deliver first aircraft will be newly added to instances.\n",
      "Instance description of object oriented software will be newly added to instances.\n",
      "Instance description will be newly added to instances.\n",
      "Instance descriptive information generation will be newly added to instances.\n",
      "Instance design and development will be newly added to instances.\n",
      "Instance design blueprint will be newly added to instances.\n",
      "Instance design concepts formalization will be newly added to instances.\n",
      "Instance design parts will be newly added to instances.\n",
      "Instance design space visualization will be newly added to instances.\n",
      "Instance design will be newly added to instances.\n",
      "Instance detailed design will be newly added to instances.\n",
      "Instance detailed will be newly added to instances.\n",
      "Instance development safeguard actualization will be newly added to instances.\n",
      "Instance development will be newly added to instances.\n",
      "Instance difficulty will be newly added to instances.\n",
      "Instance digital mockup will be newly added to instances.\n",
      "Instance direct api will be newly added to instances.\n",
      "Instance disciplinary analysis will be newly added to instances.\n",
      "Instance discussion forums, knowledge directories will be newly added to instances.\n",
      "Instance dissemination will be newly added to instances.\n",
      "Instance document type definition (dtd will be newly added to instances.\n",
      "Instance documents will be newly added to instances.\n",
      "Instance drawing exchange format (dxf) will be newly added to instances.\n",
      "Instance dtad will be newly added to instances.\n",
      "Instance dwf will be newly added to instances.\n",
      "Instance dwg will be newly added to instances.\n",
      "Instance dxf will be newly added to instances.\n",
      "Instance eclipse plugin will be newly added to instances.\n",
      "Instance eclipse will be newly added to instances.\n",
      "Instance electronic bulletin boards will be newly added to instances.\n",
      "Instance eligibility of model examiniation will be newly added to instances.\n",
      "Instance engine accessory purchase will be newly added to instances.\n",
      "Instance engine analysis will be newly added to instances.\n",
      "Instance  engineering java programming will be newly added to instances.\n",
      "Instance enterprise resource planning will be newly added to instances.\n",
      "Instance establish links from architectural elements to nodes in the data schema will be newly added to instances.\n",
      "Instance evaluation will be newly added to instances.\n",
      "Instance excel will be newly added to instances.\n",
      "Instance exchange of design intents will be newly added to instances.\n",
      "Instance exchange of models will be newly added to instances.\n",
      "Instance expert system will be newly added to instances.\n",
      "Instance explicit geometry will be newly added to instances.\n",
      "Instance export data, translate to one neutral format once, then exchange that will be newly added to instances.\n",
      "Instance express application will be newly added to instances.\n",
      "Instance express compiler in jsdai will be newly added to instances.\n",
      "Instance express ontology will be newly added to instances.\n",
      "Instance express will be newly added to instances.\n",
      "Instance expresso9 will be newly added to instances.\n",
      "Instance extended design structure matrix (xdsm) will be newly added to instances.\n",
      "Instance extensible graphics language (xgl) will be newly added to instances.\n",
      "Instance extensible markup language (xml) will be newly added to instances.\n",
      "Instance extensible stylesheet (xsl) conversion will be newly added to instances.\n",
      "Instance external web page will be newly added to instances.\n",
      "Instance f1 will be newly added to instances.\n",
      "Instance failure management will be newly added to instances.\n",
      "Instance failure will be newly added to instances.\n",
      "Instance fault knowledge graph construction and platform for aircraft phm will be newly added to instances.\n",
      "Instance feature processing will be newly added to instances.\n",
      "Instance file server will be newly added to instances.\n",
      "Instance first flight decision will be newly added to instances.\n",
      "Instance flash will be newly added to instances.\n",
      "Instance flat file will be newly added to instances.\n",
      "Instance flight test report will be newly added to instances.\n",
      "Instance flight test will be newly added to instances.\n",
      "Instance flight-test preparation will be newly added to instances.\n",
      "Instance flops will be newly added to instances.\n",
      "Instance format transforming of semantic information will be newly added to instances.\n",
      "Instance formatted documents will be newly added to instances.\n",
      "Instance forms will be newly added to instances.\n",
      "Instance fortan will be newly added to instances.\n",
      "Instance fortran application will be newly added to instances.\n",
      "Instance freebase will be newly added to instances.\n",
      "Instance fsms will be newly added to instances.\n",
      "Instance general assembly will be newly added to instances.\n",
      "Instance generate fem model will be newly added to instances.\n",
      "Instance geographic independent virtual environment (give) will be newly added to instances.\n",
      "Instance gif will be newly added to instances.\n",
      "Instance git will be newly added to instances.\n",
      "Instance gkn fokker will be newly added to instances.\n",
      "Instance google will be newly added to instances.\n",
      "Instance graph-based support in the design problem formulation will be newly added to instances.\n",
      "Instance ground test will be newly added to instances.\n",
      "Instance handling concepts and relationships between concepts (semantic techniques) will be newly added to instances.\n",
      "Instance hierarchical data format, version 5 (hdf5)  will be newly added to instances.\n",
      "Instance how it works will be newly added to instances.\n",
      "Instance html will be newly added to instances.\n",
      "Instance hyper-pareto frontier visualization will be newly added to instances.\n",
      "Instance i-deas(sdrc) will be newly added to instances.\n",
      "Instance ideas will be newly added to instances.\n",
      "Instance idef1x application will be newly added to instances.\n",
      "Instance idef1x will be newly added to instances.\n",
      "Instance identify different tooling types will be newly added to instances.\n",
      "Instance iges will be newly added to instances.\n",
      "Instance igs will be newly added to instances.\n",
      "Instance ikewiki will be newly added to instances.\n",
      "Instance image will be newly added to instances.\n",
      "Instance imagenet will be newly added to instances.\n",
      "Instance images will be newly added to instances.\n",
      "Instance implicit geometry will be newly added to instances.\n",
      "Instance incorporating visualization into multiobjective optimization tools for mdo problems will be newly added to instances.\n",
      "Instance inference will be newly added to instances.\n",
      "Instance information classification will be newly added to instances.\n",
      "Instance information clustering will be newly added to instances.\n",
      "Instance information push will be newly added to instances.\n",
      "Instance information search will be newly added to instances.\n",
      "Instance information sharing will be newly added to instances.\n",
      "Instance initiate action according to tooling types will be newly added to instances.\n",
      "Instance initiate the product term class will be newly added to instances.\n",
      "Instance integrate the mdo architecture in an executable workflow will be newly added to instances.\n",
      "Instance integration of single-media and cross-media information will be newly added to instances.\n",
      "Instance interest profiling will be newly added to instances.\n",
      "Instance inverted index will be newly added to instances.\n",
      "Instance isight will be newly added to instances.\n",
      "Instance issue tc pc ac certification will be newly added to instances.\n",
      "Instance j2ee application servers will be newly added to instances.\n",
      "Instance jade will be newly added to instances.\n",
      "Instance janus will be newly added to instances.\n",
      "Instance java agent development environment (jade) will be newly added to instances.\n",
      "Instance java application will be newly added to instances.\n",
      "Instance javascript object notation (json) will be newly added to instances.\n",
      "Instance jpeg will be newly added to instances.\n",
      "Instance jpg will be newly added to instances.\n",
      "Instance jpgs will be newly added to instances.\n",
      "Instance jsdai4 will be newly added to instances.\n",
      "Instance json will be newly added to instances.\n",
      "Instance jupyter notebook will be newly added to instances.\n",
      "Instance k-forms will be newly added to instances.\n",
      "Instance k-search will be newly added to instances.\n",
      "Instance k-store will be newly added to instances.\n",
      "Instance kadmos will be newly added to instances.\n",
      "Instance kaon will be newly added to instances.\n",
      "Instance ke-chain will be newly added to instances.\n",
      "Instance km:application will be newly added to instances.\n",
      "Instance km:knowledge creation will be newly added to instances.\n",
      "Instance km:knowledge storage / retrieval will be newly added to instances.\n",
      "Instance km:transfer will be newly added to instances.\n",
      "Instance knowledege repositories, databases will be newly added to instances.\n",
      "Instance knowledge and data sharing will be newly added to instances.\n",
      "Instance knowledge application:data labeling will be newly added to instances.\n",
      "Instance knowledge application:knowledge extraction will be newly added to instances.\n",
      "Instance knowledge application:knowledge visualisation will be newly added to instances.\n",
      "Instance knowledge application:q&a will be newly added to instances.\n",
      "Instance knowledge application:quality assessment will be newly added to instances.\n",
      "Instance knowledge architecture (ka) will be newly added to instances.\n",
      "Instance knowledge base building will be newly added to instances.\n",
      "Instance knowledge base will be newly added to instances.\n",
      "Instance knowledge based engineering will be newly added to instances.\n",
      "Instance knowledge capture will be newly added to instances.\n",
      "Instance knowledge classification will be newly added to instances.\n",
      "Instance knowledge discovery will be newly added to instances.\n",
      "Instance knowledge engineering will be newly added to instances.\n",
      "Instance knowledge exploration will be newly added to instances.\n",
      "Instance knowledge graph (kg) will be newly added to instances.\n",
      "Instance knowledge graph will be newly added to instances.\n",
      "Instance knowledge interaction will be newly added to instances.\n",
      "Instance knowledge interaction:knowledge visualisation will be newly added to instances.\n",
      "Instance knowledge interaction:q&a will be newly added to instances.\n",
      "Instance knowledge interchange format (kif) will be newly added to instances.\n",
      "Instance knowledge interfacing:classification of question intent will be newly added to instances.\n",
      "Instance knowledge interfacing:data retrieval will be newly added to instances.\n",
      "Instance knowledge interfacing:data review will be newly added to instances.\n",
      "Instance knowledge interfacing:knowledge extraction will be newly added to instances.\n",
      "Instance knowledge interfacing:model training will be newly added to instances.\n",
      "Instance knowledge interfacing:problematic entity extraction will be newly added to instances.\n",
      "Instance knowledge interfacing:q&a will be newly added to instances.\n",
      "Instance knowledge interfacing:question answer search will be newly added to instances.\n",
      "Instance knowledge management (km) will be newly added to instances.\n",
      "Instance knowledge management will be newly added to instances.\n",
      "Instance knowledge mangement tool will be newly added to instances.\n",
      "Instance knowledge optimized manufacture and design (knomad) will be newly added to instances.\n",
      "Instance knowledge representation and reasoning will be newly added to instances.\n",
      "Instance knowledge storage will be newly added to instances.\n",
      "Instance knowledge systematisation will be newly added to instances.\n",
      "Instance knowledge, skill and background profiling will be newly added to instances.\n",
      "Instance knowlege graph building:data labeling will be newly added to instances.\n",
      "Instance knowlege graph building:data management will be newly added to instances.\n",
      "Instance knowlege graph building:document classification will be newly added to instances.\n",
      "Instance knowlege graph building:file importing will be newly added to instances.\n",
      "Instance knowlege graph building:knowledge audit will be newly added to instances.\n",
      "Instance knowlege graph building:knowledge extraction will be newly added to instances.\n",
      "Instance knowlege graph building:text classification will be newly added to instances.\n",
      "Instance knowlege graph construction will be newly added to instances.\n",
      "Instance knowlege graph construction:data labeling will be newly added to instances.\n",
      "Instance knowlege graph construction:data management will be newly added to instances.\n",
      "Instance knowlege graph construction:data review will be newly added to instances.\n",
      "Instance knowlege graph construction:knowledge extraction will be newly added to instances.\n",
      "Instance knowlege graph construction:text classification will be newly added to instances.\n",
      "Instance labeled property graph database (lpg) will be newly added to instances.\n",
      "Instance learning of ontologies from textual data  will be newly added to instances.\n",
      "Instance learning tools will be newly added to instances.\n",
      "Instance lexical data will be newly added to instances.\n",
      "Instance libxml240 will be newly added to instances.\n",
      "Instance life cycle simulation will be newly added to instances.\n",
      "Instance link analysis will be newly added to instances.\n",
      "Instance live video will be newly added to instances.\n",
      "Instance lotus 123 will be newly added to instances.\n",
      "Instance lpg was a cypher-neo4j will be newly added to instances.\n",
      "Instance machine-to-machine communication will be newly added to instances.\n",
      "Instance managing project files will be newly added to instances.\n",
      "Instance manufacture will be newly added to instances.\n",
      "Instance manufacturing method will be newly added to instances.\n",
      "Instance manufacturing preparation will be newly added to instances.\n",
      "Instance manufacturing will be newly added to instances.\n",
      "Instance mapping between disparate data sets will be newly added to instances.\n",
      "Instance material will be newly added to instances.\n",
      "Instance matlab will be newly added to instances.\n",
      "Instance mdao will be newly added to instances.\n",
      "Instance mdax will be newly added to instances.\n",
      "Instance mdo fad (framework for analysis and design) will be newly added to instances.\n",
      "Instance mdo process formulation, visualizations will be newly added to instances.\n",
      "Instance mdo process formulation will be newly added to instances.\n",
      "Instance measurement data will be newly added to instances.\n",
      "Instance metadata will be newly added to instances.\n",
      "Instance mind map will be newly added to instances.\n",
      "Instance mission analysis will be newly added to instances.\n",
      "Instance model based systems engineering (mbse) will be newly added to instances.\n",
      "Instance Model-based systems engineering (MBSE) will be newly added to instances.\n",
      "Instance modelcenter will be newly added to instances.\n",
      "Instance mopcssoviz will be newly added to instances.\n",
      "Instance mov will be newly added to instances.\n",
      "Instance mp3 will be newly added to instances.\n",
      "Instance mpeg will be newly added to instances.\n",
      "Instance ms excel will be newly added to instances.\n",
      "Instance ms word will be newly added to instances.\n",
      "Instance multi-agent collaborative system will be newly added to instances.\n",
      "Instance multi-agent modeling language (maml) will be newly added to instances.\n",
      "Instance multi-disciplinary analysis and optimization (mdao) will be newly added to instances.\n",
      "Instance multidisciplinary modeler (mdm)  will be newly added to instances.\n",
      "Instance multidisciplinary modeling will be newly added to instances.\n",
      "Instance multilinq will be newly added to instances.\n",
      "Instance multimedia document processing will be newly added to instances.\n",
      "Instance multiwordnet will be newly added to instances.\n",
      "Instance name will be newly added to instances.\n",
      "Instance nasa ames function table processor scripts will be newly added to instances.\n",
      "Instance nasa semantic web for earth and environmental terminology (sweet) will be newly added to instances.\n",
      "Instance nasa taxonomy will be newly added to instances.\n",
      "Instance nation decision will be newly added to instances.\n",
      "Instance national airspace system (nas) will be newly added to instances.\n",
      "Instance natural language information analysis method (niam) application will be newly added to instances.\n",
      "Instance navigation will be newly added to instances.\n",
      "Instance neo4j will be newly added to instances.\n",
      "Instance networkx will be newly added to instances.\n",
      "Instance nextgen avionics systems will be newly added to instances.\n",
      "Instance nosql will be newly added to instances.\n",
      "Instance numerical control markup language (ncml) will be newly added to instances.\n",
      "Instance nx4 will be newly added to instances.\n",
      "Instance object definitions will be newly added to instances.\n",
      "Instance object role model (orm) will be newly added to instances.\n",
      "Instance on-board system will be newly added to instances.\n",
      "Instance on-board systems will be newly added to instances.\n",
      "Instance ontological engineering will be newly added to instances.\n",
      "Instance ontologies for nextgen avionics systems (onas)  will be newly added to instances.\n",
      "Instance ontology modeling will be newly added to instances.\n",
      "Instance ontology-driven interactive search environment for earth sciences (odisees) will be newly added to instances.\n",
      "Instance ontology will be newly added to instances.\n",
      "Instance oobc will be newly added to instances.\n",
      "Instance open agent architecture (oaa)  will be newly added to instances.\n",
      "Instance openmdao will be newly added to instances.\n",
      "Instance openvsp will be newly added to instances.\n",
      "Instance operational collaborative environment (oce) will be newly added to instances.\n",
      "Instance optimization will be newly added to instances.\n",
      "Instance optimus workflow management tool will be newly added to instances.\n",
      "Instance optimus will be newly added to instances.\n",
      "Instance oracle server will be newly added to instances.\n",
      "Instance oracle will be newly added to instances.\n",
      "Instance orbital debris ontology (odo) will be newly added to instances.\n",
      "Instance orbital space environment domain ontology will be newly added to instances.\n",
      "Instance outlook will be newly added to instances.\n",
      "Instance owl dl will be newly added to instances.\n",
      "Instance owl-ql will be newly added to instances.\n",
      "Instance owl will be newly added to instances.\n",
      "Instance p1 will be newly added to instances.\n",
      "Instance parasolid will be newly added to instances.\n",
      "Instance pattern analyis will be newly added to instances.\n",
      "Instance pdf files will be newly added to instances.\n",
      "Instance pdf will be newly added to instances.\n",
      "Instance perliminary design will be newly added to instances.\n",
      "Instance pest will be newly added to instances.\n",
      "Instance physical sensing will be newly added to instances.\n",
      "Instance plm will be newly added to instances.\n",
      "Instance png will be newly added to instances.\n",
      "Instance point clouds will be newly added to instances.\n",
      "Instance populate the objects of product term class with semantic information and enclose the objects in message being sent will be newly added to instances.\n",
      "Instance postgresql will be newly added to instances.\n",
      "Instance postscript will be newly added to instances.\n",
      "Instance prado will be newly added to instances.\n",
      "Instance preliminary design will be newly added to instances.\n",
      "Instance pro/e(ptc) will be newly added to instances.\n",
      "Instance problem solving will be newly added to instances.\n",
      "Instance process definitions will be newly added to instances.\n",
      "Instance process integration and design optimization (pido) platforms  will be newly added to instances.\n",
      "Instance process integration and design optimization (pido) will be newly added to instances.\n",
      "Instance processe data will be newly added to instances.\n",
      "Instance processing will be newly added to instances.\n",
      "Instance product data management system will be newly added to instances.\n",
      "Instance product data management will be newly added to instances.\n",
      "Instance product design will be newly added to instances.\n",
      "Instance product support will be newly added to instances.\n",
      "Instance product verification will be newly added to instances.\n",
      "Instance production & product launch will be newly added to instances.\n",
      "Instance project planning will be newly added to instances.\n",
      "Instance proteus will be newly added to instances.\n",
      "Instance protégé will be newly added to instances.\n",
      "Instance pykechain will be newly added to instances.\n",
      "Instance python application will be newly added to instances.\n",
      "Instance python will be newly added to instances.\n",
      "Instance qfview web interface will be newly added to instances.\n",
      "Instance quantity will be newly added to instances.\n",
      "Instance r will be newly added to instances.\n",
      "Instance radex (rationale-based design explanation) will be newly added to instances.\n",
      "Instance rce will be newly added to instances.\n",
      "Instance rdf will be newly added to instances.\n",
      "Instance rds will be newly added to instances.\n",
      "Instance read data from the model will be newly added to instances.\n",
      "Instance reasoner will be newly added to instances.\n",
      "Instance reasoning will be newly added to instances.\n",
      "Instance recycling will be newly added to instances.\n",
      "Instance rehost' each release of a flight dynamics model from one simulation environment to another one will be newly added to instances.\n",
      "Instance remote component environment (rce) will be newly added to instances.\n",
      "Instance requirements verification framework (rvf)  will be newly added to instances.\n",
      "Instance resource description will be newly added to instances.\n",
      "Instance resources description framework (rdf) will be newly added to instances.\n",
      "Instance rest (http) api will be newly added to instances.\n",
      "Instance retrieve product term object and task requirement will be newly added to instances.\n",
      "Instance review will be newly added to instances.\n",
      "Instance rom (recursive object model) will be newly added to instances.\n",
      "Instance rss (really simple syndication) will be newly added to instances.\n",
      "Instance rvf will be newly added to instances.\n",
      "Instance s1 will be newly added to instances.\n",
      "Instance s2 will be newly added to instances.\n",
      "Instance sales and marketing will be newly added to instances.\n",
      "Instance sas will be newly added to instances.\n",
      "Instance saxon will be newly added to instances.\n",
      "Instance scheduling will be newly added to instances.\n",
      "Instance  scientific workflows (sw) will be newly added to instances.\n",
      "Instance searching for a solution to problems that may have been solved will be newly added to instances.\n",
      "Instance section description will be newly added to instances.\n",
      "Instance sectors of database will be newly added to instances.\n",
      "Instance selection of a suitable mdo architecture for the problem at hand will be newly added to instances.\n",
      "Instance semantic information transferring will be newly added to instances.\n",
      "Instance semantic knowledge management within design engineering will be newly added to instances.\n",
      "Instance semantic lexicon will be newly added to instances.\n",
      "Instance semantic representation will be newly added to instances.\n",
      "Instance semsearch will be newly added to instances.\n",
      "Instance serql will be newly added to instances.\n",
      "Instance share documents will be newly added to instances.\n",
      "Instance share point will be newly added to instances.\n",
      "Instance sharing and usage of surrogate models will be newly added to instances.\n",
      "Instance sharing of knowledge and expertise will be newly added to instances.\n",
      "Instance simulation workflow management (swfm) will be newly added to instances.\n",
      "Instance soap will be newly added to instances.\n",
      "Instance social interaction framework (sif) will be newly added to instances.\n",
      "Instance software interlinking tools will be newly added to instances.\n",
      "Instance software interlinking will be newly added to instances.\n",
      "Instance solidedge will be newly added to instances.\n",
      "Instance solidworks will be newly added to instances.\n",
      "Instance solr will be newly added to instances.\n",
      "Instance solving will be newly added to instances.\n",
      "Instance space situational awareness ontology (ssao) will be newly added to instances.\n",
      "Instance sparql will be newly added to instances.\n",
      "Instance spss will be newly added to instances.\n",
      "Instance sql will be newly added to instances.\n",
      "Instance step part 42:edm will be newly added to instances.\n",
      "Instance step will be newly added to instances.\n",
      "Instance stl will be newly added to instances.\n",
      "Instance structural solver will be newly added to instances.\n",
      "Instance summarizing will be newly added to instances.\n",
      "Instance sumo will be newly added to instances.\n",
      "Instance support & service will be newly added to instances.\n",
      "Instance surrogate model generator (smg) will be newly added to instances.\n",
      "Instance surrogate model repository will be newly added to instances.\n",
      "Instance sweetwiki will be newly added to instances.\n",
      "Instance swf software package rce will be newly added to instances.\n",
      "Instance syndeia will be newly added to instances.\n",
      "Instance sysml model will be newly added to instances.\n",
      "Instance sysml  will be newly added to instances.\n",
      "Instance system arch will be newly added to instances.\n",
      "Instance system architecting will be newly added to instances.\n",
      "Instance system definition will be newly added to instances.\n",
      "Instance system design will be newly added to instances.\n",
      "Instance system identification will be newly added to instances.\n",
      "Instance system integration will be newly added to instances.\n",
      "Instance system specification will be newly added to instances.\n",
      "Instance system synthesis will be newly added to instances.\n",
      "Instance systems engineering (se) will be newly added to instances.\n",
      "Instance t-rex will be newly added to instances.\n",
      "Instance tables, spreadsheets will be newly added to instances.\n",
      "Instance taxonomy will be newly added to instances.\n",
      "Instance terminology recognition will be newly added to instances.\n",
      "Instance test report will be newly added to instances.\n",
      "Instance text analyisis will be newly added to instances.\n",
      "Instance text2onto will be newly added to instances.\n",
      "Instance text2rdf will be newly added to instances.\n",
      "Instance texttoonto will be newly added to instances.\n",
      "Instance tiff will be newly added to instances.\n",
      "Instance tigl will be newly added to instances.\n",
      "Instance tixi will be newly added to instances.\n",
      "Instance tooling development will be newly added to instances.\n",
      "Instance tooling parts will be newly added to instances.\n",
      "Instance tools needed will be newly added to instances.\n",
      "Instance topic maps will be newly added to instances.\n",
      "Instance tornado will be newly added to instances.\n",
      "Instance tr will be newly added to instances.\n",
      "Instance training will be newly added to instances.\n",
      "Instance translations among distinct vocabularies will be newly added to instances.\n",
      "Instance triple store will be newly added to instances.\n",
      "Instance txt will be newly added to instances.\n",
      "Instance ug will be newly added to instances.\n",
      "Instance uml will be newly added to instances.\n",
      "Instance unformatted documents will be newly added to instances.\n",
      "Instance vampzero element will be newly added to instances.\n",
      "Instance vampzero will be newly added to instances.\n",
      "Instance vba will be newly added to instances.\n",
      "Instance vdk/hc will be newly added to instances.\n",
      "Instance video will be newly added to instances.\n",
      "Instance visualization (xdsm) will be newly added to instances.\n",
      "Instance visualization in support of the shopping paradigm will be newly added to instances.\n",
      "Instance visualization interface for physical programming will be newly added to instances.\n",
      "Instance visualization of large, complex ad processes will be newly added to instances.\n",
      "Instance visualization package will be newly added to instances.\n",
      "Instance vivisimo will be newly added to instances.\n",
      "Instance vrml will be newly added to instances.\n",
      "Instance wav will be newly added to instances.\n",
      "Instance web ontology language (owl) will be newly added to instances.\n",
      "Instance web server will be newly added to instances.\n",
      "Instance web-based decision tool will be newly added to instances.\n",
      "Instance wf execution will be newly added to instances.\n",
      "Instance whatsopt will be newly added to instances.\n",
      "Instance wide body transports will be newly added to instances.\n",
      "Instance wikipedia will be newly added to instances.\n",
      "Instance wmv will be newly added to instances.\n",
      "Instance word doc will be newly added to instances.\n",
      "Instance word processor documents will be newly added to instances.\n",
      "Instance word will be newly added to instances.\n",
      "Instance wordnet will be newly added to instances.\n",
      "Instance workflow management will be newly added to instances.\n",
      "Instance workflow system will be newly added to instances.\n",
      "Instance workflow systems will be newly added to instances.\n",
      "Instance write data back to the model will be newly added to instances.\n",
      "Instance wrp will be newly added to instances.\n",
      "Instance x-forms will be newly added to instances.\n",
      "Instance xcalibr (xml capability analysis library) will be newly added to instances.\n",
      "Instance xml schema definition (xsd) will be newly added to instances.\n",
      "Instance xml-schema-definition will be newly added to instances.\n",
      "Instance xml will be newly added to instances.\n",
      "Instance xquery will be newly added to instances.\n",
      "Instance yawl will be newly added to instances.\n",
      "Populating Obsidian folder\n",
      "Added 0 instances and 5 classes to Obsidian folder\n"
     ]
    }
   ],
   "source": [
    "# director.build_obsidian_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Setup done.\n",
    "Proceeding to:\n",
    "\n",
    "## Matrix calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free unneeded memory\n",
    "# del paper_nlp_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# run_debug_test(config, instances, papers, paper_instance_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdirector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_occurence_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\workspace\\borgnetzwerk\\tools\\bnw_tools\\SLR\\builder.py:289\u001b[0m, in \u001b[0;36mPaperInstanceDirector.setup_occurence_matrix\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccurrence_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# self.paper_instance_occurrence_matrix = self.builder[\"occurrence_matrix\"].matrix\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\workspace\\borgnetzwerk\\tools\\bnw_tools\\SLR\\builder.py:279\u001b[0m, in \u001b[0;36mPaperInstanceDirector.sort_instances\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# sort all matrixes accordingly\u001b[39;00m\n\u001b[0;32m    277\u001b[0m new_order \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(new_order)\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moccurrence_matrix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreorder_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpapers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccurrence_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mhandle_deletions(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpapers)\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccurrence_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39minstances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstances\n",
      "File \u001b[1;32mC:\\workspace\\borgnetzwerk\\tools\\bnw_tools\\SLR\\builder.py:1000\u001b[0m, in \u001b[0;36mMatrixBuilder.reorder_matrix\u001b[1;34m(self, new_order, cols)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreorder_matrix\u001b[39m(\u001b[38;5;28mself\u001b[39m, new_order, cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cols:\n\u001b[1;32m-> 1000\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_order\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1001\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix[new_order, :]\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "director.setup_occurence_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_debug_test(config, literals, papers, paper_instance_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all text files\n",
    "def get_paper_full_text(directory):\n",
    "    paper_full_text = {}\n",
    "    for folder in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    paper_full_text[file[:-4]] = file_path\n",
    "                    break\n",
    "\n",
    "    return paper_full_text\n",
    "\n",
    "\n",
    "paper_full_text = get_paper_full_text(\n",
    "    \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/00_PDFs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: find occurrences of instances in full text of papers\n",
    "import sys\n",
    "from bisect import bisect_left\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "class PosInPaper:\n",
    "    def __init__(self):\n",
    "        # List of paper identifiers\n",
    "        self.papers = []\n",
    "        # List of literals\n",
    "        self.literals = []\n",
    "        # Dict of unique words across all literals\n",
    "        self.words = {}\n",
    "        self.word_len = []\n",
    "        # List of unique combinations of words across all literals\n",
    "        self.word_combinations = {}\n",
    "        self.word_combination_lists = []\n",
    "        self.word_combination_index_literal = {}\n",
    "        # 2D list mapping pairs of literals to their word combination index\n",
    "        self.word_combination_index_literal_literal = []\n",
    "        # 2D list of SortedSets, each containing the positions of a word in a paper\n",
    "        self.word_occurrences_in_papers = []\n",
    "        # 3D list containing the minimum distances between word combinations in each paper\n",
    "        self.min_distances = []\n",
    "\n",
    "    @time_function\n",
    "    def populate(\n",
    "        self,\n",
    "        config: Config,\n",
    "        papers: list,\n",
    "        literals: list[str],\n",
    "        paper_full_text,\n",
    "        optimize=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Populates the internal data structures with occurrences and distances of literals in papers.\n",
    "\n",
    "        Parameters:\n",
    "        - config (Config): Configuration object containing settings.\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - literals (list[str]): List of literals to process.\n",
    "        - paper_full_text (dict): Mapping from paper identifiers to their full text file paths.\n",
    "        - optimize (bool): Flag to optimize data structures after population.\n",
    "        \"\"\"\n",
    "        self.initialize_variables(papers, literals)\n",
    "        self.process_literals()\n",
    "        self.process_literal_combinations()\n",
    "        self.setup_data_structures()\n",
    "        self.find_occurrences_in_texts(paper_full_text)\n",
    "        if optimize:\n",
    "            self.optimize_data()\n",
    "\n",
    "    def update_list_attribute(self, list, name):\n",
    "        existing = getattr(self, name)\n",
    "        if not existing:\n",
    "            setattr(self, name, list)\n",
    "        else:\n",
    "            if existing != list:\n",
    "                print(f\"Warning: {name} has changed.\")\n",
    "                for item in list:\n",
    "                    if item not in existing:\n",
    "                        print(f\"Item {item} is new.\")\n",
    "                        existing.append(item)\n",
    "                # update\n",
    "                raise NotImplementedError(\n",
    "                    \"create a function to update other relying attributes\"\n",
    "                )\n",
    "\n",
    "    @time_function\n",
    "    def initialize_variables(self, papers, literals):\n",
    "        \"\"\"\n",
    "        Initializes basic variables for the class instance.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        if not self.papers:\n",
    "            self.papers = papers\n",
    "        else:\n",
    "            if self.papers != papers:\n",
    "                print(\"Warning: Papers have changed.\")\n",
    "                for paper in papers:\n",
    "                    if paper not in self.papers:\n",
    "                        print(f\"Paper {paper} is new.\")\n",
    "                        self.papers.append(paper)\n",
    "        # self.papers = papers\n",
    "\n",
    "        if not self.literals:\n",
    "            self.literals = literals\n",
    "        else:\n",
    "            if self.literals != literals:\n",
    "                print(\"Warning: Literals have changed.\")\n",
    "                for literal in literals:\n",
    "                    if literal not in self.literals:\n",
    "                        print(f\"Literal {literal} is new.\")\n",
    "                        self.literals.append(literal)\n",
    "                        self.word_combination_index_literal[literal] = None\n",
    "\n",
    "        if len(self.literals) != len(self.word_combination_index_literal):\n",
    "            for literal in self.literals:\n",
    "                if literal not in self.word_combination_index_literal:\n",
    "                    self.word_combination_index_literal[literal] = None\n",
    "            # sort self.word_combination_index_literal by self.literals\n",
    "            self.word_combination_index_literal = {\n",
    "                k: self.word_combination_index_literal[k] for k in self.literals\n",
    "            }\n",
    "        if (\n",
    "            isinstance(self.word_combination_index_literal_literal, np.ndarray)\n",
    "            and self.word_combination_index_literal_literal.size == 0\n",
    "        ):\n",
    "            self.word_combination_index_literal_literal = np.full(\n",
    "                (len(self.literals), len(self.literals)), None, dtype=object\n",
    "            )\n",
    "        elif (\n",
    "            isinstance(self.word_combination_index_literal_literal, list)\n",
    "            and self.word_combination_index_literal_literal == []\n",
    "        ):\n",
    "            self.word_combination_index_literal_literal = [\n",
    "                [None] * len(self.literals) for _ in range(len(self.literals))\n",
    "            ]\n",
    "        elif len(self.literals) != len(self.word_combination_index_literal_literal):\n",
    "            # pad self.word_combination_index_literal_literal\n",
    "            len_dif = len(self.literals) - len(\n",
    "                self.word_combination_index_literal_literal\n",
    "            )\n",
    "            self.word_combination_index_literal_literal = np.pad(\n",
    "                self.word_combination_index_literal_literal,\n",
    "                ((0, len_dif), (0, len_dif)),\n",
    "                \"constant\",\n",
    "                constant_values=None,\n",
    "            )\n",
    "            # self.word_combination_index_literal_literal = [[None] * len(self.literals) for _ in range(len(self.literals))]\n",
    "\n",
    "        # self.literals = literals\n",
    "\n",
    "    @time_function\n",
    "    def process_literals(self):\n",
    "        \"\"\"\n",
    "        Processes each literal to extract and store unique words and word combinations.\n",
    "\n",
    "        Parameters:\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        for lit in self.literals:\n",
    "            word_list = split_string(lit)\n",
    "            self.add_words(word_list)\n",
    "            self.add_if_word_combination(word_list, lit)\n",
    "\n",
    "    def add_words(self, word_list):\n",
    "        \"\"\"\n",
    "        Adds unique words from a list to the internal list of words.\n",
    "\n",
    "        Parameters:\n",
    "        - word_list (list): List of words to add.\n",
    "        \"\"\"\n",
    "        for word in word_list:\n",
    "            if word not in self.words:\n",
    "                self.words[word] = len(self.words)\n",
    "                self.word_len.append(len(word))\n",
    "\n",
    "    def add_if_word_combination(self, word_list, lit):\n",
    "        \"\"\"\n",
    "        Adds a unique combination of words from a list to the internal list of word combinations.\n",
    "\n",
    "        Parameters:\n",
    "        - word_list (list): List of words forming a combination.\n",
    "        - lit (str): The literal corresponding to the word combination.\n",
    "        \"\"\"\n",
    "        if len(word_list) > 1:\n",
    "            pos = self.word_combination_index_literal.get(lit, -1)\n",
    "            if pos == -1 or pos == None:\n",
    "                froz = frozenset(word_list)\n",
    "                pos = len(self.word_combinations)\n",
    "                self.add_word_combination(froz, pos)\n",
    "                self.word_combination_index_literal[lit] = pos\n",
    "\n",
    "    def add_word_combination(self, froz, pos):\n",
    "        self.word_combinations[froz] = pos\n",
    "        self.word_combination_lists.append(\n",
    "            [self.words[word] for word in sorted(froz, key=len, reverse=True)]\n",
    "        )\n",
    "\n",
    "    @time_function\n",
    "    def process_literal_combinations(self):\n",
    "        \"\"\"\n",
    "        Processes combinations of literals to store their indices in the internal data structure.\n",
    "\n",
    "        Parameters:\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        # Use a dictionary for quick lookup and storage\n",
    "        combination_index = len(self.word_combinations)\n",
    "\n",
    "        for id1, literal1 in enumerate(self.literals):\n",
    "            for id2 in range(id1 + 1, len(self.literals)):\n",
    "                if self.word_combination_index_literal_literal[id1][id2] is not None:\n",
    "                    continue\n",
    "                literal2 = self.literals[id2]\n",
    "                # Use a sorted tuple for consistent ordering\n",
    "                froz = frozenset(split_string(literal1) + split_string(literal2))\n",
    "                # Check if the combination is already in the dictionary\n",
    "                pos = self.word_combinations.get(froz, -1)\n",
    "                if pos == -1:\n",
    "                    pos = combination_index\n",
    "                    combination_index += 1\n",
    "\n",
    "                    self.add_word_combination(froz, pos)\n",
    "\n",
    "                # Update the matrix with the index of the combination\n",
    "                self.word_combination_index_literal_literal[id1][id2] = pos\n",
    "                self.word_combination_index_literal_literal[id2][id1] = pos\n",
    "\n",
    "    @time_function\n",
    "    def setup_data_structures(self):\n",
    "        \"\"\"\n",
    "        Initializes the data structures for storing word occurrences and minimum distances.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        \"\"\"\n",
    "        if self.word_occurrences_in_papers == []:\n",
    "            self.word_occurrences_in_papers = [\n",
    "                [[] for _ in self.words] for _ in self.papers\n",
    "            ]\n",
    "        new_papers = len(self.papers) - len(self.word_occurrences_in_papers)\n",
    "        new_words = len(self.words) - len(self.word_occurrences_in_papers[0])\n",
    "        if new_words > 0:\n",
    "            for paper in self.word_occurrences_in_papers:\n",
    "                paper += [[] for _ in range(new_words)]\n",
    "        if new_papers > 0:\n",
    "            self.word_occurrences_in_papers += [\n",
    "                [[] for _ in self.words] for _ in range(new_papers)\n",
    "            ]\n",
    "\n",
    "\n",
    "        if (\n",
    "            isinstance(self.min_distances, list)\n",
    "            and self.min_distances == []\n",
    "            or self.min_distances is None\n",
    "        ):\n",
    "            self.min_distances = np.full(\n",
    "                (len(self.papers), len(self.word_combinations)), -2, dtype=int\n",
    "            )\n",
    "        # If new papers or words have been added, update the data structures\n",
    "        if len(self.papers) > len(self.min_distances):\n",
    "            self.min_distances = np.pad(\n",
    "                self.min_distances,\n",
    "                ((0, len(self.papers) - len(self.min_distances)), (0, 0)),\n",
    "                \"constant\",\n",
    "                constant_values=-2,\n",
    "            )\n",
    "        if len(self.word_combinations) > len(self.min_distances[0]):\n",
    "            len_dif = len(self.word_combinations) - len(self.min_distances[0])\n",
    "            self.min_distances = np.pad(\n",
    "                self.min_distances,\n",
    "                ((0, 0), (0, len_dif)),\n",
    "                \"constant\",\n",
    "                constant_values=-2,\n",
    "            )\n",
    "\n",
    "    @time_function\n",
    "    def find_occurrences_in_texts(self, paper_full_text):\n",
    "        \"\"\"\n",
    "        Finds and stores the occurrences of each word in the full text of each paper.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - paper_full_text (dict): Mapping from paper identifiers to their full text file paths.\n",
    "        \"\"\"\n",
    "        for paperID, paper in enumerate(self.papers):\n",
    "            if paper in paper_full_text:\n",
    "                with open(paper_full_text[paper], \"r\", encoding=\"utf8\") as f:\n",
    "                    text = f.read().lower()\n",
    "                    for word, wordID in self.words.items():\n",
    "                        if not self.word_occurrences_in_papers[paperID][wordID]:\n",
    "                            self.find_and_add_word_occurrences(\n",
    "                                paperID, wordID, word, text\n",
    "                            )\n",
    "            else:\n",
    "                print(f\"Paper {paper} has no full text available.\")\n",
    "\n",
    "    def find_and_add_word_occurrences(self, paperID, wordID, word, text):\n",
    "        \"\"\"\n",
    "        Finds and adds the occurrences of a word in a paper's text to the internal data structure.\n",
    "\n",
    "        Parameters:\n",
    "        - paperID (int): The index of the paper in the internal list.\n",
    "        - wordID (int): The index of the word in the internal list.\n",
    "        - word (str): The word to find occurrences of.\n",
    "        - text (str): The full text of the paper.\n",
    "        \"\"\"\n",
    "        pos = text.find(word)\n",
    "        while pos != -1:\n",
    "            # self.word_occurrences_in_papers[paperID][wordID].add(pos)\n",
    "            self.word_occurrences_in_papers[paperID][wordID].append([pos, wordID])\n",
    "            pos = text.find(word, pos + 1)\n",
    "\n",
    "    @time_function\n",
    "    def optimize_data(self):\n",
    "        \"\"\"\n",
    "        Optimizes the internal data structures for faster access and smaller memory footprint.\n",
    "        \"\"\"\n",
    "        # self.word_combination_index_literal_literal = np.array(self.word_combination_index_literal_literal, dtype=int)\n",
    "        for paperID in range(len(self.papers)):\n",
    "            for wordID in range(len(self.words)):\n",
    "                # self.word_occurrences_in_papers[paperID][wordID] = SortedSet(self.word_occurrences_in_papers[paperID][wordID])\n",
    "                if not self.word_occurrences_in_papers[paperID][wordID]:\n",
    "                    continue\n",
    "                if isinstance(self.word_occurrences_in_papers[paperID][wordID][0], int):\n",
    "                    self.word_occurrences_in_papers[paperID][wordID] = [\n",
    "                        (x, wordID)\n",
    "                        for x in self.word_occurrences_in_papers[paperID][wordID]\n",
    "                    ]\n",
    "                    print(\n",
    "                        f\"Optimizing {list(self.words.keys())[wordID]} in paper {paperID}\"\n",
    "                    )\n",
    "                for occurrence in self.word_occurrences_in_papers[paperID][wordID]:\n",
    "                    if isinstance(occurrence, int):\n",
    "                        self.word_occurrences_in_papers[paperID][wordID] = [\n",
    "                            (occurrence, wordID)\n",
    "                        ]\n",
    "                        # break\n",
    "                        raise Exception(\"This should not happen\")\n",
    "\n",
    "    def save_to_csv(self, config: Config = None, path=None, name=\"pos_in_paper\"):\n",
    "        if path is None:\n",
    "            path = config.get_output_path()\n",
    "\n",
    "        # save min_distances to csv\n",
    "        # dump self.min_distances to csv, with self.papers as row headers and self.word_combinations as column headers\n",
    "        filepath = os.path.join(path, name + \"_min_distances.csv\")\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            word_combinations = [\n",
    "                \"_\".join(sorted(froz, key=len, reverse=True))\n",
    "                for froz in self.word_combinations.keys()\n",
    "            ]\n",
    "            f.write(\n",
    "                \"papers\"\n",
    "                + config.csv_separator\n",
    "                + config.csv_separator.join(word_combinations)\n",
    "                + \"\\n\"\n",
    "            )\n",
    "            for i, paper in enumerate(self.papers):\n",
    "                f.write(\n",
    "                    paper\n",
    "                    + config.csv_separator\n",
    "                    + config.csv_separator.join(map(str, self.min_distances[i]))\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "\n",
    "    @time_function\n",
    "    def save_to_file(\n",
    "        self,\n",
    "        config,\n",
    "        path=None,\n",
    "        name=\"pos_in_paper\",\n",
    "        check_size=False,\n",
    "        min_distance_to_csv=False,\n",
    "        backup=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Saves the internal data structures to files for persistence.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str, optional): The base path for the output files. Defaults to \"pos_in_paper\".\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = config.get_output_path()\n",
    "        filepath = os.path.join(path, name + \".json\")\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        for key, value in self.__dict__.items():\n",
    "            # if key == \"min_distances\" or key == \"word_combination_index_literal_literal\":\n",
    "            if (\n",
    "                value.__class__.__name__ == \"ndarray\"\n",
    "            ):  # min_distances, word_combination_index_literal_literal\n",
    "                data[key] = value.tolist()\n",
    "            # if key == \"min_distances\":\n",
    "            #     data[key] = value.tolist()\n",
    "            elif key == \"word_combinations\":\n",
    "                data[key] = {\n",
    "                    \"_\".join(key): value\n",
    "                    for key, value in self.word_combinations.items()\n",
    "                }\n",
    "            else:\n",
    "                data[key] = value\n",
    "            if check_size:\n",
    "                # Construct the file name for each sub-dictionary\n",
    "                filepath = os.path.join(path, f\"{name}_{key}.json\")\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(data[key], f, ensure_ascii=False)\n",
    "            pass\n",
    "\n",
    "        if min_distance_to_csv:\n",
    "            self.save_to_csv(config, path, name)\n",
    "\n",
    "        if backup:\n",
    "            backup_path = os.path.join(path, name + \"_backup\" + \".json\")\n",
    "            if os.path.exists(filepath):\n",
    "                existing_is_healthy = True\n",
    "                try:\n",
    "                    self.load_from_file(config, path, name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Overwriting existing save\")\n",
    "                    existing_is_healthy = False\n",
    "                if existing_is_healthy:\n",
    "                    if os.path.exists(backup_path):\n",
    "                        os.remove(backup_path)\n",
    "                    os.rename(filepath, backup_path)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "    @time_function\n",
    "    def load_from_file(self, config, path=None, name=\"pos_in_paper\", backup=True):\n",
    "        \"\"\"\n",
    "        Loads the internal data structures from files.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str, optional): The base path for the input files. Defaults to \"pos_in_paper\".\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = config.get_output_path()\n",
    "        filepath = os.path.join(path, name + \".json\")\n",
    "\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {filepath}: {e}\")\n",
    "            if backup:\n",
    "                backup_path = os.path.join(path, name + \"_backup\" + \".json\")\n",
    "                if os.path.exists(backup_path):\n",
    "                    print(f\"Trying to load backup file {backup_path}\")\n",
    "                    with open(backup_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)\n",
    "                else:\n",
    "                    raise Exception(f\"No backup file found at {backup_path}\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        for key, value in data.items():\n",
    "            if key == \"word_combinations\":\n",
    "                setattr(\n",
    "                    self,\n",
    "                    key,\n",
    "                    {\n",
    "                        frozenset(split_string(sub_key)): i\n",
    "                        for i, sub_key in enumerate(value)\n",
    "                    },\n",
    "                )\n",
    "            elif (\n",
    "                key == \"min_distances\"\n",
    "                or key == \"word_combination_index_literal_literal\"\n",
    "            ):\n",
    "                setattr(self, key, np.array(value))\n",
    "            else:\n",
    "                try:\n",
    "                    setattr(self, key, value)\n",
    "                except Exception as e:\n",
    "                    raise Exception(f\"Error loading pos_in_paper attribute {key}: {e}\")\n",
    "\n",
    "        self.setup_data_structures()\n",
    "\n",
    "    @time_function\n",
    "    def calculate_all_possible(self):\n",
    "        \"\"\"\n",
    "        Calculates the minimum distances between all possible combinations of literals in all papers.\n",
    "        \"\"\"\n",
    "        save_every = None\n",
    "        if len(self.papers) > 300:\n",
    "            print(\n",
    "                \"Warning: This operation is computationally expensive and may take a long time.\"\n",
    "            )\n",
    "            save_every = 300\n",
    "        for p in range(len(self.papers)):\n",
    "            if save_every and p % save_every == 0:\n",
    "                print(f\"Processing paper {p} of {len(self.papers)}\")\n",
    "                self.save_to_file(config)\n",
    "            for w in range(len(self.word_combinations)):\n",
    "                self.find_min_distance_by_id(p, w)\n",
    "            # for i in range(len(self.literals)):\n",
    "            #     for j in range(i + 1, len(self.literals)):\n",
    "            #         # get word_combination_index_literal_literal\n",
    "            # self.find_min_distance_by_id(p, self.word_combination_index_literal_literal[i][j])\n",
    "\n",
    "    def find_min_distance_by_id(self, paperID, wcID):\n",
    "        \"\"\"\n",
    "        Finds the minimum distance between occurrences of literals in a paper.\n",
    "\n",
    "        Parameters:\n",
    "        - paper (str): The identifier for the paper.\n",
    "        - literals (list): A list of literals for which the distance is to be found.\n",
    "        - allow_call (bool): Flag to allow recursive call to get_min_distance.\n",
    "\n",
    "        Returns:\n",
    "        - int: The minimum distance between occurrences of the literals.\n",
    "        \"\"\"\n",
    "        distance = self.min_distances[paperID][wcID]\n",
    "\n",
    "        if distance == -1:\n",
    "            # word combination not found in paper\n",
    "            return -1\n",
    "        if distance == -2:\n",
    "            # calculate distance\n",
    "            pass\n",
    "        else:\n",
    "            return distance\n",
    "\n",
    "        list_ids = self.word_combination_lists[wcID]\n",
    "        # since we have attached global Word IDs to the occurrences, we need to map to their local position\n",
    "        list_ids_map = {list_ids[i]: i for i in range(len(list_ids))}\n",
    "        # literals = [list(self.words)[i] for i in list_ids]\n",
    "\n",
    "        # TODO: It should be possible to remove smaller words from the list,\n",
    "        # if a larger word contains it:\n",
    "        # e.g. remove \"engine\" if \"engineer\" is in the list\n",
    "        ## The following implementation works, but is not used for now. Reasons:\n",
    "        ## 1. It could be slower than the current implementation\n",
    "        ## 2. It might be beneficial to future use-cases to not remove smaller words\n",
    "        # literals = [list(self.words)[key] for key in list_ids_map]\n",
    "        # # check if any literal is a substring of another\n",
    "        # for i, lit1 in enumerate(literals):\n",
    "        #     for j, lit2 in enumerate(literals):\n",
    "        #         if i != j and lit1 in lit2:\n",
    "        #             # if lit1 is a substring of lit2, remove it from the list\n",
    "        #             list_ids_map.pop(list_ids[i])\n",
    "        #             break\n",
    "\n",
    "        lit_len = [self.word_len[i] for i in list_ids]\n",
    "\n",
    "        for i in list_ids:\n",
    "            if not self.word_occurrences_in_papers[paperID][i]:\n",
    "                self.min_distances[paperID][wcID] = -1\n",
    "                return -1\n",
    "        # Outsourced to optimize\n",
    "        # inputs = [[(x, i) for x in self.word_occurrences_in_papers[paperID][wordID]] for i, wordID in enumerate(list_ids)]\n",
    "        inputs = [\n",
    "            self.word_occurrences_in_papers[paperID][wordID] for wordID in list_ids\n",
    "        ]\n",
    "\n",
    "        indices = [lst[0][0] for lst in inputs]\n",
    "        best = float(\"inf\")\n",
    "\n",
    "        for item in sorted(sum(inputs, [])):\n",
    "            if item[0] not in indices:\n",
    "                continue\n",
    "            indices[list_ids_map[item[1]]] = item[0]\n",
    "            arr_min = min(indices)\n",
    "            best = min(max(indices) - arr_min - lit_len[indices.index(arr_min)], best)\n",
    "            if best <= 0:\n",
    "                best = 0\n",
    "                break\n",
    "        self.min_distances[paperID][wcID] = best\n",
    "\n",
    "        return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_in_paper = PosInPaper()\n",
    "\n",
    "# config.recalculate_pos_in_paper = True # May be used for debug purposes\n",
    "\n",
    "if not config.recalculate_pos_in_paper:\n",
    "    try:\n",
    "        pos_in_paper.load_from_file(config)\n",
    "    # print exception\n",
    "    except Exception as e:\n",
    "        # if config.debug:\n",
    "        #     raise e\n",
    "        print(e)\n",
    "        print(\"Starting from scratch.\")\n",
    "\n",
    "        config.recalculate_pos_in_paper = True\n",
    "\n",
    "# TODO:\n",
    "# raise NotImplementedError(\"Implement a check that compares the loaded instances and papers with the current ones\")\n",
    "\n",
    "# if config.recalculate_pos_in_paper:\n",
    "pos_in_paper.populate(\n",
    "    config,\n",
    "    list(director.papers.keys()),\n",
    "    list(director.instances.keys()),\n",
    "    paper_full_text,\n",
    ")\n",
    "# pos_in_paper.save_to_file(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.recalculate_pos_in_paper or -2 in pos_in_paper.min_distances:\n",
    "    pos_in_paper.calculate_all_possible()\n",
    "    # Info: This method is extremely slow. requires more testing, which is currently done in a side project:\n",
    "    ## scripts\\SLR\\MVP\\test_case.ipynb\n",
    "    pos_in_paper.save_to_file(config)\n",
    "    config.recalculate_pos_in_paper = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # intermediate stitching:\n",
    "# papers = list(director.papers.keys())\n",
    "# literals = list(director.instances.keys())\n",
    "# paper_instance_occurrence_matrix = director.paper_instance_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "director.paper_full_text = paper_full_text\n",
    "\n",
    "director.builder[\"error_matrix_builder\"] = ErrorMatrixBuilder(director, pos_in_paper)\n",
    "director.builder[\"error_matrix_builder\"].build()\n",
    "director.builder[\"error_matrix_builder\"].save()\n",
    "director.sort_instances()\n",
    "director.builder[\"occurrence_matrix\"].save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance_instance Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_instance_co_occurrence_matrix = np.dot(\n",
    "    director.builder[\"occurrence_matrix\"].matrix.T, director.builder[\"occurrence_matrix\"].matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_instance_relative_co_occurrence_matrix = (\n",
    "    instance_instance_co_occurrence_matrix\n",
    "    / np.diag(instance_instance_co_occurrence_matrix)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize timeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def visualize_timeline(\n",
    "    config: Config,\n",
    "    year_instance_occurrence_matrix,\n",
    "    year_papers,\n",
    "    instances,\n",
    "    instance_types_dicts,\n",
    "    name=\"some_timeline\",\n",
    "    path=None,\n",
    "    recursion_depth=0,\n",
    "    start_index=0,\n",
    "    error_matrix=None,\n",
    "    error_instances=None,\n",
    "):\n",
    "    if not path:\n",
    "        path = config.get_output_path(path, visualization=True)\n",
    "    years = list(year_papers.keys())\n",
    "    max_papers = max([len(year_papers[year]) for year in years])\n",
    "    yearly_papers = [len(year_papers[year]) for year in years]\n",
    "\n",
    "    ALPHA_ERROR_LINE = 0.3\n",
    "    ALPHA_ERROR_ZONE = 0.2\n",
    "    ALPHA_PAPER_BAR = 0.3\n",
    "\n",
    "    for type in instance_types_dicts:\n",
    "        use = [instance in instance_types_dicts[type] for instance in instances]\n",
    "        type_instances = [\n",
    "            instance for instance, use_flag in zip(instances, use) if use_flag\n",
    "        ]\n",
    "        total_occurrences = [\n",
    "            np.sum(year_instance_occurrence_matrix[:, instances.index(instance)])\n",
    "            for instance in type_instances\n",
    "        ]\n",
    "        type_instances_sorted = [\n",
    "            x\n",
    "            for _, x in sorted(\n",
    "                zip(total_occurrences, type_instances),\n",
    "                key=lambda pair: pair[0],\n",
    "                reverse=True,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        PARTITION_SIZE = 10\n",
    "        # if error_instances is not None:\n",
    "        #     PARTITION_SIZE = int(0.5 * PARTITION_SIZE)\n",
    "\n",
    "        type_matrix = year_instance_occurrence_matrix[\n",
    "            :, [instances.index(instance) for instance in type_instances_sorted]\n",
    "        ]\n",
    "        factor = 1\n",
    "        size_x = (2 + len(years) / 6) * factor\n",
    "        size_y = (2 + max_papers / 15) * factor\n",
    "        size_y_2 = (2 + PARTITION_SIZE / 2) * factor\n",
    "        size_y = max(size_y, size_y_2)\n",
    "        fig, ax = plt.subplots(figsize=(size_x, size_y), dpi=300)\n",
    "\n",
    "        ax.set_xticks(range(len(years)))\n",
    "        years_labels = [year if len(year_papers[year]) > 0 else \"\" for year in years]\n",
    "        ax.set_xticklabels(years_labels, fontsize=10, rotation=90)\n",
    "\n",
    "        step_size = max(1, math.ceil(max_papers / 10))\n",
    "        ax.set_yticks(np.arange(0, max_papers + 1, step=step_size))\n",
    "        ax.set_yticklabels(\n",
    "            [str(int(x)) for x in np.arange(0, max_papers + 1, step=step_size)],\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "        # set y axis label\n",
    "        ax.set_ylabel(\"absolute\", fontsize=10)\n",
    "\n",
    "        plt.bar(\n",
    "            range(len(years)),\n",
    "            yearly_papers,\n",
    "            color=\"black\",\n",
    "            alpha=ALPHA_PAPER_BAR,\n",
    "            label=f\"Total papers ({sum(yearly_papers)})\",\n",
    "            zorder=0,\n",
    "        )\n",
    "\n",
    "        line_count = 0\n",
    "        i = start_index\n",
    "        while line_count < PARTITION_SIZE and i < len(type_instances_sorted):\n",
    "            instance = type_instances_sorted[i]\n",
    "            yearly_occurrences = type_matrix[:, i]\n",
    "            i_total_occurrences = yearly_occurrences.sum()\n",
    "            label = f\"{instance} ({i_total_occurrences})\"\n",
    "            values = yearly_occurrences\n",
    "            line = plt.plot(range(len(years)), values, label=label, zorder=3)[0]\n",
    "            line_count += 1\n",
    "            if error_matrix is not None and instance in error_instances:\n",
    "                color = line.get_color()\n",
    "                errors = error_matrix[:, error_instances.index(instance)]\n",
    "                errors_plus = yearly_occurrences + errors\n",
    "                line.set_label(f\"{instance} ({i_total_occurrences}-{sum(errors_plus)})\")\n",
    "                # Plot the error as a half transparent line on top of the normal line\n",
    "                plt.plot(\n",
    "                    range(len(years)),\n",
    "                    errors_plus,\n",
    "                    color=color,\n",
    "                    alpha=ALPHA_ERROR_LINE,\n",
    "                    label=f\"{instance} (w/o proximity)\",\n",
    "                    zorder=2,\n",
    "                )\n",
    "                line_count += 1\n",
    "                # color in the area between the normal line and the error line\n",
    "                plt.fill_between(\n",
    "                    range(len(years)),\n",
    "                    yearly_occurrences,\n",
    "                    errors_plus,\n",
    "                    color=color,\n",
    "                    alpha=ALPHA_ERROR_ZONE,\n",
    "                    zorder=1,\n",
    "                )\n",
    "            i += 1\n",
    "\n",
    "            # plt.scatter(range(len(years)), errors, color='red', label=f\"{instance} (error)\", zorder=1)\n",
    "        stop_index = i\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        plt.title(\n",
    "            f\"Number of papers covering {type} instances (#{start_index+1} to #{stop_index} of {len(type_instances_sorted)})\"\n",
    "        )\n",
    "\n",
    "        # Inset for relative values\n",
    "        fig.canvas.draw()\n",
    "        x_lim = ax.get_xlim()  # Get the current x-axis limits from the main plot\n",
    "\n",
    "        bbox = ax.get_position()\n",
    "        bb_left, bb_bottom = bbox.x0, bbox.y0\n",
    "        bb_width, bb_height = bbox.width, bbox.height\n",
    "\n",
    "        ax_inset = plt.axes(\n",
    "            [bb_left, 0.05, bb_width, 0.15],\n",
    "            alpha=ALPHA_PAPER_BAR,\n",
    "            facecolor=\"lightgrey\",\n",
    "        )\n",
    "        for i, instance in enumerate(\n",
    "            type_instances_sorted[start_index:stop_index], start=start_index\n",
    "        ):\n",
    "            yearly_occurrences = type_matrix[:, i]\n",
    "            values_relative = [\n",
    "                occurrences / papers if papers > 0 else 0\n",
    "                for occurrences, papers in zip(yearly_occurrences, yearly_papers)\n",
    "            ]\n",
    "            line_relative = ax_inset.plot(\n",
    "                range(len(years)),\n",
    "                values_relative,\n",
    "                label=f\"{instance} (relative)\",\n",
    "                zorder=3,\n",
    "            )[0]\n",
    "\n",
    "            # add the error part\n",
    "            if error_matrix is not None and instance in error_instances:\n",
    "                color = line_relative.get_color()\n",
    "                errors = error_matrix[:, error_instances.index(instance)]\n",
    "                errors_plus = yearly_occurrences + errors\n",
    "                errors_relative = [\n",
    "                    error / papers if papers > 0 else 0\n",
    "                    for error, papers in zip(errors_plus, yearly_papers)\n",
    "                ]\n",
    "                if max(errors_relative) > 1:\n",
    "                    print(f\"Error: {instance} has a relative error > 1\")\n",
    "                    # throw an exception because this should never be the case:\n",
    "                    # raise Exception(f\"Error: relative {instance} occurrence + error > 1\")\n",
    "\n",
    "                ax_inset.plot(\n",
    "                    range(len(years)),\n",
    "                    errors_relative,\n",
    "                    alpha=ALPHA_ERROR_LINE,\n",
    "                    color=color,\n",
    "                    label=f\"{instance} (error, relative)\",\n",
    "                    zorder=2,\n",
    "                )\n",
    "                # color in the area between the normal line and the error line\n",
    "                ax_inset.fill_between(\n",
    "                    range(len(years)),\n",
    "                    values_relative,\n",
    "                    errors_relative,\n",
    "                    alpha=ALPHA_ERROR_ZONE,\n",
    "                    color=color,\n",
    "                    zorder=1,\n",
    "                )\n",
    "\n",
    "        ax_inset.set_xlim(x_lim)\n",
    "\n",
    "        ax_inset.set_xticks([])\n",
    "        ax_inset.set_yticks(np.arange(0, 1.1, step=0.5))\n",
    "        ax_inset.set_yticklabels(\n",
    "            [f\"{int(x*100)}%\" for x in np.arange(0, 1.1, step=0.5)], fontsize=8\n",
    "        )\n",
    "\n",
    "        # set y axis label\n",
    "        ax_inset.set_ylabel(\"relative\", fontsize=10)\n",
    "\n",
    "        plt.subplots_adjust(bottom=0.3)\n",
    "\n",
    "        start_string = f\"{start_index+1}\"\n",
    "        stop_string = f\"{stop_index}\"\n",
    "\n",
    "        # fill up with 0 to have a constant length\n",
    "        start_string = \"0\" * (3 - len(start_string)) + start_string\n",
    "        stop_string = \"0\" * (3 - len(stop_string)) + stop_string\n",
    "\n",
    "        part_appendix = f\"{start_string}_to_{stop_string}\"\n",
    "        filepath = os.path.join(path, name)\n",
    "        plt.savefig(f\"{filepath}_{type.replace(' ', '_')}_{part_appendix}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        start_index = stop_index\n",
    "        if start_index < len(type_instances_sorted):\n",
    "            # if recursion_depth > 0:\n",
    "            #     break\n",
    "            visualize_timeline(\n",
    "                config,\n",
    "                year_instance_occurrence_matrix,\n",
    "                year_papers,\n",
    "                instances,\n",
    "                {type: instance_types_dicts[type]},\n",
    "                name,\n",
    "                path=path,\n",
    "                recursion_depth=recursion_depth + 1,\n",
    "                start_index=start_index,\n",
    "                error_matrix=error_matrix,\n",
    "                error_instances=error_instances,\n",
    "            )\n",
    "        start_index = 0\n",
    "\n",
    "\n",
    "# if config.visualize:\n",
    "#     yearly_error_matrix, year_error_papers = create_year_paper_occurrence_matrix(\n",
    "#         papers_metadata, error_matrix, error_papers, is_error_matrix=True\n",
    "#     )\n",
    "#     visualize_timeline(\n",
    "#         config,\n",
    "#         year_instance_occurrence_matrix,\n",
    "#         year_papers,\n",
    "#         instances,\n",
    "#         instance_types_dicts,\n",
    "#         name=\"year_instance_occurrence_matrix\",\n",
    "#         error_matrix=yearly_error_matrix,\n",
    "#         error_instances=error_instances,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year_paper_occurrence_matrix\n",
    "class YearPaperOccurrenceMatrixBuilder(MatrixBuilder):\n",
    "    def __init__(self, director, papers = None, paper_instance_occurrence_matrix = None, is_error_matrix=False, ):\n",
    "        super().__init__(director)\n",
    "\n",
    "        # self.papers_metadata = papers_metadata\n",
    "        self.papers:dict[str,Instance] = papers or director.papers\n",
    "        self.paper_instance_occurrence_matrix = paper_instance_occurrence_matrix or director.builder[\"occurrence_matrix\"].matrix\n",
    "        self.is_error_matrix = is_error_matrix\n",
    "        self.year_papers:dict[int,dict[str,Instance]] = {}\n",
    "\n",
    "\n",
    "\n",
    "    def build_matrix(self, paper_instance_occurrence_matrix = None, papers = None, is_error_matrix=False):\n",
    "        paper_instance_occurrence_matrix = paper_instance_occurrence_matrix or self.paper_instance_occurrence_matrix\n",
    "        papers = papers or self.papers\n",
    "        # self.matrix, self.year_papers = create_year_paper_occurrence_matrix(\n",
    "        #     papers_metadata, paper_instance_occurrence_matrix, papers, is_error_matrix\n",
    "        # )\n",
    "\n",
    "        # def create_year_paper_occurrence_matrix(\n",
    "        #     papers_metadata, paper_instance_occurrence_matrix, papers, is_error_matrix=False\n",
    "        # ):\n",
    "        indexed_papers = {paper: i for i, paper in enumerate(papers)}\n",
    "        for paper, instance in self.papers.items():\n",
    "            if hasattr(instance, \"year\"):\n",
    "                year = int(getattr(instance, \"year\"))\n",
    "                if year not in self.year_papers:\n",
    "                    self.year_papers[year] = {}\n",
    "                self.year_papers[year][paper] = instance\n",
    "\n",
    "        earliest = min(self.year_papers)\n",
    "        latest = max(self.year_papers)\n",
    "        span = latest - earliest + 1\n",
    "\n",
    "        for year in range(earliest, latest):\n",
    "            if year not in self.year_papers:\n",
    "                self.year_papers[year] = []\n",
    "\n",
    "        self.year_papers = {\n",
    "            k: v for k, v in sorted(self.year_papers.items(), key=lambda item: item[0])\n",
    "        }\n",
    "\n",
    "        if is_error_matrix:\n",
    "            # convert any value != 0 to 1\n",
    "            paper_instance_occurrence_matrix = np.where(\n",
    "                paper_instance_occurrence_matrix != 0, 1, 0\n",
    "            )\n",
    "\n",
    "        # create a year_instance_occurrence matrix from the paper_instance_occurrence_matrix\n",
    "        year_instance_occurrence_matrix = np.zeros(\n",
    "            (span, paper_instance_occurrence_matrix.shape[1]), dtype=int\n",
    "        )\n",
    "        for yearID, year in enumerate(self.year_papers):\n",
    "            for paper in self.year_papers[year]:\n",
    "                if paper in papers:\n",
    "                    paperID = indexed_papers[paper]\n",
    "                    year_instance_occurrence_matrix[\n",
    "                        yearID\n",
    "                    ] += paper_instance_occurrence_matrix[paperID]\n",
    "\n",
    "    def build(self):\n",
    "        self.build_matrix()\n",
    "\n",
    "\n",
    "director.builder['year_instance_occurrence_matrix'] = YearPaperOccurrenceMatrixBuilder(director)\n",
    "director.builder['year_instance_occurrence_matrix'].build()\n",
    "\n",
    "# year_instance_occurrence_matrix, year_papers = create_year_paper_occurrence_matrix(\n",
    "#     papers_metadata, paper_instance_occurrence_matrix, papers\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Complete\n",
    "\n",
    "We now have:\n",
    "\n",
    "| Variable                          | Type    | Size         | Comments |\n",
    "|-----------------------------------|---------|--------------|----------|\n",
    "| error_instances                   | list    | 165          | Comments |\n",
    "| error_matrix                      | ndarray | (999, 165)   | Comments |\n",
    "| error_papers                      | list    | 999          | Comments |\n",
    "| gap_too_large_threshold           | int     | n.a.         | Comments |\n",
    "| instance_piece_gap                | dict    | 151          | Comments |\n",
    "| instance_types_dicts              | dict    | 5            | Comments |\n",
    "| instances                         | list    | 315          | Comments |\n",
    "| paper_full_text                   | dict    | 1029         | Comments |\n",
    "| paper_instance_occurrence_matrix  | ndarray | (1003, 315)  | Comments |\n",
    "| papers                            | list    | 1003         | Comments |\n",
    "| pos_in_paper                      | dict    | 1003         | Comments |\n",
    "\n",
    "Consisting of:\n",
    "* The paper_instance_occurrence_matrix, binary listing if a term (instance) is present in a paper\n",
    "  * papers x instances\n",
    "* The error_matrix, of all instances that were dropped from the paper_instance_occurrence_matrix\n",
    "  * error_papers x error_instances\n",
    "\n",
    "And some leftover variables:\n",
    "* instance_types_dicts, listing all instance types (\"process\", \"software\", ...) and their respective instance sets (\"Curation\", \"Knowledge Work\", ...)\n",
    "* paper_full_text, containing each papers full text\n",
    "  * pos_in_paper, listing for each paper: for each instance: each position of that instance in that papers full text.\n",
    "* instance_piece_gap, a dict listing all instances made up from compound words (e.g. \"Knowledge Work\", and their minimum distance in each papers full text)\n",
    "  * gap_too_large_threshold, defining how far appart a finding of \"Knowledge\" and \"Work\" would qualify as \"Knowledge Work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~3 min | {( len(papers) * len(instances) ) / (3 * 1000) }seconds  compare proximity of all instances with one antoher\n",
    "# ~8 min right now.\n",
    "# 3 min 30 sec with 164 papers and 339 instances\n",
    "class ProximityMatrixBuilder(MatrixBuilder):\n",
    "    def __init__(self, director:Director, instances = None, papers = None, pos_in_paper = None, mode = \"sqrt\"):\n",
    "        super().__init__(director)\n",
    "\n",
    "        self.instances:dict[str,Instance] = instances or director.instances\n",
    "        self.papers:dict[str,Instance] = papers or director.papers\n",
    "        self.pos_in_paper:PosInPaper = pos_in_paper or director.pos_in_paper\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "    def build_matrix(self, instances = None, papers = None, pos_in_paper = None):\n",
    "        instances = instances or self.instances\n",
    "        papers = papers or self.papers\n",
    "        pos_in_paper = pos_in_paper or self.pos_in_paper\n",
    "\n",
    "        # self.matrix, self.proximity_instances = calculate_proximity_matrix(\n",
    "        #     self.config, pos_in_paper, instances, mode=\"sqrt\"\n",
    "        # )\n",
    "\n",
    "    def build(self):\n",
    "        self.build_matrix()\n",
    "        self.remove_zeros()\n",
    "        self.instances = self.handle_deletions(self.instances)\n",
    "\n",
    "    @time_function\n",
    "    def build_matrix(self,\n",
    "        # config: Config,\n",
    "        # pos_in_paper: PosInPaper,\n",
    "        # instances,\n",
    "        # mode=\"sqrt\",\n",
    "        try_to_save_time=False,\n",
    "    ):\n",
    "        # TODO: Optimize this function.\n",
    "        # each instance needs to have it's occurrences as pieces clustered together, so that only those below max distance are considered\n",
    "\n",
    "        # create a np zeros matrix of size instances x instances\n",
    "        indexed_instances = {instance: i for i, instance in enumerate(self.instances)}\n",
    "\n",
    "        self.matrix = np.zeros(\n",
    "            (len(self.instances), len(self.instances)), dtype=float\n",
    "        )\n",
    "\n",
    "        # alternatives are:\n",
    "        # \"sqrt\" - 1 / (square root of the distance)\n",
    "        # \"linear\" - 1 / distance\n",
    "        # \"binary\" - 1 if distance < MAX_GAP_THRESHOLD, 0 otherwise\n",
    "        # \"log\" - 1 / log(distance)\n",
    "\n",
    "        # There is a chance that pos_in_paper papers and instances are out of sync with the current papers and instances\n",
    "        paperIDs = [\n",
    "            paperID for paperID, name in enumerate(pos_in_paper.papers) if name in self.papers\n",
    "        ]\n",
    "        lID_map = {\n",
    "            indexed_instances[name]: instanceID\n",
    "            for instanceID, name in enumerate(pos_in_paper.literals)\n",
    "            if name in self.instances\n",
    "        }\n",
    "\n",
    "        for id1 in range(len(self.instances)):\n",
    "            # print (f\"Processing {id1} of {len(instances)}: {instance1}\")\n",
    "            for id2 in range(id1 + 1, len(self.instances)):\n",
    "                # FIXME: this resulted in a matrix which was not symmetric.\n",
    "                # That hints at a problem with the calclulation, [id1][id2] and [id2][id1] should be the same\n",
    "                wcID = pos_in_paper.word_combination_index_literal_literal[lID_map[id1]][\n",
    "                    lID_map[id2]\n",
    "                ]\n",
    "                for paperID in paperIDs:\n",
    "                    distance = pos_in_paper.find_min_distance_by_id(paperID, wcID)\n",
    "\n",
    "                    if distance < 0:\n",
    "                        # print(f\"Error: {instance1} and {instance2} not found in {paper}\")\n",
    "                        continue\n",
    "                    result = 0.0\n",
    "                    if distance == 0:\n",
    "                        result = 1\n",
    "                    elif distance == 1:\n",
    "                        result = 1\n",
    "                    elif self.mode == \"sqrt\":\n",
    "                        result = 1 / np.sqrt(distance)\n",
    "                    elif self.mode == \"linear\":\n",
    "                        result = 1 / distance\n",
    "                    elif self.mode == \"binary\":\n",
    "                        result = 1 if distance < config.gap_too_large_threshold else 0\n",
    "                    elif self.mode == \"log\":\n",
    "                        result = 1 / np.log(distance)\n",
    "                    else:\n",
    "                        print(\"Error: unknown mode\")\n",
    "                        break\n",
    "                    if result > 0.0:\n",
    "                        self.matrix[id1][id2] += result\n",
    "                        self.matrix[id2][id1] += result\n",
    "\n",
    "        # TODO rest doesnt seem to work, short fix implemented:\n",
    "        # create a copy of labels that only contains instances that are in the proximity matrix\n",
    "\n",
    "        # instance_instance_proximity_matrix, deletions = remove_zeros(\n",
    "        #     instance_instance_proximity_matrix\n",
    "        # )\n",
    "        # proximity_instances = handle_deletions(instances, deletions, rows=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "director.pos_in_paper = pos_in_paper\n",
    "director.builder['proximity_matrix'] = ProximityMatrixBuilder(director)\n",
    "director.builder['proximity_matrix'].build()\n",
    "# instance_instance_proximity_matrix, proximity_instances = calculate_proximity_matrix(\n",
    "#     config, pos_in_paper, instances\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "\n",
    "def get_rules(matrix, columns):\n",
    "    # AttributeError: 'numpy.ndarray' object has no attribute 'dtypes'\n",
    "    dataframe = pd.DataFrame(matrix, columns=columns).astype(bool)\n",
    "\n",
    "    # for each process:\n",
    "    # create one res\n",
    "\n",
    "    res = apriori(dataframe, min_support=0.4, use_colnames=True, max_len=2)\n",
    "\n",
    "    # visualize res\n",
    "    res = res.sort_values(by=\"support\", ascending=False)\n",
    "    res = res.reset_index(drop=True)\n",
    "    # res\n",
    "\n",
    "    rules = association_rules(res)\n",
    "    # sort rules by confidence\n",
    "    # rules = rules.sort_values(by='confidence', ascending=False)\n",
    "    rules = rules.sort_values(by=\"lift\", ascending=False)  # (propably most important)\n",
    "    # rules = rules.sort_values(by='leverage', ascending=False)\n",
    "    # export rules to csv\n",
    "    return rules\n",
    "\n",
    "\n",
    "rules = get_rules(director.builder[\"occurrence_matrix\"].matrix, list(director.instances.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules\n",
    "process_dataframe(config, rules, \"rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_cross_type_rules(rules, director:Director):\n",
    "    cross_type = [False] * len(rules)\n",
    "\n",
    "    for i, antecentent in enumerate(rules.antecedents):\n",
    "        if not isinstance(antecentent, str):\n",
    "            (antecentent,) = antecentent\n",
    "        consequent = rules.iloc[i].consequents\n",
    "        if not isinstance(consequent, str):\n",
    "            (consequent,) = consequent\n",
    "        type1, type2 = None, None\n",
    "        type1 = director.instances.get(antecentent, {}).get(\"instance_of\", [None])[0]\n",
    "        type2 = director.instances.get(consequent, {}).get(\"instance_of\", [None])[0]\n",
    "        # for type in director.classes:\n",
    "        #     if antecentent in instance_types_dicts[type]:\n",
    "        #         type1 = type\n",
    "        #     if consequent in instance_types_dicts[type]:\n",
    "        #         type2 = type\n",
    "        #     if type1 and type2:\n",
    "        #         break\n",
    "        if type1 and type2 and type1 != type2:\n",
    "            cross_type[i] = True\n",
    "            # print(rules.iloc[i])\n",
    "\n",
    "    # create a copy for all rules that are cross type\n",
    "    rules_cross_type = rules[cross_type].copy()\n",
    "    return rules_cross_type\n",
    "\n",
    "\n",
    "rules_cross_type = identify_cross_type_rules(rules, director)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_dataframe(config:Config, input_df, name = \"some_df\", path=None):\n",
    "#     if path is None:\n",
    "#         path = config.get_output_path()\n",
    "#     filepath = os.path.join(path, name)\n",
    "\n",
    "#     # convert all froensets to strings\n",
    "#     for col in input_df.columns:\n",
    "#         if isinstance(col[0], frozenset):\n",
    "#             # input_df[col] = input_df[col].apply(lambda x: \"_\".join(x))\n",
    "#             # input_df[col] = input_df[col].apply(lambda x: \"_\".join(x))\n",
    "#             input_df[col] = input_df[col].apply(lambda x: x + \"_HI!\")\n",
    "#             pass\n",
    "\n",
    "#     input_df.to_csv(filepath + '.csv', sep=config.csv_separator, decimal=config.csv_decimal)\n",
    "#     show(input_df)\n",
    "\n",
    "# rules_cross_type = identify_cross_type_rules(rules)\n",
    "\n",
    "process_dataframe(config, rules_cross_type, \"rules_cross_type\")\n",
    "# cross_type_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_done = False\n",
    "\n",
    "\n",
    "def print_kg_dict(config: Config, kg_dict, header):\n",
    "    filepath = os.path.join(config.get_output_path(), \"instance_relations.csv\")\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(header + \"\\n\")\n",
    "        total_comma = len(kg_dict) - 1\n",
    "        for pos1, type1 in enumerate(kg_dict):\n",
    "            preamble = \",\" * pos1\n",
    "            for pos2, type2 in enumerate(kg_dict[type1]):\n",
    "                intermediate = \",\" * (pos2 + 1)\n",
    "                rest_comma = \",\" * (total_comma - pos1 - pos2)\n",
    "                for i1, i2 in kg_dict[type1][type2]:\n",
    "                    f.write(preamble + i1 + intermediate + i2 + rest_comma + \"\\n\")\n",
    "\n",
    "\n",
    "def knowledge_graph_population_cross_type_rules(\n",
    "    config: Config, rules: association_rules, instance_types_dicts\n",
    "):\n",
    "    header = config.csv_separator.join(instance_types_dicts.keys())\n",
    "    # Triangular dict\n",
    "    dummy_dict = {}\n",
    "    for instance_type in instance_types_dicts:\n",
    "        dummy_dict[instance_type] = {}\n",
    "        for type in instance_types_dicts:\n",
    "            if type not in dummy_dict:\n",
    "                dummy_dict[instance_type][type] = []\n",
    "    for i, antecentent in enumerate(rules.antecedents):\n",
    "        (antecentent,) = antecentent\n",
    "        (consequent,) = rules.iloc[i].consequents\n",
    "        first_type = None\n",
    "        second_type = None\n",
    "        for type in instance_types_dicts:\n",
    "            if antecentent in instance_types_dicts[type]:\n",
    "                # type1 = type\n",
    "                if not first_type:\n",
    "                    first_type = type\n",
    "                    first_instance = antecentent\n",
    "                else:\n",
    "                    second_type = type\n",
    "                    second_instance = antecentent\n",
    "            if consequent in instance_types_dicts[type]:\n",
    "                if not first_type:\n",
    "                    first_type = type\n",
    "                    first_instance = consequent\n",
    "                else:\n",
    "                    second_type = type\n",
    "                    second_instance = consequent\n",
    "            if first_type and second_type:\n",
    "                break\n",
    "        if first_type != second_type:\n",
    "            dummy_dict[first_type][second_type].append(\n",
    "                (first_instance, second_instance)\n",
    "            )\n",
    "\n",
    "    print_kg_dict(config, dummy_dict, header)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "## Disabled. likely not needed anymore\n",
    "# try:\n",
    "#     kg_done = knowledge_graph_population_cross_type_rules(\n",
    "#         config, rules_cross_type, director\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     if config.debug:\n",
    "#         raise e\n",
    "#     else:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare csv file again\n",
    "# process,software,data item,data model,data format specification,interchange format,data visualization,data validation,inference,source\n",
    "\n",
    "\n",
    "@time_function\n",
    "def knowledge_graph_population(\n",
    "    config: Config,\n",
    "    instance_types_dicts,\n",
    "    property_types_dicts,\n",
    "    instance_instance_proximity_matrix,\n",
    "    proximity_instances,\n",
    "):\n",
    "    columns = list(instance_types_dicts.keys())\n",
    "    # columns += list(property_types_dicts.keys())\n",
    "    # columns = ['process', 'software', 'data item', 'data model', 'data format specification', 'data visualization', 'data validation', 'inference']\n",
    "\n",
    "    rows = []\n",
    "    for c_ID, column in enumerate(columns):\n",
    "        for instance in instance_types_dicts[column]:\n",
    "            # add the instance to the csv with each of their relations\n",
    "            if instance not in proximity_instances:\n",
    "                continue\n",
    "            instance_index = proximity_instances.index(instance)\n",
    "            for oc_ID, other_column in enumerate(columns):\n",
    "                if other_column not in instance_types_dicts:\n",
    "                    if other_column in property_types_dicts:\n",
    "                        # TODO: handle properties specially\n",
    "                        continue\n",
    "                    continue\n",
    "                if other_column != column:\n",
    "                    other_column_instances = instance_types_dicts[other_column]\n",
    "                    for other_instance in other_column_instances:\n",
    "                        if other_instance not in proximity_instances:\n",
    "                            continue\n",
    "                        other_instance_index = proximity_instances.index(other_instance)\n",
    "                        if (\n",
    "                            instance_instance_proximity_matrix[instance_index][\n",
    "                                other_instance_index\n",
    "                            ]\n",
    "                            > config.proximity_min_value\n",
    "                        ):\n",
    "                            # build row column by column\n",
    "                            row = [\"\"] * len(columns)\n",
    "                            row[c_ID] = instance\n",
    "                            row[oc_ID] = other_instance\n",
    "                            rows.append(row)\n",
    "\n",
    "    # write to csv\n",
    "    filepath = os.path.join(config.get_output_path(), \"instance_relations.csv\")\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(config.csv_separator.join(columns) + \"\\n\")\n",
    "        for row in rows:\n",
    "            f.write(config.csv_separator.join(row) + \"\\n\")\n",
    "    return True\n",
    "\n",
    "## Disabled. likely not needed anymore\n",
    "# if not kg_done:\n",
    "#     kg_done = knowledge_graph_population(\n",
    "#         config,\n",
    "#         instance_types_dicts,\n",
    "#         property_types_dicts,\n",
    "#         instance_instance_proximity_matrix,\n",
    "#         proximity_instances,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "import types\n",
    "\n",
    "\n",
    "# process,software,data item,data model,data format specification,interchange format,data visualization,data validation,inference,source\n",
    "# process,software,data item,data model,data format specification\n",
    "\n",
    "\n",
    "def save_as_owl(config: Config, path=None):\n",
    "    onto_path = config.ontology_path\n",
    "    df_cl = pd.read_csv(os.path.join(onto_path, \"classes.csv\"))\n",
    "    df_re = pd.read_csv(os.path.join(onto_path, \"relations.csv\"))\n",
    "    df_re = df_re.set_index(\"Domain\\Range\")\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    data_path = os.path.join(path, \"instance_relations.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    # df = pd.read_csv('data.csv')\n",
    "\n",
    "    onto = get_ontology(\"http://tib.eu/slr\")\n",
    "\n",
    "    df_contributions = pd.read_csv(\n",
    "        os.path.join(path, \"paper_instance_occurrence_matrix.csv\")\n",
    "    )\n",
    "    df_rules = pd.read_csv(os.path.join(path, \"rules_cross_type.csv\"))\n",
    "\n",
    "    with open(os.path.join(path, \"instance_types_dicts.json\")) as file:\n",
    "        inst_data = json.load(file)\n",
    "\n",
    "    onto = get_ontology(\"http://tib.eu/slr\")\n",
    "\n",
    "    with onto:\n",
    "\n",
    "        # Classes\n",
    "        for ind, row in df_cl.iterrows():\n",
    "            cl = types.new_class(row[\"URI\"], (Thing,))\n",
    "            cl.label = row[\"Label\"]\n",
    "            re = types.new_class(\n",
    "                f'has{row[\"Label\"].title().replace(\" \", \"\")}', (ObjectProperty,)\n",
    "            )\n",
    "            re.label = f'has {row[\"Label\"]}'\n",
    "\n",
    "        # Instances\n",
    "        for key, value in inst_data.items():\n",
    "            cl = onto.search_one(label=key)\n",
    "            if cl:\n",
    "                for item in value:\n",
    "                    inst = cl()\n",
    "                    inst.label = item\n",
    "\n",
    "        # Statements\n",
    "        Contribution = types.new_class(\"Contribution\", (Thing,))\n",
    "        mentions = types.new_class(\"mentions\", (ObjectProperty,))\n",
    "        mentions.label = \"mentions\"\n",
    "        for ind, row in df_contributions.iterrows():\n",
    "            contrib_inst = Contribution()\n",
    "            contrib_inst.label = row[0]\n",
    "            for col in df_contributions.columns:\n",
    "                if row[col]:\n",
    "                    inst = onto.search_one(label=col)\n",
    "                    if inst:\n",
    "                        contrib_inst.mentions.append(inst)\n",
    "\n",
    "        # Rules\n",
    "        for ind, row in df_rules.iterrows():\n",
    "            subj_inst = onto.search_one(label=row[\"antecedents\"])\n",
    "            obj_inst = onto.search_one(label=row[\"consequents\"])\n",
    "            if subj_inst and obj_inst:\n",
    "                obj_cl = obj_inst.is_a[0]\n",
    "                rel_label = f\"has {str(obj_cl.label[0])}\"\n",
    "                rel = onto.search_one(label=rel_label)\n",
    "                if rel:\n",
    "                    rel[subj_inst].append(obj_inst)\n",
    "\n",
    "    output_path = os.path.join(onto_path, \"onto.owl\")\n",
    "    # onto.save('onto.owl')\n",
    "    onto.save(output_path)\n",
    "    onto.destroy()\n",
    "\n",
    "\n",
    "# save_as_owl(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for ORKG\n",
    "# header:\n",
    "# paper:title,paper:authors,paper:publication_month,paper:publication_year,paper:published_in,paper:research_field,paper:doi,paper:url,contribution:research_problem,contribution:extraction_method,Property 1,Property 2\n",
    "\n",
    "\n",
    "def flatten_nested_properties(data, pefix=\"\"):\n",
    "    res = {}\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            res.update(flatten_nested_properties(value, f\"{pefix}{key}:\"))\n",
    "        else:\n",
    "            res[f\"{pefix}{key}\"] = value\n",
    "    return res\n",
    "\n",
    "\n",
    "class Paper:\n",
    "    order = [\n",
    "        \"title\",\n",
    "        \"authors\",\n",
    "        \"publication_month\",\n",
    "        \"publication_year\",\n",
    "        \"published_in\",\n",
    "        \"research_field\",\n",
    "        \"doi\",\n",
    "        \"url\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, paperID, data={}):\n",
    "        self.paperID: str = paperID\n",
    "        self.title: str = data.get(\"title\", \"\")\n",
    "        ## now handled later\n",
    "        # if self.title:\n",
    "        #     self.title = '\"' + self.title + '\"'\n",
    "        if self.title and \"{\" in self.title or \"}\" in self.title:\n",
    "            self.title = self.title.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "        self.authors: list[str] = data.get(\"author\", \"\")\n",
    "        if isinstance(self.authors, str):\n",
    "            authors = self.authors.split(\"and \")\n",
    "            if authors:\n",
    "                for i, author in enumerate(authors):\n",
    "                    name = author.split(\",\")\n",
    "                    if len(name) > 1:\n",
    "                        name = f\"{name[1].strip()} {name[0].strip()}\"\n",
    "                    else:\n",
    "                        name = name[0].strip()\n",
    "                    authors[i] = name\n",
    "            self.authors = \"; \".join(authors)\n",
    "        self.publication_month: int = data.get(\"publication_month\", \"\")\n",
    "        self.publication_year: int = data.get(\"year\", \"\")\n",
    "        self.published_in = \"\"\n",
    "        for key in [\"journal\", \"conference\", \"journal\"]:\n",
    "            if key in data:\n",
    "                self.published_in = data[key]\n",
    "                break\n",
    "        self.research_field: str = data.get(\"research_field\", \"\")\n",
    "        if not self.research_field:\n",
    "            # TODO: find a way to get the research field\n",
    "            self.research_field = \"R195\"\n",
    "        self.doi: str = data.get(\"doi\", \"\")\n",
    "        self.url: str = data.get(\"url\", \"\")\n",
    "\n",
    "\n",
    "class Contribution:\n",
    "    def __init__(self, paperID, properties={}):\n",
    "        self.paperID: str = paperID\n",
    "        self.properties: dict = flatten_nested_properties(properties)\n",
    "\n",
    "\n",
    "class ORKGComparison:\n",
    "    def __init__(self):\n",
    "        self.papers = {}  # paperID:paper data\n",
    "        self.contibutions: dict[str:Contribution] = {}  # paperID:contribution data\n",
    "        self.properties = {}\n",
    "\n",
    "    def populate(\n",
    "        self,\n",
    "        config: Config,\n",
    "        papers,\n",
    "        instances,\n",
    "        instance_types_dicts,\n",
    "        paper_instance_occurrence_matrix,\n",
    "        papers_metadata,\n",
    "    ):\n",
    "        # Create a dictionary to hold the count of existing values for each property\n",
    "        property_ranges = {\n",
    "            property: sum(\n",
    "                value in instances for value in values\n",
    "            )  # Count how many values exist in 'instances'\n",
    "            for property, values in instance_types_dicts.items()  # Iterate over each property and its values\n",
    "        }\n",
    "        # floor = 0\n",
    "        # for prop, value in property_ranges.items():\n",
    "        #     property_ranges[prop] += floor\n",
    "        #     floor += value\n",
    "\n",
    "        for paperID, paper in enumerate(papers):\n",
    "            paper_data = papers_metadata.get(paper, {})\n",
    "            self.papers[paperID] = Paper(paper, paper_data)\n",
    "            properties = {prop: [] for prop in property_ranges}\n",
    "            floor = 0\n",
    "            for prop, prop_range in property_ranges.items():\n",
    "                for i in range(floor, floor + prop_range):\n",
    "                    if paper_instance_occurrence_matrix[paperID][i] == 1:\n",
    "                        properties[prop].append(instances[i])\n",
    "                floor += prop_range\n",
    "            self.contibutions[paperID] = Contribution(paper, properties)\n",
    "        return self\n",
    "\n",
    "    def populate_properties(self):\n",
    "        for contribution in self.contibutions.values():\n",
    "            for prop, value in contribution.properties.items():\n",
    "                len_values = len(value) if isinstance(value, list) else 1\n",
    "                if prop not in self.properties or self.properties[prop] < len_values:\n",
    "                    self.properties[prop] = len_values\n",
    "        return self.properties\n",
    "\n",
    "    def get(self, key):\n",
    "        if key == \"properties\" and not self.properties:\n",
    "            self.populate_properties()\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def save(self, config: Config, path=None, name=\"orkg_comparison\"):\n",
    "        if path is None:\n",
    "            path = config.orkg_path\n",
    "        filepath = os.path.join(path, name)\n",
    "        if not filepath.endswith(\".csv\"):\n",
    "            filepath += \".csv\"\n",
    "\n",
    "        rows = []\n",
    "        row = [\"paper:\" + prop for prop in Paper.order]\n",
    "        for prop, count in self.get(\"properties\").items():\n",
    "            # row += [f\"contribution:{prop}\"] * count\n",
    "            row += [prop] * count\n",
    "        rows.append(row)\n",
    "\n",
    "        for paperID, contribution in self.contibutions.items():\n",
    "            paper = self.papers[paperID]\n",
    "            row = [getattr(paper, key, \"\") for key in Paper.order]\n",
    "            for prop, count in self.properties.items():\n",
    "                value = contribution.properties.get(prop, \"\")\n",
    "                if not isinstance(value, list):\n",
    "                    value = [value]\n",
    "                len_taken = len(value)\n",
    "                if len_taken < count:\n",
    "                    value += [\"\"] * (count - len_taken)\n",
    "                row += value\n",
    "            rows.append(row)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for row in rows:\n",
    "                for id, item in enumerate(row):\n",
    "                    if config.csv_separator in item:\n",
    "                        if item.startswith('\"') and item.endswith('\"'):\n",
    "                            continue\n",
    "                        row[id] = '\"' + item + '\"'\n",
    "                f.write(config.csv_separator.join(row) + \"\\n\")\n",
    "\n",
    "\n",
    "# orkg_comparison = ORKGComparison()\n",
    "# orkg_comparison.populate(\n",
    "#     config,\n",
    "#     papers,\n",
    "#     instances,\n",
    "#     instance_types_dicts,\n",
    "#     paper_instance_occurrence_matrix,\n",
    "#     papers_metadata,\n",
    "# )\n",
    "# orkg_comparison.save(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, builder in director.builder.items():\n",
    "    if not hasattr(builder, \"matrix\"):\n",
    "        continue\n",
    "    if name in [\"year_instance_occurrence_matrix\"]:\n",
    "        continue\n",
    "    rows = []\n",
    "    candidates = [\"papers\", \"instances\"]\n",
    "    for candidate in candidates:\n",
    "        if hasattr(builder, candidate):\n",
    "            rows = getattr(builder, candidate)\n",
    "            break\n",
    "    if not rows:\n",
    "        raise Exception(f\"Could not find rows for {name}\")\n",
    "    if isinstance(rows, dict):\n",
    "        rows = list(rows.keys())\n",
    "    \n",
    "    cols = []\n",
    "    candidates = [\"instances\", \"literals\"]\n",
    "    for candidate in candidates:\n",
    "        if hasattr(builder, candidate):\n",
    "            cols = getattr(builder, candidate)\n",
    "            break\n",
    "    if not cols:\n",
    "        raise Exception(f\"Could not find cols for {name}\")\n",
    "    if isinstance(cols, dict):\n",
    "        cols = list(cols.keys())\n",
    "    \n",
    "    builder.save()\n",
    "    # process_matrix(director.config, builder.matrix, rows, cols, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.visualize = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_list(config, instances, \"instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Dicts: instance_types_dicts, papers_metadata, instance_piece_gap\n",
    "# process_dict(config, instance_types_dicts, \"instance_types_dicts\")\n",
    "# process_dict(config, papers_metadata, \"papers_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     paper_instance_occurrence_matrix,\n",
    "#     rows=papers,\n",
    "#     columns=instances,\n",
    "#     name=\"paper_instance_occurrence_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     error_matrix,\n",
    "#     rows=error_papers,\n",
    "#     columns=error_instances,\n",
    "#     name=\"error_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     instance_instance_co_occurrence_matrix,\n",
    "#     rows=instances,\n",
    "#     columns=instances,\n",
    "#     name=\"instance_instance_co_occurrence_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     instance_instance_relative_co_occurrence_matrix,\n",
    "#     rows=instances,\n",
    "#     columns=instances,\n",
    "#     name=\"instance_instance_relative_co_occurrence_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     instance_instance_proximity_matrix,\n",
    "#     rows=proximity_instances,\n",
    "#     columns=proximity_instances,\n",
    "#     name=\"instance_instance_proximity_matrix\",\n",
    "#     instance_types_dicts=instance_types_dicts,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "## Pre-Processing\n",
    "Using Completion Rating in %\n",
    "\n",
    "### 80 %: Full Text extraction\n",
    "* lacking noise removal (Headings, page numbers, ...)\n",
    "* lacking line-break mending\n",
    "\n",
    "### 100 %: Bag of Words\n",
    "* The problem with BoW that the words are looked at seperatly and correlation is not really clear.\n",
    "\n",
    "\n",
    "### 99 %: TF-IDF\n",
    "* tf-idf only on terms\n",
    "\n",
    "### ? %: Part Of Speech (POS) Tagging, Named Entity Recognition (NER) \n",
    "* ready, but not used currently\n",
    "\n",
    "## Visualize\n",
    "\n",
    "### 85 % Matrix\n",
    "* CSV and Dataframe dumps work fine\n",
    "* Visualization as PNG or SVG are extremely large.\n",
    "  * DPI regulation works to somewhat keep this in check, but images still reach 20 MB\n",
    "* An interactive matrix would be preferred.\n",
    "  * If you hover on a cell, it shows you the x and y label and it's value.\n",
    "\n",
    "### 100 % Timeline\n",
    "* arrange the papers on a timeline and identify the flow of:\n",
    "  * Processes\n",
    "  * File formats\n",
    "  * software\n",
    "  * ...\n",
    "* Additional ideas:\n",
    "  * Compare this to goolge trends\n",
    "\n",
    "### GraphDB\n",
    "* Visualize\n",
    "\n",
    "## Future Work\n",
    "Using Difficulty ranked (DR) solutions:\n",
    "\n",
    "### Step 0: Look it up\n",
    "\n",
    "#### Wikidata linking & more\n",
    "* https://openrefine.org/\n",
    "\n",
    "#### More visualization\n",
    "* https://github.com/JasonKessler/scattertext \n",
    "* https://pypi.org/project/yellowbrick/\n",
    "\n",
    "#### NLP Pipelines:\n",
    "https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "\n",
    "#### BLAST: Basic Local Alignment Search Tool\n",
    "  * starting point: https://academic.oup.com/bioinformatics/article/39/12/btad716/7450067\n",
    "\n",
    "#### AMIE 3\n",
    "  * https://luisgalarraga.de/docs/amie3.pdf\n",
    "  * https://github.com/dig-team/amie\n",
    "\n",
    "### Step 1: Low hanging fruits\n",
    "\n",
    "#### 1/5 DR: multi-word detection (n-gram)\n",
    "Tools:  nltk, spaCy, etc.\n",
    "\n",
    "### Step 2: Not-to-tricky follow-up\n",
    "\n",
    "#### 3/5 DR: Acronym Expansion\n",
    "Tools: spaCy - https://spacy.io/universe/project/neuralcoref\n",
    "\n",
    "#### 3/5 DR: CoReference resolution\n",
    "Tools: spaCy - https://spacy.io/universe/project/neuralcoref or https://huggingface.co/coref/ (you can use the model out of the box)\n",
    "\n",
    "### Step 3: Vector-magic\n",
    "\n",
    "#### 2-4/5 DR: Word embedding\n",
    "* Find out, that jpeg and png are similar\n",
    "\n",
    "(depending on your needs) - Tools: gensim - https://www.analyticsvidhya.com/blog/2023/07/step-by-step-guide-to-word2vec-with-gensim/\n",
    "\n",
    "#### 3/5 DR: document embedding\n",
    "Tools: gensim - https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "I would also check graph embeddings, sentence embeddings, and recently there is LLM2Vec\n",
    "\n",
    "### Step 3.1: Reaping the vector-rewards\n",
    "\n",
    "#### 1/5 DR: clustering\n",
    "Tools: sklearn\n",
    "\n",
    "Requirements: Need to have data as numbers first. This is quite possible after generating embeddings\n",
    "\n",
    "### Step 9: Won't be happening in this paper\n",
    "* Paper classes\n",
    "* Subclasses of paper classes\n",
    "* model which process is a subprocess of another process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
