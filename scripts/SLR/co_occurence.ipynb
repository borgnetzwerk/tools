{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Instance occurrence in Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_config import Config\n",
    "\n",
    "# Usage\n",
    "config = Config(for_git=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itables import init_notebook_mode, show\n",
    "\n",
    "# if not config.for_git:\n",
    "#     init_notebook_mode(all_interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_export import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_ontology import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_builder import *\n",
    "director = PaperInstanceDirector(config)\n",
    "# director.set(\"ontology\", ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no new Zotero export at G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR:\n",
      "There should be a folder called 'files'\n",
      "We now have 1035 PDFs stored at G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR\\00_PDFs\n",
      "1024 out of 1028 paper instances have metadata.\n",
      "get_metadata executed in 22.88017225265503 seconds\n"
     ]
    }
   ],
   "source": [
    "# Extract Paper Metadata\n",
    "director.get_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "director.reduce_to_reviewed_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: find occurrences of instances in bag of words of papers\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class InstanceBuilder(Builder):\n",
    "    def __init__(self, director: Director):\n",
    "        super().__init__(director)\n",
    "\n",
    "    def build(self):\n",
    "        instance_types_dicts = {}\n",
    "\n",
    "        # paper_instance_occurrence_matrix = np.zeros((), dtype=int)\n",
    "\n",
    "        # TODO: Delete first 2 lines and see why this throws error then\n",
    "        instance_types_dicts = self.csv_to_dict_of_sets(config.csv_file, config)\n",
    "\n",
    "        # Extract instance types that are actually property types\n",
    "        instance_types_dicts, property_types_dicts = self.prune_properties(\n",
    "            instance_types_dicts, properties_to_prune=self.config.properties\n",
    "        )\n",
    "\n",
    "        return instance_types_dicts\n",
    "\n",
    "    def preprocess_csv(self, csv_file, config: Config, writeback=True):\n",
    "        with open(csv_file, \"r\", encoding=\"utf8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        expected_columns = len(lines[0].split(config.csv_separator))\n",
    "        processed_lines = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line.count('\"') % 2 == 1:\n",
    "                # if the number of quotes is odd, the line is not complete\n",
    "                error = True\n",
    "                for j in range(i + 1, len(lines)):\n",
    "                    line = line + \" \" + lines[j].strip()\n",
    "                    if line.count('\"') % 2 == 0:\n",
    "                        error = False\n",
    "                        print(f\"Merged rows {i} to {j}\")\n",
    "                        i = j + 1\n",
    "                        break\n",
    "                if error:\n",
    "                    raise Exception(\n",
    "                        f\"Error: Lines {i} to {j-1} could not be processed. Odd number of quotes.\"\n",
    "                    )\n",
    "            else:\n",
    "                i += 1\n",
    "            if '\"\"' in line:\n",
    "                # remove quotes from line\n",
    "                line = line.replace('\"\"', \"\")\n",
    "            if line.count('\"') % 2 == 0:\n",
    "                pos1 = line.find('\"')\n",
    "                while pos1 != -1:\n",
    "                    pos2 = line.find('\"', pos1 + 1)\n",
    "                    if not config.csv_separator in line[pos1:pos2]:\n",
    "                        line = line[pos1:pos2] + line[pos2 + 1 :]\n",
    "                    pos1 = line.find('\"', pos2 + 1)\n",
    "            processed_lines.append(line)\n",
    "        if writeback and len(processed_lines) != len(lines) or lines != processed_lines:\n",
    "            print(\n",
    "                f\"CSV file improved, found {len(lines) - len(processed_lines)} errors.\"\n",
    "            )\n",
    "            with open(csv_file, \"w\", encoding=\"utf8\") as f:\n",
    "                f.write(\"\\n\".join(processed_lines))\n",
    "\n",
    "        return processed_lines\n",
    "\n",
    "    def csv_to_dict_of_sets(self, csv_file, config: Config, prune_nan=True):\n",
    "        dict_of_sets = {}\n",
    "        # try:\n",
    "        #     df = pd.read_csv(csv_file)\n",
    "        # except pd.errors.ParserError:\n",
    "        #     print(\"Error parsing CSV file. Trying again with 'error_bad_lines=False'\")\n",
    "        # TODO: Specify modular separator and decimal here as well\n",
    "\n",
    "        self.preprocess_csv(csv_file, config)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                csv_file,\n",
    "                on_bad_lines=\"warn\",\n",
    "                delimiter=config.csv_separator,\n",
    "                encoding=\"utf-8\",\n",
    "            )\n",
    "        except:\n",
    "            print(\"Error parsing CSV file. Trying again with 'encoding=ISO-8859-1'\")\n",
    "            df = pd.read_csv(\n",
    "                csv_file,\n",
    "                on_bad_lines=\"warn\",\n",
    "                delimiter=config.csv_separator,\n",
    "                encoding=\"ISO-8859-1\",\n",
    "            )\n",
    "        for column in df.columns:\n",
    "            if df[column].isnull().all():\n",
    "                print(f\"column {column} is empty\")\n",
    "                dict_of_sets[column] = set([np.nan])\n",
    "            else:\n",
    "                dict_of_sets[column] = set(df[column].str.lower())\n",
    "            if prune_nan and np.nan in dict_of_sets[column]:\n",
    "                dict_of_sets[column].remove(np.nan)\n",
    "\n",
    "            # Will be added in next version\n",
    "            deletes = []\n",
    "            adds = []\n",
    "            for original_entry in dict_of_sets[column]:\n",
    "                entry = original_entry.strip()\n",
    "                while entry.startswith('\"'):\n",
    "                    entry = entry[1:]\n",
    "                while entry.endswith('\"'):\n",
    "                    entry = entry[:-1]\n",
    "                if original_entry != entry:\n",
    "                    deletes.append(original_entry)\n",
    "                    if entry not in dict_of_sets[column]:\n",
    "                        adds.append(entry)\n",
    "            for entry in deletes:\n",
    "                dict_of_sets[column].remove(entry)\n",
    "            dict_of_sets[column].update(adds)\n",
    "\n",
    "        # saved_column = df['process'] #you can also use df['column_name']\n",
    "        # delete all that exists in two or more columns\n",
    "        for key in dict_of_sets:\n",
    "            for other_key in dict_of_sets:\n",
    "                if key != other_key:\n",
    "                    dict_of_sets[key] = dict_of_sets[key].difference(\n",
    "                        dict_of_sets[other_key]\n",
    "                    )\n",
    "        return dict_of_sets\n",
    "\n",
    "    def prune_properties(\n",
    "        self,\n",
    "        instance_types_dicts,\n",
    "        properties_to_prune=[],\n",
    "        prune_empty=True,\n",
    "        prune_x=True,\n",
    "    ):\n",
    "        properties = {}\n",
    "\n",
    "        # merge \"interchange format\" into \"data format specification\"\n",
    "        if \"interchange format\" in instance_types_dicts:\n",
    "            pruned = []\n",
    "            for key in instance_types_dicts[\"interchange format\"]:\n",
    "                if len(key) > 1:\n",
    "                    instance_types_dicts[\"data format specification\"].add(key)\n",
    "                    pruned.append(key)\n",
    "            for key in pruned:\n",
    "                instance_types_dicts[\"interchange format\"].remove(key)\n",
    "\n",
    "        for instance_type in instance_types_dicts:\n",
    "            prune = False\n",
    "            if instance_type in properties_to_prune:\n",
    "                prune = True\n",
    "            elif prune_empty and len(instance_types_dicts[instance_type]) == 0:\n",
    "                # prune empty sets\n",
    "                prune = True\n",
    "            elif prune_x and len(max(instance_types_dicts[instance_type], key=len)) < 2:\n",
    "                # prune sets with only one character entries\n",
    "                prune = True\n",
    "\n",
    "            if prune:\n",
    "                properties[instance_type] = instance_types_dicts[instance_type]\n",
    "\n",
    "        for instance_type in properties:\n",
    "            instance_types_dicts.pop(instance_type)\n",
    "\n",
    "        return instance_types_dicts, properties\n",
    "\n",
    "    # # ---------------------- Variables ----------------------\n",
    "\n",
    "    # ## instances: A list of all instances, regardless of their type\n",
    "    # # first all type 1, then all type 2, etc.\n",
    "    # # if possible, instance sare ordered by their occurrence\n",
    "\n",
    "    # ## instances_dicts: A dictionary of all different types (columns) of instances\n",
    "    # #\n",
    "    # # types:\n",
    "    # #  - process\n",
    "    # #  - software\n",
    "    # #  - data item\n",
    "    # #  - data model\n",
    "    # #  - data format specification\n",
    "    # #  - interchange format\n",
    "    # #  - source\n",
    "    # #\n",
    "    # # instances_dicts['process']: A set of all instances of the type 'process'\n",
    "    # #\n",
    "\n",
    "    # instance_types_dicts = {}\n",
    "\n",
    "    # ## paper_nlp_dict: A dictionary of all papers and their NLP data (as dict)\n",
    "\n",
    "    # ## occurrences: A matrix of binary occurrences of instances in papers\n",
    "    # #\n",
    "    # # rows: papers\n",
    "    # # columns: instances\n",
    "    # # cells: 1 if instance is present in paper, 0 otherwise\n",
    "    # #\n",
    "    # paper_instance_occurrence_matrix = np.zeros((), dtype=int)\n",
    "\n",
    "    # # ---------------------- Main ----------------------\n",
    "\n",
    "    # # Usage example\n",
    "\n",
    "    # # TODO: Delete first 2 lines and see why this throws error then\n",
    "    # instance_types_dicts = csv_to_dict_of_sets(config.csv_file, config)\n",
    "\n",
    "    # # Extract instance types that are actually property types\n",
    "    # instance_types_dicts, property_types_dicts = prune_properties(\n",
    "    #     instance_types_dicts, properties_to_prune=config.properties\n",
    "    # )\n",
    "\n",
    "    # def get_instances_list(instance_types_dicts):\n",
    "    #     instances = []\n",
    "    #     # merge all sets into one set\n",
    "    #     for instance_type in instance_types_dicts:\n",
    "    #         instances += instance_types_dicts[instance_type]\n",
    "    #     return instances\n",
    "\n",
    "    # # instances = get_instances_list(instance_types_dicts)\n",
    "\n",
    "\n",
    "director.get_instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualize co-occurrences\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import math\n",
    "import pandas as pd\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "class MatrixExporter(Builder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        director: Director,\n",
    "        matrix: np.ndarray,\n",
    "        rows: List[str],\n",
    "        columns: List[str] = None,\n",
    "        name: str = \"some_matrix\",\n",
    "        format=\".png\",\n",
    "        path=None,\n",
    "    ):\n",
    "        self.director = director\n",
    "        self.config = director.config\n",
    "        self.matrix = matrix\n",
    "        self.rows = rows\n",
    "        self.columns = (\n",
    "            columns or rows\n",
    "        )  # if columns are not given, assume symmetric matrix\n",
    "        self.name: str = name\n",
    "        self.format: str = format\n",
    "        self.output_path: str = path or self.config.get_output_path()\n",
    "        self.visualization_path = self.config.get_output_path(visualization=True)\n",
    "        self.mode = \"sqrt\"\n",
    "\n",
    "        self.instances = None\n",
    "\n",
    "        self.size_x = None\n",
    "        self.size_y = None\n",
    "        self.dpi = None\n",
    "\n",
    "    def save(self):\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        self.to_csv()\n",
    "        if config.visualize:\n",
    "            self.visualize_matrix(self.config, self.matrix, self.rows, self.columns, self.name, path=self.visualization_path)\n",
    "            # if instance_types_dicts:\n",
    "            if self.instances:\n",
    "                self.sankey(\n",
    "                    self.config, self.matrix, self.rows, self.name + \"_sankey\", path=self.visualization_path\n",
    "                )\n",
    "                if self.mode:\n",
    "                    self.visualize_matrix_graph(\n",
    "                        self.config,\n",
    "                        self.matrix,\n",
    "                        self.rows,\n",
    "                        # instance_types_dicts,\n",
    "                        self.name + \"_graph\",\n",
    "                        path=self.visualization_path,\n",
    "                        node_size_mode=self.config.proximity_mode,\n",
    "                    )\n",
    "                else:\n",
    "                    self.visualize_matrix_graph(\n",
    "                        self.config,\n",
    "                        self.matrix,\n",
    "                        self.rows,\n",
    "                        # instance_types_dicts,\n",
    "                        self.name + \"_graph\",\n",
    "                        path=self.visualization_path,\n",
    "                    )\n",
    "\n",
    "    def to_csv(self):\n",
    "        df = pd.DataFrame(self.matrix, columns=self.columns, index=self.rows)\n",
    "        filepath = os.path.join(self.output_path, self.name)\n",
    "        df.to_csv(\n",
    "            filepath + \".csv\",\n",
    "            sep=self.config.csv_separator,\n",
    "            decimal=self.config.csv_decimal,\n",
    "        )\n",
    "\n",
    "    def get_figsize(self):\n",
    "        if self.size_x and self.size_y:\n",
    "            return self.size_x, self.size_y\n",
    "        ## Calculate the maximum size of the plot\n",
    "        dpi = 300\n",
    "        max_dpi = 600\n",
    "        if config.for_git:\n",
    "            dpi = 96\n",
    "            max_dpi = 200\n",
    "        max_pixel = 2**16  # Maximum size in any direction\n",
    "        max_size = max_pixel / dpi  # Maximum size in any direction\n",
    "        max_size_total = max_size * max_size  # Maximum size in total\n",
    "        max_size_total *= 0.05  # produce smaller files\n",
    "\n",
    "        # Experience value of space required per cell\n",
    "        factor = 0.18\n",
    "        size_x: float = 2 + len(self.columns) * factor\n",
    "        size_y: float = 3 + len(self.rows) * 0.8 * factor\n",
    "\n",
    "        while size_x * size_y < max_size_total and dpi < max_dpi:\n",
    "            dpi /= 0.95\n",
    "            max_size_total *= 0.95\n",
    "\n",
    "        if dpi > max_dpi:\n",
    "            dpi = max_dpi\n",
    "\n",
    "        while size_x * size_y > max_size_total:\n",
    "            dpi *= 0.95\n",
    "            max_size_total /= 0.95\n",
    "\n",
    "        self.size_x = size_x\n",
    "        self.size_y = size_y\n",
    "        self.dpi = dpi\n",
    "\n",
    "        return size_x, size_y\n",
    "\n",
    "    def visualize_matrix(self,                         \n",
    "            config: Config,\n",
    "            matrix: np.ndarray,\n",
    "            rows: list[str],\n",
    "            columns: list[str] = None,\n",
    "            name: str = \"some_matrix\",\n",
    "            format=\".png\",\n",
    "            path=None,\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Visualizes a matrix as a heatmap.\n",
    "        matrix: The matrix to visualize\n",
    "        rows: The labels for the rows\n",
    "        columns: The labels for the columns\n",
    "        name: The name of the file to save\n",
    "        format: The format of the file to save (default: '.png', also accepts '.svg' and '.pdf', also accepts a list of formats)\n",
    "        \"\"\"\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=self.get_figsize(), dpi=self.dpi)\n",
    "\n",
    "        cax = ax.matshow(self.matrix, cmap=\"viridis\")\n",
    "\n",
    "        # use labels from instance_occurrences\n",
    "        ax.set_xticks(range(len(self.columns)))\n",
    "        ax.set_xticklabels(list(self.columns), fontsize=10, rotation=90)\n",
    "        ax.set_yticks(range(len(self.rows)))\n",
    "        ax.set_yticklabels(list(self.rows), fontsize=10)\n",
    "\n",
    "        # # adjust the spacing between the labels\n",
    "        # plt.gca().tick_params(axis='x', which='major', pad=15)\n",
    "        # plt.gca().tick_params(axis='y', which='major', pad=15)\n",
    "\n",
    "        # show the number of co-occurrences in each cell, if greater than 0\n",
    "        for i in range(len(self.matrix)):\n",
    "            for j in range(len(self.matrix[i])):\n",
    "                if self.matrix[i, j] == 0:\n",
    "                    continue\n",
    "                # if co_occurrences[i, j] > 100:\n",
    "                #     continue\n",
    "\n",
    "                # make sure the text is at most 3 digits and a dot\n",
    "                decimals = 2\n",
    "                if self.matrix[i, j] > 99:\n",
    "                    decimals = 0\n",
    "                elif self.matrix[i, j] > 9:\n",
    "                    decimals = 1\n",
    "                cell_text = round(self.matrix[i, j], decimals)\n",
    "                if decimals == 0:\n",
    "                    cell_text = int(cell_text)\n",
    "                plt.text(\n",
    "                    j, i, cell_text, ha=\"center\", va=\"center\", color=\"white\", fontsize=4\n",
    "                )\n",
    "\n",
    "        # plt.show()\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # title\n",
    "        plt.title(self.name)\n",
    "\n",
    "        if isinstance(format, list):\n",
    "            for f in format:\n",
    "                if f[0] != \".\":\n",
    "                    f = \".\" + f\n",
    "                filepath = os.path.join(path, name + f)\n",
    "                fig.savefig(filepath)\n",
    "        else:\n",
    "            if format[0] != \".\":\n",
    "                format = \".\" + format\n",
    "            filepath = os.path.join(self.visualization_path, self.name + format)\n",
    "            fig.savefig(filepath)\n",
    "\n",
    "    def visualize_matrix_graph(self,\n",
    "        config: Config,\n",
    "        matrix,\n",
    "        instances,\n",
    "        # instance_types_dicts,\n",
    "        name=\"some_matrix_graph\",\n",
    "        path=None,\n",
    "        node_size_mode=\"sqrt\",\n",
    "        raise_mode=\"prune\",\n",
    "        ):\n",
    "        if not self.instances:\n",
    "            return\n",
    "\n",
    "        SEED = config.proximity_seed or 17\n",
    "        K_SPRRING = config.proximity_k_spring or 18\n",
    "        MIN_VALUE = config.proximity_min_value or 0.01\n",
    "\n",
    "        scale = len(instances) * 0.12\n",
    "        # Create a new figure\n",
    "        x = scale / 10 * 16\n",
    "        y = scale / 10 * 9\n",
    "        fig = plt.figure(figsize=(x, y))\n",
    "\n",
    "        # normalize the proximity matrix\n",
    "        matrix = matrix / matrix.max()\n",
    "\n",
    "        # Make sure the matrix is not completely stretched out\n",
    "        if matrix.min() < MIN_VALUE:\n",
    "            if raise_mode == \"prune\":\n",
    "                # remove every value that is below MIN_VALUE\n",
    "                matrix = np.where(matrix < MIN_VALUE, 0, matrix)\n",
    "            elif raise_mode == \"sqrt\":\n",
    "                while np.min(matrix[np.nonzero(matrix)]) < MIN_VALUE:\n",
    "                    matrix = np.sqrt(matrix)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown raise mode\")\n",
    "\n",
    "        # alternatives are:\n",
    "        # \"linear\" - take proximity as is\n",
    "        # \"sqrt\" - sqrt(proximity)\n",
    "        # \"log\" - log(proximity)\n",
    "        if node_size_mode == \"log\":\n",
    "            # TODO: see how this works with log(1)\n",
    "            nodesize_map = [\n",
    "                np.log(matrix[:, i].sum() + 1) for i in range(len(instances))\n",
    "            ]\n",
    "        elif node_size_mode == \"sqrt\":\n",
    "            nodesize_map = [np.sqrt(matrix[:, i].sum()) for i in range(len(instances))]\n",
    "        elif node_size_mode == \"linear\":\n",
    "            nodesize_map = [matrix[:, i].sum() for i in range(len(instances))]\n",
    "        else:\n",
    "            nodesize_map = [matrix[:, i].sum() for i in range(len(instances))]\n",
    "\n",
    "        # print(max(nodesize_map))\n",
    "        # print(min(nodesize_map))\n",
    "\n",
    "        nodesize_map = np.array(nodesize_map) / max(nodesize_map) * 1000\n",
    "\n",
    "        # print(max(nodesize_map))\n",
    "        # print(min(nodesize_map))\n",
    "\n",
    "        # Create a graph from the proximity matrix\n",
    "        G = nx.from_numpy_array(matrix)\n",
    "\n",
    "        # Specify the layout\n",
    "        pos = nx.spring_layout(\n",
    "            G, seed=SEED, k=K_SPRRING / math.sqrt(G.order())\n",
    "        )  # Seed for reproducibility\n",
    "\n",
    "        color_map = []\n",
    "\n",
    "        color = {\n",
    "            \"process\": \"#1f77b4\",  # muted blue\n",
    "            \"software\": \"#ff7f0e\",  # safety orange\n",
    "            \"data item\": \"#2ca02c\",  # cooked asparagus green\n",
    "            \"data model\": \"#d62728\",  # brick red\n",
    "            \"data format specification\": \"#9467bd\",  # muted purple\n",
    "            # \"interchange format\": \"#8c564b\",  # chestnut brown\n",
    "            # \"source\": \"#e377c2\",  # raspberry yogurt pink\n",
    "        }\n",
    "\n",
    "        for instance in instances:\n",
    "            added = False\n",
    "            for instance_type in instance_types_dicts:\n",
    "                if instance in instance_types_dicts[instance_type]:\n",
    "                    color_map.append(color[instance_type])\n",
    "                    added = True\n",
    "                    break\n",
    "            if not added:\n",
    "                color_map.append(\"grey\")\n",
    "\n",
    "        # Draw the graph\n",
    "        options = {\n",
    "            \"edge_color\": \"grey\",\n",
    "            \"linewidths\": 0.5,\n",
    "            \"width\": 0.5,\n",
    "            \"with_labels\": True,  # This will add labels to the nodes\n",
    "            \"labels\": {i: label for i, label in enumerate(instances)},\n",
    "            \"node_color\": color_map,\n",
    "            \"node_size\": nodesize_map,\n",
    "            # \"edge_color\": \"white\",\n",
    "            # \"alpha\": 0.9,\n",
    "        }\n",
    "\n",
    "        # print(nx.is_weighted(G))\n",
    "\n",
    "        # nx.set_edge_attributes(G, values = 1, name = 'weight')\n",
    "\n",
    "        nx.draw(G, pos, **options, ax=fig.add_subplot(111))\n",
    "\n",
    "        # Make the graph more spacious\n",
    "        fig.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.9)\n",
    "\n",
    "        # Create a patch for each color\n",
    "        patches = [mpatches.Patch(color=color[key], label=key) for key in color]\n",
    "\n",
    "        # Add the legend to the graph\n",
    "        plt.legend(handles=patches, loc=\"upper right\", fontsize=\"x-large\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # save plot to file\n",
    "        filepath = os.path.join(path, name)\n",
    "        fig.savefig(filepath + \".png\")\n",
    "        fig.savefig(filepath + \".svg\")\n",
    "\n",
    "        # nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "    def sankey(self,\n",
    "        config: Config,\n",
    "        matrix,\n",
    "        instances,\n",
    "        instance_types_dicts,\n",
    "        name=\"some_sankey\",\n",
    "        path=None,\n",
    "    ):\n",
    "        # TODO: Implement a method to create one graph per Process\n",
    "        # if path is None:\n",
    "        #     path = config.get_output_path(path, visualization=True)\n",
    "        # Convert the proximity matrix into a list of source nodes, target nodes, and values\n",
    "        sources = []\n",
    "        targets = []\n",
    "        values = []\n",
    "\n",
    "        x_pos = [0] * len(instances)\n",
    "        y_pos = [0] * len(instances)\n",
    "        color_map = [0] * len(instances)\n",
    "\n",
    "        max_types = len(instance_types_dicts)\n",
    "        type_positions = [0.1 + (i / max_types) * 0.8 for i in range(max_types)]\n",
    "\n",
    "        color = {\n",
    "            \"process\": \"#1f77b4\",  # muted blue\n",
    "            \"software\": \"#ff7f0e\",  # safety orange\n",
    "            \"data item\": \"#2ca02c\",  # cooked asparagus green\n",
    "            \"data model\": \"#d62728\",  # brick red\n",
    "            \"data format specification\": \"#9467bd\",  # muted purple\n",
    "            \"interchange format\": \"#8c564b\",  # chestnut brown\n",
    "            # \"source\": \"#e377c2\",  # raspberry yogurt pink\n",
    "        }\n",
    "        color = list(color.values())\n",
    "\n",
    "        space = {}\n",
    "\n",
    "        for i in range(matrix.shape[0]):\n",
    "            source_type = None\n",
    "\n",
    "            for j in range(matrix.shape[1]):\n",
    "                target_type = None\n",
    "\n",
    "                for type_depth, type in enumerate(instance_types_dicts):\n",
    "                    if instances[i] in instance_types_dicts[type]:\n",
    "                        source_type = type_depth\n",
    "                    if instances[j] in instance_types_dicts[type]:\n",
    "                        target_type = type_depth\n",
    "\n",
    "                # only keep directly forward moving connections\n",
    "                if target_type - source_type != 1:\n",
    "                    continue\n",
    "\n",
    "                # only keep forward moving connections\n",
    "                if target_type - source_type <= 0:\n",
    "                    continue\n",
    "\n",
    "                if source_type not in space:\n",
    "                    space[source_type] = {}\n",
    "                if i not in space[source_type]:\n",
    "                    space[source_type][i] = 0\n",
    "                space[source_type][i] += matrix[i][j]\n",
    "\n",
    "                if target_type not in space:\n",
    "                    space[target_type] = {}\n",
    "                if j not in space[target_type]:\n",
    "                    space[target_type][j] = 0\n",
    "                space[target_type][j] += matrix[i][j]\n",
    "\n",
    "                x_pos[i] = type_positions[source_type]\n",
    "                x_pos[j] = type_positions[target_type]\n",
    "                color_map[i] = color[source_type]\n",
    "                color_map[j] = color[target_type]\n",
    "                if matrix[i][j] > 0.0:  # Ignore zero values\n",
    "                    sources.append(i)\n",
    "                    targets.append(j)\n",
    "                    values.append(matrix[i][j])\n",
    "\n",
    "        for type in space:\n",
    "            sum_values = sum(space[type].values())\n",
    "            space[type] = {\n",
    "                k: v / sum_values\n",
    "                for k, v in sorted(\n",
    "                    space[type].items(), key=lambda item: item[1], reverse=True\n",
    "                )\n",
    "            }\n",
    "\n",
    "        # assign each instance a proper y position\n",
    "        for type in space:\n",
    "            bottom = 0.1\n",
    "            for i, instance in enumerate(space[type]):\n",
    "                y_pos[instance] = bottom\n",
    "                bottom += space[type][instance] * 0.8\n",
    "\n",
    "        nodes = dict(\n",
    "            # pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color=\"black\", width=0.5),\n",
    "            label=instances,\n",
    "            color=color_map,\n",
    "            x=x_pos,\n",
    "            y=y_pos,\n",
    "            align=\"right\",\n",
    "        )\n",
    "\n",
    "        # Create a Sankey diagram\n",
    "        fig = go.Figure(\n",
    "            data=[\n",
    "                go.Sankey(\n",
    "                    node=nodes, link=dict(source=sources, target=targets, value=values)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        fig.update_layout(width=1920, height=1080)\n",
    "\n",
    "        fig.update_layout(title_text=\"Sankey Diagram\", font_size=10)\n",
    "        # fig.show()\n",
    "\n",
    "        filepath = os.path.join(self.path, self.name)\n",
    "        fig.write_image(filepath + \".png\")\n",
    "        fig.write_image(filepath + \".svg\")\n",
    "        fig.write_html(filepath + \".html\")\n",
    "\n",
    "class MatrixBuilder(Builder):\n",
    "    def __init__(self, director: Director):\n",
    "        super().__init__(director)\n",
    "\n",
    "        self.matrix: np.ndarray = None\n",
    "        self.row_labels = [] # papers\n",
    "        self.col_labels = [] # instances\n",
    "\n",
    "        self.deletions = [np.array([]), np.array([])]\n",
    "        # TODO: Likely add more proerties to map from full corpus to reduced corpus\n",
    "\n",
    "    def save(self, name=\"some_matrix\"):\n",
    "        self.exporter:MatrixExporter = MatrixExporter(self.director, self.matrix, self.row_labels, self.col_labels, name)\n",
    "        self.exporter.save()\n",
    "\n",
    "    def remove_zeros(self, columns=True, rows=True):\n",
    "        # remove all columns that are all zeros\n",
    "        # setup deleted_columns empty at first:\n",
    "        deleted_columns, deleted_rows = self.deletions\n",
    "        if columns:\n",
    "            deleted_columns = np.all(self.matrix == 0, axis=0)\n",
    "            self.matrix = self.matrix[:, ~np.all(self.matrix == 0, axis=0)]\n",
    "\n",
    "        # remove all rows that are all zeros\n",
    "        if rows:\n",
    "            deleted_rows = np.all(self.matrix == 0, axis=1)\n",
    "            self.matrix = self.matrix[~np.all(self.matrix == 0, axis=1)]\n",
    "        self.deletions = [deleted_columns, deleted_rows]\n",
    "\n",
    "    def handle_deletions(self, input, deletions=None, rows=True):\n",
    "        \"\"\"\n",
    "        input: list, dict or np.ndarray\n",
    "        deletions: list of bools\n",
    "        rows: if True, deletions[1] is used, else deletions[0]\n",
    "        \"\"\"\n",
    "        delID = 1 if rows else 0\n",
    "\n",
    "        if not deletions:\n",
    "            deletions = self.deletions\n",
    "\n",
    "        if deletions[delID].any():\n",
    "            # rows were deleted, in this case: papers\n",
    "            if isinstance(input, list):\n",
    "                input = [\n",
    "                    item for i, item in enumerate(input) if not deletions[delID][i]\n",
    "                ]\n",
    "            elif isinstance(input, dict):\n",
    "                input = {\n",
    "                    key: item\n",
    "                    for i, (key, item) in enumerate(input.items())\n",
    "                    if not deletions[delID][i]\n",
    "                }\n",
    "            elif isinstance(input, np.ndarray):\n",
    "                input = input[~deletions[delID]]\n",
    "        return input\n",
    "\n",
    "    def reorder_matrix(self, new_order, cols=True):\n",
    "        if cols:\n",
    "            self.matrix = self.matrix[:, new_order]\n",
    "        else:\n",
    "            self.matrix = self.matrix[new_order, :]\n",
    "        self.remove_zeros()\n",
    "        # this isn't quite right anymore\n",
    "        # self.papers = self.handle_deletions(self.papers, self.deletions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OccurrenceMatrixBuilder(MatrixBuilder):\n",
    "    def __init__(self, director: PaperInstanceDirector, papers=None, instances=None):\n",
    "        super().__init__(director)\n",
    "        self.director: PaperInstanceDirector = director\n",
    "\n",
    "        \n",
    "        self.papers = papers or self.director.papers  # first dimension\n",
    "        self.instances = instances or self.director.instances  # second dimension\n",
    "\n",
    "        self.row_labels = self.get_labels(rows=True)\n",
    "        self.col_labels = self.get_labels(rows=False)\n",
    "    \n",
    "    def get_labels(self, rows=True):\n",
    "        if rows:\n",
    "            return list(self.papers.keys())\n",
    "        return list(self.instances.keys())\n",
    "\n",
    "    def count_occurrences(self, papers, instances):\n",
    "        papers = papers or self.papers\n",
    "        instances = instances or self.instances\n",
    "\n",
    "        occurrences = np.zeros((len(papers), len(instances)), dtype=int)\n",
    "\n",
    "        for p, paperpath in enumerate(papers.values()):\n",
    "            if isinstance(paperpath, dict) or isinstance(paperpath, Instance):\n",
    "                paperpath = paperpath.get(\"nlp_path\", None)\n",
    "            with open(paperpath, \"r\", encoding=\"utf8\") as f:\n",
    "                paper = json.load(f)\n",
    "                for i, instance in enumerate(instances):\n",
    "                    present = True\n",
    "                    pieces = split_string(instance)\n",
    "                    for piece in pieces:\n",
    "                        if piece.lower() not in paper[\"bag_of_words\"]:\n",
    "                            present = False\n",
    "                            break\n",
    "\n",
    "                    # if instance == \"system integration\":\n",
    "                    #     if \"Liu und Hu - 2013 - A reuse oriented representation model for capturin\" in paperpath:\n",
    "                    #         print(present)\n",
    "                    if present:\n",
    "                        occurrences[p][i] = 1\n",
    "        return occurrences\n",
    "\n",
    "    def build(self, papers=None, instances=None):\n",
    "        self.matrix = self.count_occurrences(papers, instances)\n",
    "        # process_matrix(\n",
    "        #     self.config,\n",
    "        #     self.matrix,\n",
    "        #     list(self.papers.keys()),\n",
    "        #     list(self.instances.keys()),\n",
    "        #     name=\"paper_instance_occurrence_matrix\",\n",
    "        # )\n",
    "        # return self.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorMatrixBuilder(MatrixBuilder):\n",
    "    def __init__(self, director: Director, pos_in_paper: PosInPaper):\n",
    "        super().__init__(director)\n",
    "\n",
    "        self.row_labels = list(director.papers.keys())\n",
    "        self.col_labels = list(director.instances.keys())\n",
    "        self.pos_in_paper = pos_in_paper\n",
    "        self.paper_full_text = director.paper_full_text\n",
    "        self.paper_instance_occurrence_matrix = director.builder[\n",
    "            \"occurrence_matrix\"\n",
    "        ].matrix\n",
    "\n",
    "    @time_function\n",
    "    def build_matrix(self):\n",
    "        # self.matrix = find_instance_piece_gap(\n",
    "        # self.config,\n",
    "        # self.papers,\n",
    "        # self.paper_full_text,\n",
    "        # self.literals,\n",
    "        # self.paper_instance_occurrence_matrix,\n",
    "        # self.pos_in_paper,\n",
    "        # )\n",
    "        # def find_instance_piece_gap(\n",
    "        #     config: Config,\n",
    "        #     papers,\n",
    "        #     paper_full_text,\n",
    "        #     instances,\n",
    "        #     paper_instance_occurrence_matrix,\n",
    "        #     pos_in_paper: PosInPaper,\n",
    "        # ):\n",
    "        self.matrix = np.zeros(self.paper_instance_occurrence_matrix.shape, dtype=float)\n",
    "        for paperID, paper in enumerate(self.row_labels):\n",
    "            if paperID % 100 == 0:\n",
    "                # print(f\"Processing paper {paperID} of {len(papers)}\")\n",
    "                pass\n",
    "            if paper in self.paper_full_text:\n",
    "                for i, instance in enumerate(self.col_labels):\n",
    "                    if self.paper_instance_occurrence_matrix[paperID][i] == 0:\n",
    "                        continue\n",
    "                    wcID = self.pos_in_paper.word_combination_index_literal[instance]\n",
    "                    # TODO: handle if that instance has no word combination index entry\n",
    "                    if wcID is None:\n",
    "                        # word has no distance\n",
    "                        continue\n",
    "                    min_distance = self.pos_in_paper.find_min_distance_by_id(paperID, wcID)\n",
    "                    if min_distance is None:\n",
    "                        pass\n",
    "                    if min_distance > config.gap_too_large_threshold:\n",
    "                        # print(f\"Gap for {instance} in {paper} ({min_distance} > {GAP_TOO_LARGE_THRESHOLD})\")\n",
    "                        self.paper_instance_occurrence_matrix[paperID][i] = 0\n",
    "                        # get log base 10 of min distance\n",
    "                        self.matrix[paperID][i] = round(np.log10(min_distance), 1)\n",
    "\n",
    "                    # Some pieces may not be found in the full text\n",
    "                    if min_distance == -1:\n",
    "                        # print(f\"{instance} not found in {paper} at all\")\n",
    "                        self.paper_instance_occurrence_matrix[paperID][i] = 0\n",
    "                        self.matrix[paperID][i] = min_distance\n",
    "                        # for these, we do not store the gap\n",
    "                        continue\n",
    "\n",
    "        # return error_matrix\n",
    "\n",
    "    def build(self):\n",
    "        self.build_matrix()\n",
    "\n",
    "        self.remove_zeros()\n",
    "        # error_matrix, has_error = remove_zeros(error_matrix)\n",
    "\n",
    "        self.row_labels = self.handle_deletions(self.row_labels)\n",
    "        self.col_labels = self.handle_deletions(self.col_labels, rows=False)\n",
    "\n",
    "        # process_matrix(\n",
    "        #     self.config,\n",
    "        #     self.matrix,\n",
    "        #     self.papers,\n",
    "        #     self.literals,\n",
    "        #     \"error_matrix\",\n",
    "        # )\n",
    "        # paper_instance_occurrence_matrix, instances, deletions = update_instances(\n",
    "        #     paper_instance_occurrence_matrix, literals, instance_types_dicts\n",
    "        # )\n",
    "\n",
    "        # papers = handle_deletions(papers, deletions)\n",
    "\n",
    "        # free unneeded memory\n",
    "        # del deletions, has_error\n",
    "\n",
    "\n",
    "# instance_instance_co_occurrence_matrix = np.dot(\n",
    "#     paper_instance_occurrence_matrix.T, paper_instance_occurrence_matrix\n",
    "# )\n",
    "\n",
    "# error_matrix = find_instance_piece_gap(\n",
    "#     config,\n",
    "#     papers,\n",
    "#     paper_full_text,\n",
    "#     literals,\n",
    "#     paper_instance_occurrence_matrix,\n",
    "#     pos_in_paper,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status: Instances and Papers setup\n",
    "\n",
    "## Next: Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "class WikiData:\n",
    "    queries_done = 0\n",
    "    new_labels = 0\n",
    "    new_entries = 0\n",
    "\n",
    "    def print_updates():\n",
    "        print(f\"Queries done: {WikiData.queries_done}\")\n",
    "        print(f\"New labels: {WikiData.new_labels}\")\n",
    "        print(f\"New entries: {WikiData.new_entries}\")\n",
    "\n",
    "    def __init__(self, config: Config = None):\n",
    "        self.entries = {}\n",
    "        self.label_entry_map = {}\n",
    "        if config:\n",
    "            self.load(config)\n",
    "\n",
    "    def save(self, config, path=None, name=\"wikidata.json\"):\n",
    "        if not path:\n",
    "            path = config.ontology_path\n",
    "        if not name.endswith(\".json\"):\n",
    "            name += \".json\"\n",
    "        filepath = os.path.join(path, name)\n",
    "        with open(filepath, \"w\", encoding=\"utf8\") as f:\n",
    "            data = self.__dict__\n",
    "            json.dump(data, f)\n",
    "\n",
    "    def load(self, config, path=None, name=\"wikidata.json\"):\n",
    "        if not path:\n",
    "            path = config.ontology_path\n",
    "        if not name.endswith(\".json\"):\n",
    "            name += \".json\"\n",
    "        filepath = os.path.join(path, name)\n",
    "        if not os.path.exists(filepath):\n",
    "            return\n",
    "        with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "            for key, value in data.items():\n",
    "                setattr(self, key, value)\n",
    "\n",
    "    def query_wikidata(\n",
    "        self, config: Config, label: str, select=\"label\", limit=None, nested=False\n",
    "    ):\n",
    "        if select == \"label\" and not nested:\n",
    "            if WikiData.queries_done > config.wikidata_query_limit:\n",
    "                print(\"Wikidata query limit reached.\")\n",
    "                WikiData.print_updates()\n",
    "                return False\n",
    "            else:\n",
    "                WikiData.queries_done += 1\n",
    "\n",
    "        def transform_results(results):\n",
    "            transformed = {}\n",
    "            for result in results:\n",
    "                item_uri = result[\"item\"][\"value\"]\n",
    "                item_label = result[\"itemLabel\"][\"value\"]\n",
    "                # Handle cases where altLabels or description might not be present\n",
    "                alt_labels = result.get(\"altLabels\", {}).get(\"value\", \"\")\n",
    "                description = result.get(\"description\", {}).get(\"value\", \"\")\n",
    "\n",
    "                transformed[item_uri] = {\n",
    "                    \"itemLabel\": item_label,\n",
    "                    \"altLabels\": alt_labels,\n",
    "                    \"description\": description,  # Include this line only if descriptions are desired\n",
    "                }\n",
    "            return transformed\n",
    "\n",
    "        if not limit:\n",
    "            limit = config.wikidata_query_limit\n",
    "        endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "        selection = {\n",
    "            \"label\": f'?item rdfs:label \"{label}\"@en.',\n",
    "            \"altLabel\": f'?item skos:altLabel \"{label}\"@en.',\n",
    "        }\n",
    "\n",
    "        # query = f\"\"\"\n",
    "        # SELECT ?item ?itemLabel (GROUP_CONCAT(DISTINCT ?altLabel; separator = \", \") AS ?altLabels) WHERE {{\n",
    "        # {selection[select]}\n",
    "        # SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        # }}\n",
    "        # GROUP BY ?item ?itemLabel\n",
    "        # LIMIT {limit}\n",
    "        # \"\"\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT ?item ?itemLabel (GROUP_CONCAT(DISTINCT ?altLabel; separator = \", \") AS ?altLabels) \n",
    "        (SAMPLE(?description) AS ?description) WHERE {{\n",
    "        {selection[select]}\n",
    "        OPTIONAL {{ ?item skos:altLabel ?altLabel FILTER(LANG(?altLabel) = \"en\") }}\n",
    "        OPTIONAL {{ ?item schema:description ?description FILTER(LANG(?description) = \"en\") }}\n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        GROUP BY ?item ?itemLabel\n",
    "        LIMIT {limit}\n",
    "        \"\"\"\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": \"WDQS-example Python/%s.%s\"\n",
    "            % (requests.__version__, \"MyScript\"),\n",
    "            \"Accept\": \"application/json\",\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                endpoint_url, headers=headers, params={\"query\": query, \"format\": \"json\"}\n",
    "            )\n",
    "            response.raise_for_status()  # Raises stored HTTPError, if one occurred\n",
    "\n",
    "            data = response.json()\n",
    "            results = data[\"results\"][\"bindings\"]\n",
    "            results = transform_results(results)\n",
    "            if select == \"label\":\n",
    "                if len(results) < limit:\n",
    "                    results_altLabel = self.query_wikidata(\n",
    "                        config,\n",
    "                        label,\n",
    "                        select=\"altLabel\",\n",
    "                        limit=limit - len(results),\n",
    "                        nested=True,\n",
    "                    )\n",
    "                    results.update(results_altLabel)\n",
    "                if not nested:\n",
    "                    if len(results) < limit and label.lower() != label:\n",
    "                        results_lower = self.query_wikidata(\n",
    "                            config,\n",
    "                            label.lower(),\n",
    "                            select=select,\n",
    "                            limit=limit - len(results),\n",
    "                            nested=True,\n",
    "                        )\n",
    "                        results.update(results_lower)\n",
    "                    if len(results) < limit and label.capitalize() != label:\n",
    "                        results_capitalize = self.query_wikidata(\n",
    "                            config,\n",
    "                            label.capitalize(),\n",
    "                            select=select,\n",
    "                            limit=limit - len(results),\n",
    "                            nested=True,\n",
    "                        )\n",
    "                        results.update(results_capitalize)\n",
    "                    if len(results) < limit and label.upper() != label:\n",
    "                        results_upper = self.query_wikidata(\n",
    "                            config,\n",
    "                            label.upper(),\n",
    "                            select=select,\n",
    "                            limit=limit - len(results),\n",
    "                            nested=True,\n",
    "                        )\n",
    "                        results.update(results_upper)\n",
    "\n",
    "                    wikidata.label_entry_map[label] = list(results.keys())\n",
    "                    WikiData.new_labels += 1\n",
    "                    for key, value in results.items():\n",
    "                        if key not in wikidata.entries:\n",
    "                            wikidata.entries[key] = value\n",
    "                            WikiData.new_entries += 1\n",
    "            if results:\n",
    "                return results\n",
    "            else:\n",
    "                # print(\"No matching Wikidata entry found.\")\n",
    "                return []\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Query failed: {e}\")\n",
    "\n",
    "    def get_uri(self, config: Config, label: str, allow_query=True):\n",
    "        if label in self.label_entry_map:\n",
    "            return self.label_entry_map[label]\n",
    "        elif allow_query:\n",
    "            res = self.query_wikidata(config, label)\n",
    "            if res == False:\n",
    "                print(\"Could not get URI. Query limit reached.\")\n",
    "                return False\n",
    "            if res:\n",
    "                self.save(config)\n",
    "            return self.label_entry_map[label]\n",
    "\n",
    "\n",
    "wikidata = WikiData(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_function\n",
    "def query_wikidata_for_instances(\n",
    "    config: Config, ontology: Ontology, wikidata: WikiData, stop_at=None\n",
    "):\n",
    "    for instance in ontology.instances.values():\n",
    "        if instance.wikidata_uri:\n",
    "            continue\n",
    "        elif instance.wikidata_candidates:\n",
    "            continue\n",
    "\n",
    "        temp_res = []\n",
    "        check = [instance.label] + getattr(instance, \"also_known_as\", [])\n",
    "        i = 0\n",
    "        while i < len(check):\n",
    "            label = check[i]\n",
    "            res = wikidata.get_uri(config, label)\n",
    "            if res == False:\n",
    "                print(\"Could not query properly. Not saving this instance.\")\n",
    "                ontology.save(config)\n",
    "                return\n",
    "            else:\n",
    "                temp_res += res\n",
    "            i += 1\n",
    "        if temp_res:\n",
    "            # We found some results\n",
    "            instance.uri = temp_res\n",
    "        else:\n",
    "            instance.uri = -1\n",
    "    ontology.save(config)\n",
    "    WikiData.print_updates()\n",
    "\n",
    "# TODO: Rework wikidata querrying\n",
    "# query_wikidata_for_instances(config, ontology, wikidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from orkg import ORKG  # import base class from package\n",
    "\n",
    "# # https://orkg.readthedocs.io/en/latest/client/resources.html#getting-resources-by-lookup\n",
    "\n",
    "# orkg = ORKG(host=\"https://sandbox.orkg.org/\")  # create the connector to the ORKG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: implement ORKG interface\n",
    "# matlab_findings = orkg.resources.get(\n",
    "#     q=\"matlab\", exact=False, size=30, sort=\"label\", desc=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obsidian\n",
    "\n",
    "\n",
    "def fill_digits(var_s, cap=2):\n",
    "    \"\"\"Add digits until cap is reached\"\"\"\n",
    "    var_s = str(var_s)\n",
    "    while len(var_s) < cap:\n",
    "        var_s = \"0\" + var_s\n",
    "    return var_s\n",
    "\n",
    "\n",
    "def get_clean_title(title, eID=None, obsidian=False):\n",
    "    \"\"\"\n",
    "    Cleans a given title to make it suitable for filenames.\n",
    "\n",
    "    Parameters:\n",
    "    title (string): The title. \"episode1\"\n",
    "    eID (int): The position of that episode in the playlist.\n",
    "\n",
    "    Returns:\n",
    "    string: cleaned title for use as filename\n",
    "    \"\"\"\n",
    "    noFileChars = r'\":\\<>*?/'\n",
    "\n",
    "    replace_dict = {\n",
    "        \"–\": \"-\",\n",
    "        \"’\": \"´\",\n",
    "        \" \": \" \",\n",
    "    }\n",
    "    clean_title = title\n",
    "    for each in noFileChars:\n",
    "        clean_title = clean_title.replace(each, \"\")\n",
    "    for key, value in replace_dict.items():\n",
    "        clean_title = clean_title.replace(key, value)\n",
    "    if obsidian:\n",
    "        clean_title = clean_title.replace(\"#\", \"\")\n",
    "    if eID:\n",
    "        clean_title = fill_digits(eID, 3) + \"_\" + clean_title\n",
    "    return clean_title.strip()\n",
    "\n",
    "\n",
    "def clear_name(name, can_be_folder=True):\n",
    "    if can_be_folder:\n",
    "        clean_title = name.replace(\"/\", \"[SLASH]\")\n",
    "    else:\n",
    "        clean_title = name.replace(\"/\", \" or \")\n",
    "    clean_title = get_clean_title(clean_title, obsidian=True)\n",
    "    if can_be_folder:\n",
    "        clean_title = clean_title.replace(\"[SLASH]\", \"/\")\n",
    "    if clean_title.startswith(\"https\"):\n",
    "        clean_title = clean_title[5:]\n",
    "    while clean_title.startswith(\"/\"):\n",
    "        clean_title = clean_title[1:]\n",
    "    if clean_title.startswith(\"www.\"):\n",
    "        clean_title = clean_title[4:]\n",
    "    while clean_title.endswith(\"/\"):\n",
    "        clean_title = clean_title[:-1]\n",
    "    return clean_title\n",
    "\n",
    "\n",
    "def parse_link(text):\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1]\n",
    "    if text.startswith(\"[[\") and text.endswith(\"]]\"):\n",
    "        text = text[2:-2]\n",
    "    return text\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self, instance: KnowledgeGraphEntry = None, path: str = None, content: str = None\n",
    "    ):\n",
    "        self.instance: KnowledgeGraphEntry = instance\n",
    "        self.path = path if path else self.instance.get(\"path\", None) if self.instance else None\n",
    "        self.content = content if content else self.instance.get(\"content\", None) if self.instance else None\n",
    "\n",
    "        if path and not instance:\n",
    "            self.load()\n",
    "\n",
    "    def get(self, key, default=None):\n",
    "        if hasattr(self, key):\n",
    "            return getattr(self, key)\n",
    "        elif hasattr(self.instance, key):\n",
    "            return self.instance.get(key)\n",
    "        elif key == \"properties\":\n",
    "            return self.instance.__dict__\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "    def load(self, path=None):\n",
    "        if not path:\n",
    "            path = self.path\n",
    "        with open(self.path, \"r\", encoding=\"utf8\") as f:\n",
    "            content = f.read()\n",
    "            # read properties from yaml frontmatter\n",
    "            yaml_find = 0\n",
    "            lines = content.splitlines()\n",
    "            properties = {}\n",
    "            key = None\n",
    "            value = None\n",
    "            while lines:\n",
    "                line = lines.pop(0)\n",
    "                if line.startswith(\"---\"):\n",
    "                    # yaml frontmatter either begins or ends\n",
    "                    yaml_find += 1\n",
    "\n",
    "                elif yaml_find == 1:\n",
    "                    # inside yaml frontmatter\n",
    "                    # if \":\" in line:\n",
    "                    if not line.strip().startswith(\"-\") and \":\" in line:\n",
    "                        if key:\n",
    "                            properties[key] = value\n",
    "                        pieces = line.split(\":\")\n",
    "                        key = pieces[0].strip()\n",
    "                        value = \":\".join(pieces[1:]).strip()\n",
    "                        value = parse_link(value)\n",
    "                    else:\n",
    "                        # multiline value\n",
    "                        if not isinstance(value, list):\n",
    "                            if value:\n",
    "                                value = [value]\n",
    "                            else:\n",
    "                                value = []\n",
    "\n",
    "                        prefix = \" - \"\n",
    "                        line = line.replace(\n",
    "                            prefix, \"\"\n",
    "                        ).strip()  # TODO: remove [[ ]] from links\n",
    "                        line = parse_link(line)\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        else:\n",
    "                            value.append(line)\n",
    "\n",
    "                elif yaml_find == 2:\n",
    "                    # end of yaml frontmatter\n",
    "                    if key:\n",
    "                        properties[key] = value\n",
    "                    self.content = \"\\n\".join(lines)\n",
    "                    break\n",
    "            if properties:\n",
    "                self.instance = KnowledgeGraphEntryFactory.create(properties = properties)\n",
    "\n",
    "    def prep_for_yaml(self, text):\n",
    "        # this needs to be improved\n",
    "        mapping = {\n",
    "            # \"\\&\": \"&\",\n",
    "            \"\\\\&\": \"&\",\n",
    "            '\"': \"'quotationmark'\",\n",
    "            \"\\n\": \" 'newline' \",\n",
    "        }\n",
    "        if \"\\\\\" in text and \"/\" in text:\n",
    "            text = path_cleaning(text)\n",
    "        for key, value in mapping.items():\n",
    "            text = text.replace(key, value)\n",
    "        \n",
    "        if not text.startswith('\"') and not text.endswith('\"'):\n",
    "            text = '\"{}\"'.format(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def save(self, path=None):\n",
    "        # lists = [\n",
    "        #     \"layman term\",\n",
    "        #     \"subclass of\",\n",
    "        #     \"instance of\",\n",
    "        #     \"aliases\",\n",
    "        #     \"tags\",\n",
    "        #     \"wikidata candidates\",\n",
    "        #     \"orkg candidates\",\n",
    "        # ]\n",
    "        # data = self.convert_python_to_obsidian()\n",
    "        # if data:\n",
    "        if not path:\n",
    "            path = self.path\n",
    "        data = {}\n",
    "        for key, value in self.instance.__dict__.items():\n",
    "            # if key in [\"path\", \"content\"]:\n",
    "            #     continue\n",
    "            if value == None:\n",
    "                value = \"\"\n",
    "            # if key == \"wikidata ID\" and isinstance(value, list):\n",
    "            #     data[\"wikidata candidates\"] = value\n",
    "            #     value = \"\"\n",
    "            # elif key in lists:\n",
    "            #     if not isinstance(value, list):\n",
    "            #         value = [value]\n",
    "            # if key == \"instance of\":\n",
    "            if isinstance(value, str):\n",
    "                # if \"{\" in value or \"}\" in value:\n",
    "                value = self.prep_for_yaml(value)\n",
    "\n",
    "            if isinstance(value, list):\n",
    "                # TODO: improve this lookup\n",
    "                if key in [\"instance_of\", \"subclass_of\"]:\n",
    "                    value = ['\"[[{}]]\"'.format(val) for val in value]\n",
    "                value = \"\\n - \" + \"\\n - \".join(value)\n",
    "            elif isinstance(value, dict):\n",
    "                # value = \"\\n\".join([f\" - {k}: {v}\" for k, v in value.items()])\n",
    "                value = json.dumps(value)\n",
    "            data[key] = value\n",
    "\n",
    "        with open(path, \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(\"---\\n\")\n",
    "            for key, value in data.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "            f.write(\"---\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            if self.content:\n",
    "                self.content.strip()\n",
    "                while self.content.startswith(\"\\n\"):\n",
    "                    self.content = self.content[1:]\n",
    "                while self.content.endswith(\"\\n\"):\n",
    "                    self.content = self.content[:-1]\n",
    "                f.write(self.content)\n",
    "                f.write(\"\\n\")\n",
    "        return True\n",
    "\n",
    "\n",
    "class ObsidianFolderBuilder(Builder):\n",
    "    \"\"\"\n",
    "    Represents the Obsidian readable representation of our ontology.\n",
    "    Structure:\n",
    "    /templates # contains templates for the different node types\n",
    "        - Instance.md\n",
    "        - Class.md\n",
    "    And then the actual nodes, each accoring to their type specified in templates\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, director: Director, classes={}, instances={}):\n",
    "        super().__init__(director)\n",
    "\n",
    "        self.path = (\n",
    "            self.config.obsidian_path\n",
    "            if self.config and hasattr(self.config, \"obsidian_path\")\n",
    "            else None\n",
    "        )\n",
    "        self.templates: dict[str, Node] = {}\n",
    "        self.nodes: dict[str, Node] = {}\n",
    "\n",
    "        self.classes: dict[str, Instance] = {}\n",
    "        self.instances: dict[str, Instance] = {}\n",
    "        self.papers: dict[str, Instance] = {}\n",
    "        # self.other_nodes: dict[str, Node] = {}\n",
    "\n",
    "        if self.path:\n",
    "            self.load(self.path)\n",
    "\n",
    "    def build(self):\n",
    "        self.populate(force=True)\n",
    "        self.save()\n",
    "\n",
    "    def load(self, path):\n",
    "        self.path = path\n",
    "        templates_path = os.path.join(path, \"templates\")\n",
    "        for file in os.listdir(templates_path):\n",
    "            if file.endswith(\".md\"):\n",
    "                filepath = os.path.join(templates_path, file)\n",
    "                self.templates[file[:-3]] = Node(path=filepath)\n",
    "\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\".md\"):\n",
    "                filepath = os.path.join(path, file)\n",
    "                node = Node(path=filepath)\n",
    "                try:\n",
    "                    if \"class\" in node.instance.tags or hasattr(node.instance, \"subclass of\"):\n",
    "                        self.classes[node.instance.label] = node.instance\n",
    "                    elif \"instance\" in node.instance.tags or hasattr(\n",
    "                        node.instance, \"instance of\"\n",
    "                    ):\n",
    "                        if \"paper\" in node.instance.instance_of:\n",
    "                            self.papers[node.instance.label] = node.instance\n",
    "                        else:\n",
    "                            self.instances[node.instance.label] = node.instance\n",
    "                    else:\n",
    "                        self.other_nodes[node.instance.label] = node.instance\n",
    "                except:\n",
    "                    # self.other_nodes[node.name] = node\n",
    "                    print(f\"Error: Could not determine type of node {filepath}\")\n",
    "\n",
    "        print(\"Loaded Obsidian folder:\")\n",
    "        print(\n",
    "            \"number of tempaltes: \"\n",
    "            + str(len(self.templates))\n",
    "            + \" (\"\n",
    "            + \", \".join(self.templates.keys())\n",
    "            + \")\"\n",
    "        )\n",
    "        print(\"number of classes: \" + str(len(self.classes)))\n",
    "        print(\"number of instances: \" + str(len(self.instances)))\n",
    "        # print(\"number of other nodes: \" + str(len(self.other_nodes)))\n",
    "\n",
    "    def save(self, path=None, overwrite_templates=False):\n",
    "        if not path:\n",
    "            path = self.path\n",
    "        templates_path = os.path.join(path, \"templates\")\n",
    "        if not os.path.exists(templates_path):\n",
    "            os.makedirs(templates_path)\n",
    "\n",
    "        paths = []\n",
    "\n",
    "        if overwrite_templates:\n",
    "            for template in self.templates.values():\n",
    "                template.save()\n",
    "\n",
    "        for node in self.nodes.values():\n",
    "            node.save()\n",
    "            print(node.instance.label + \": \" + node.path)\n",
    "            if node.path not in paths:\n",
    "                paths.append(node.path)\n",
    "            else:\n",
    "                Warning(f\"Error: Duplicate path {node.path}\")\n",
    "\n",
    "    # def obsidify_property(self, text:str):\n",
    "    #     mapping = {\n",
    "    #         \"_\": \" \",\n",
    "    #     }\n",
    "    #     for key, value in mapping.items():\n",
    "    #         text = text.replace(key, value)\n",
    "    #     return text\n",
    "\n",
    "    def properties_from_template(self, template_name, node: Node):\n",
    "        # print(node)\n",
    "        template = self.templates.get(template_name, {})\n",
    "        if not template:\n",
    "            return\n",
    "\n",
    "        for key, value in template.__dict__.items():\n",
    "            # node properties\n",
    "            if not hasattr(node, key) or not getattr(node, key):\n",
    "                setattr(node, key, value)\n",
    "\n",
    "        # properties.update(template.instance.get(\"properties\", {}))\n",
    "\n",
    "        properties = {}\n",
    "        properties.update(template.instance.__dict__)\n",
    "        for key, value in node.instance.__dict__.items():\n",
    "            # key = self.obsidify_property(key)\n",
    "            if key not in properties or not properties[key]:\n",
    "                properties[key] = value\n",
    "            elif not value:\n",
    "                continue\n",
    "            else:\n",
    "                # both exist, try to merge\n",
    "                # print(f\"Merging {key} ({value}, {properties[key]}) for {name}\")\n",
    "                if isinstance(value, list):\n",
    "                    for entry in value:\n",
    "                        entry = entry.strip()\n",
    "                        if entry not in properties[key]:\n",
    "                            properties[key].append(entry)\n",
    "\n",
    "                    if key == \"tags\":\n",
    "                        added = []\n",
    "                        to_delete = []\n",
    "                        for listID, entry in enumerate(properties[key]):\n",
    "                            entry = entry.strip()\n",
    "                            if entry.startswith(\"#\"):\n",
    "                                entry = entry[1:]\n",
    "                            if entry in added:\n",
    "                                to_delete.append(listID)\n",
    "                            properties[key][listID] = entry\n",
    "                            added.append(entry)\n",
    "                        for listID in to_delete:\n",
    "                            properties[key].pop(listID)\n",
    "\n",
    "                elif isinstance(value, dict):\n",
    "                    properties[key].update(value)\n",
    "                else:\n",
    "                    name = getattr(node, \"name\", getattr(node, \"label\", \"unknown name\"))\n",
    "                    Warning(\n",
    "                        f\"Error: Could not merge properties {key} ({value}, {properties[key]}) for {name}\"\n",
    "                    )\n",
    "        node.instance.set_properties(properties)\n",
    "\n",
    "    def add_nodes(\n",
    "        self, instances: dict[str, Instance], template_name=\"Instance\", force=False\n",
    "    ):\n",
    "        for name, instance in instances.items():\n",
    "            if name not in self.nodes or force:\n",
    "                filepath = os.path.join(\n",
    "                    self.path, f\"{clear_name(name, can_be_folder = False)}.md\"\n",
    "                )\n",
    "                node = Node(instance, filepath)\n",
    "                self.properties_from_template(template_name, node)\n",
    "                self.nodes[name] = node\n",
    "\n",
    "    def populate(self, instances={}, classes={}, force=False):\n",
    "        self.instances.update(instances)\n",
    "        self.classes.update(classes)\n",
    "\n",
    "        print(\"Populating Obsidian folder\")\n",
    "        self.add_nodes(self.instances, template_name=\"Instance\", force=force)\n",
    "        self.add_nodes(self.papers, template_name=\"Instance\", force=force)\n",
    "        self.add_nodes(self.classes, template_name=\"Class\", force=force)\n",
    "\n",
    "        # if force:\n",
    "        #     print(\"Forcing overwrite of all nodes\")\n",
    "        #     self.classes = {}\n",
    "        #     self.instances = {}\n",
    "        #     self.other_nodes = {}\n",
    "        # for name, instance in instances.items():\n",
    "        #     if name not in self.instances:\n",
    "        #         filepath = os.path.join(\n",
    "        #             self.path, f\"{clear_name(name, can_be_folder = False)}.md\"\n",
    "        #         )\n",
    "        #         node = Node(instance, filepath)\n",
    "        #         self.properties_from_template(\"Instance\", node)\n",
    "        #         self.instances[name] = instance\n",
    "        #         self.nodes[name] = node\n",
    "        # for name, instance_type in classes.items():\n",
    "        #     if name not in self.classes:\n",
    "        #         filepath = os.path.join(\n",
    "        #             self.path, f\"{clear_name(name, can_be_folder = False)}.md\"\n",
    "        #         )\n",
    "        #         properties = self.properties_from_template(\"Class\", instance_type)\n",
    "        #         self.classes[name] = Node(filepath, properties)\n",
    "        print(\n",
    "            \"Added \"\n",
    "            + str(len(self.instances))\n",
    "            + \" instances and \"\n",
    "            + str(len(self.classes))\n",
    "            + \" classes to Obsidian folder\"\n",
    "        )\n",
    "\n",
    "        # def __init__(self, path, properties={}):\n",
    "\n",
    "    #     self.path = path\n",
    "    #     self.content = properties.get(\"content\", \"\")\n",
    "    #     self.set_properties(properties)\n",
    "\n",
    "    # def get(self, label=\"\", default={}):\n",
    "    #     if not label:\n",
    "    #         return self.__dict__\n",
    "    #     if label == \"properties\":\n",
    "    #         return {\n",
    "    #             key: value\n",
    "    #             for key, value in self.__dict__.items()\n",
    "    #             if key not in [\"path\"]\n",
    "    #         }\n",
    "    #     elif label in self.__dict__:\n",
    "    #         return self.__dict__[label]\n",
    "    #     else:\n",
    "    #         return default\n",
    "\n",
    "    # def set_properties(self, properties={}, force=False):\n",
    "    #     if properties:\n",
    "    #         # properties = self.convert_python_to_obsidian(properties)\n",
    "    #         for key, value in properties.items():\n",
    "    #             if hasattr(self, key):\n",
    "    #                 existing = getattr(self, key)\n",
    "    #                 if existing != value:\n",
    "    #                     if not force:\n",
    "    #                         print(f\"Preserving existing value for {key}: {existing}\")\n",
    "    #                         continue\n",
    "    #                     else:\n",
    "    #                         print(f\"Overwriting existing value for {key}: {existing}\")\n",
    "    #             setattr(self, key, value)\n",
    "    #     else:\n",
    "    #         try:\n",
    "    #             self.load()\n",
    "    #         except:\n",
    "    #             pass\n",
    "\n",
    "    # def convert_python_to_obsidian(self, input={}, back_to_python=False):\n",
    "    #     if not input:\n",
    "    #         input = self.__dict__\n",
    "\n",
    "    #     forbidden = [\"path\", \"content\"]\n",
    "    #     data = {k: v for k, v in input.items() if k not in forbidden}\n",
    "\n",
    "    # data = {}\n",
    "    # for key, value in input.items():\n",
    "    #     if key == \"path\" or key == \"content\":\n",
    "    #         continue\n",
    "\n",
    "    # for key, value in Node.obsidian_python_property_map.items():\n",
    "    #     if key == \"path\" or key == \"content\" or hasattr(Node, key):\n",
    "    #         continue\n",
    "\n",
    "    #     # pos_key = key if not back_to_python else value\n",
    "    #     # if isinstance(value, dict):\n",
    "    #     #     if \"instance of\" in input and input[\"instance of\"] == key:\n",
    "    #     #         for subkey, subvalue in value.items():\n",
    "    #     #             pos_key = subkey if not back_to_python else subvalue\n",
    "    #     #             if subvalue in input:\n",
    "    #     #                 data[pos_key] = input[subvalue]\n",
    "    #     #             elif subkey in input:\n",
    "    #     #                 data[pos_key] = input[subkey]\n",
    "    #     #             else:\n",
    "    #     #                 data[pos_key] = None\n",
    "    #     # elif value in input:\n",
    "    #     #     data[pos_key] = input[value]\n",
    "    #     # elif key in input:\n",
    "    #     #     data[pos_key] = input[key]\n",
    "    #     # else:\n",
    "    #     #     data[pos_key] = None\n",
    "    # if data[\"instance of\"]:\n",
    "    #     # is an instance, delete subclass attribute\n",
    "    #     del data[\"subclass of\"]\n",
    "    # else:\n",
    "    #     # is a class, delete instance attribute\n",
    "    #     del data[\"instance of\"]\n",
    "\n",
    "    # return data\n",
    "\n",
    "\n",
    "director.build_obsidian_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Setup done.\n",
    "Proceeding to:\n",
    "\n",
    "## Matrix calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free unneeded memory\n",
    "# del paper_nlp_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# run_debug_test(config, instances, papers, paper_instance_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "director.setup_occurence_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_debug_test(config, literals, papers, paper_instance_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all text files\n",
    "def get_paper_full_text(directory):\n",
    "    paper_full_text = {}\n",
    "    for folder in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    paper_full_text[file[:-4]] = file_path\n",
    "                    break\n",
    "\n",
    "    return paper_full_text\n",
    "\n",
    "\n",
    "paper_full_text = get_paper_full_text(\n",
    "    \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/00_PDFs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: find occurrences of instances in full text of papers\n",
    "import sys\n",
    "from bisect import bisect_left\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "class PosInPaper:\n",
    "    def __init__(self):\n",
    "        # List of paper identifiers\n",
    "        self.papers = []\n",
    "        # List of literals\n",
    "        self.literals = []\n",
    "        # Dict of unique words across all literals\n",
    "        self.words = {}\n",
    "        self.word_len = []\n",
    "        # List of unique combinations of words across all literals\n",
    "        self.word_combinations = {}\n",
    "        self.word_combination_lists = []\n",
    "        self.word_combination_index_literal = {}\n",
    "        # 2D list mapping pairs of literals to their word combination index\n",
    "        self.word_combination_index_literal_literal = []\n",
    "        # 2D list of SortedSets, each containing the positions of a word in a paper\n",
    "        self.word_occurrences_in_papers = []\n",
    "        # 3D list containing the minimum distances between word combinations in each paper\n",
    "        self.min_distances = []\n",
    "\n",
    "    @time_function\n",
    "    def populate(\n",
    "        self,\n",
    "        config: Config,\n",
    "        papers: list,\n",
    "        literals: list[str],\n",
    "        paper_full_text,\n",
    "        optimize=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Populates the internal data structures with occurrences and distances of literals in papers.\n",
    "\n",
    "        Parameters:\n",
    "        - config (Config): Configuration object containing settings.\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - literals (list[str]): List of literals to process.\n",
    "        - paper_full_text (dict): Mapping from paper identifiers to their full text file paths.\n",
    "        - optimize (bool): Flag to optimize data structures after population.\n",
    "        \"\"\"\n",
    "        self.initialize_variables(papers, literals)\n",
    "        self.process_literals()\n",
    "        self.process_literal_combinations()\n",
    "        self.setup_data_structures()\n",
    "        self.find_occurrences_in_texts(paper_full_text)\n",
    "        if optimize:\n",
    "            self.optimize_data()\n",
    "\n",
    "    def update_list_attribute(self, list, name):\n",
    "        existing = getattr(self, name)\n",
    "        if not existing:\n",
    "            setattr(self, name, list)\n",
    "        else:\n",
    "            if existing != list:\n",
    "                print(f\"Warning: {name} has changed.\")\n",
    "                for item in list:\n",
    "                    if item not in existing:\n",
    "                        print(f\"Item {item} is new.\")\n",
    "                        existing.append(item)\n",
    "                # update\n",
    "                raise NotImplementedError(\n",
    "                    \"create a function to update other relying attributes\"\n",
    "                )\n",
    "\n",
    "    @time_function\n",
    "    def initialize_variables(self, papers, literals):\n",
    "        \"\"\"\n",
    "        Initializes basic variables for the class instance.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        if not self.papers:\n",
    "            self.papers = papers\n",
    "        else:\n",
    "            if self.papers != papers:\n",
    "                print(\"Warning: Papers have changed.\")\n",
    "                for paper in papers:\n",
    "                    if paper not in self.papers:\n",
    "                        print(f\"Paper {paper} is new.\")\n",
    "                        self.papers.append(paper)\n",
    "        # self.papers = papers\n",
    "\n",
    "        if not self.literals:\n",
    "            self.literals = literals\n",
    "        else:\n",
    "            if self.literals != literals:\n",
    "                print(\"Warning: Literals have changed.\")\n",
    "                for literal in literals:\n",
    "                    if literal not in self.literals:\n",
    "                        print(f\"Literal {literal} is new.\")\n",
    "                        self.literals.append(literal)\n",
    "                        self.word_combination_index_literal[literal] = None\n",
    "\n",
    "        if len(self.literals) != len(self.word_combination_index_literal):\n",
    "            for literal in self.literals:\n",
    "                if literal not in self.word_combination_index_literal:\n",
    "                    self.word_combination_index_literal[literal] = None\n",
    "            # sort self.word_combination_index_literal by self.literals\n",
    "            self.word_combination_index_literal = {\n",
    "                k: self.word_combination_index_literal[k] for k in self.literals\n",
    "            }\n",
    "        if (\n",
    "            isinstance(self.word_combination_index_literal_literal, np.ndarray)\n",
    "            and self.word_combination_index_literal_literal.size == 0\n",
    "        ):\n",
    "            self.word_combination_index_literal_literal = np.full(\n",
    "                (len(self.literals), len(self.literals)), None, dtype=object\n",
    "            )\n",
    "        elif (\n",
    "            isinstance(self.word_combination_index_literal_literal, list)\n",
    "            and self.word_combination_index_literal_literal == []\n",
    "        ):\n",
    "            self.word_combination_index_literal_literal = [\n",
    "                [None] * len(self.literals) for _ in range(len(self.literals))\n",
    "            ]\n",
    "        elif len(self.literals) != len(self.word_combination_index_literal_literal):\n",
    "            # pad self.word_combination_index_literal_literal\n",
    "            len_dif = len(self.literals) - len(\n",
    "                self.word_combination_index_literal_literal\n",
    "            )\n",
    "            self.word_combination_index_literal_literal = np.pad(\n",
    "                self.word_combination_index_literal_literal,\n",
    "                ((0, len_dif), (0, len_dif)),\n",
    "                \"constant\",\n",
    "                constant_values=None,\n",
    "            )\n",
    "            # self.word_combination_index_literal_literal = [[None] * len(self.literals) for _ in range(len(self.literals))]\n",
    "\n",
    "        # self.literals = literals\n",
    "\n",
    "    @time_function\n",
    "    def process_literals(self):\n",
    "        \"\"\"\n",
    "        Processes each literal to extract and store unique words and word combinations.\n",
    "\n",
    "        Parameters:\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        for lit in self.literals:\n",
    "            word_list = split_string(lit)\n",
    "            self.add_words(word_list)\n",
    "            self.add_if_word_combination(word_list, lit)\n",
    "\n",
    "    def add_words(self, word_list):\n",
    "        \"\"\"\n",
    "        Adds unique words from a list to the internal list of words.\n",
    "\n",
    "        Parameters:\n",
    "        - word_list (list): List of words to add.\n",
    "        \"\"\"\n",
    "        for word in word_list:\n",
    "            if word not in self.words:\n",
    "                self.words[word] = len(self.words)\n",
    "                self.word_len.append(len(word))\n",
    "\n",
    "    def add_if_word_combination(self, word_list, lit):\n",
    "        \"\"\"\n",
    "        Adds a unique combination of words from a list to the internal list of word combinations.\n",
    "\n",
    "        Parameters:\n",
    "        - word_list (list): List of words forming a combination.\n",
    "        - lit (str): The literal corresponding to the word combination.\n",
    "        \"\"\"\n",
    "        if len(word_list) > 1:\n",
    "            pos = self.word_combination_index_literal.get(lit, -1)\n",
    "            if pos == -1 or pos == None:\n",
    "                froz = frozenset(word_list)\n",
    "                pos = len(self.word_combinations)\n",
    "                self.add_word_combination(froz, pos)\n",
    "                self.word_combination_index_literal[lit] = pos\n",
    "\n",
    "    def add_word_combination(self, froz, pos):\n",
    "        self.word_combinations[froz] = pos\n",
    "        self.word_combination_lists.append(\n",
    "            [self.words[word] for word in sorted(froz, key=len, reverse=True)]\n",
    "        )\n",
    "\n",
    "    @time_function\n",
    "    def process_literal_combinations(self):\n",
    "        \"\"\"\n",
    "        Processes combinations of literals to store their indices in the internal data structure.\n",
    "\n",
    "        Parameters:\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        # Use a dictionary for quick lookup and storage\n",
    "        combination_index = len(self.word_combinations)\n",
    "\n",
    "        for id1, literal1 in enumerate(self.literals):\n",
    "            for id2 in range(id1 + 1, len(self.literals)):\n",
    "                if self.word_combination_index_literal_literal[id1][id2] is not None:\n",
    "                    continue\n",
    "                literal2 = self.literals[id2]\n",
    "                # Use a sorted tuple for consistent ordering\n",
    "                froz = frozenset(split_string(literal1) + split_string(literal2))\n",
    "                # Check if the combination is already in the dictionary\n",
    "                pos = self.word_combinations.get(froz, -1)\n",
    "                if pos == -1:\n",
    "                    pos = combination_index\n",
    "                    combination_index += 1\n",
    "\n",
    "                    self.add_word_combination(froz, pos)\n",
    "\n",
    "                # Update the matrix with the index of the combination\n",
    "                self.word_combination_index_literal_literal[id1][id2] = pos\n",
    "                self.word_combination_index_literal_literal[id2][id1] = pos\n",
    "\n",
    "    @time_function\n",
    "    def setup_data_structures(self):\n",
    "        \"\"\"\n",
    "        Initializes the data structures for storing word occurrences and minimum distances.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        \"\"\"\n",
    "        if self.word_occurrences_in_papers == []:\n",
    "            self.word_occurrences_in_papers = [\n",
    "                [[] for _ in self.words] for _ in self.papers\n",
    "            ]\n",
    "        new_papers = len(self.papers) - len(self.word_occurrences_in_papers)\n",
    "        new_words = len(self.words) - len(self.word_occurrences_in_papers[0])\n",
    "        if new_words > 0:\n",
    "            for paper in self.word_occurrences_in_papers:\n",
    "                paper += [[] for _ in range(new_words)]\n",
    "        if new_papers > 0:\n",
    "            self.word_occurrences_in_papers += [\n",
    "                [[] for _ in self.words] for _ in range(new_papers)\n",
    "            ]\n",
    "\n",
    "\n",
    "        if (\n",
    "            isinstance(self.min_distances, list)\n",
    "            and self.min_distances == []\n",
    "            or self.min_distances is None\n",
    "        ):\n",
    "            self.min_distances = np.full(\n",
    "                (len(self.papers), len(self.word_combinations)), -2, dtype=int\n",
    "            )\n",
    "        # If new papers or words have been added, update the data structures\n",
    "        if len(self.papers) > len(self.min_distances):\n",
    "            self.min_distances = np.pad(\n",
    "                self.min_distances,\n",
    "                ((0, len(self.papers) - len(self.min_distances)), (0, 0)),\n",
    "                \"constant\",\n",
    "                constant_values=-2,\n",
    "            )\n",
    "        if len(self.word_combinations) > len(self.min_distances[0]):\n",
    "            len_dif = len(self.word_combinations) - len(self.min_distances[0])\n",
    "            self.min_distances = np.pad(\n",
    "                self.min_distances,\n",
    "                ((0, 0), (0, len_dif)),\n",
    "                \"constant\",\n",
    "                constant_values=-2,\n",
    "            )\n",
    "\n",
    "    @time_function\n",
    "    def find_occurrences_in_texts(self, paper_full_text):\n",
    "        \"\"\"\n",
    "        Finds and stores the occurrences of each word in the full text of each paper.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - paper_full_text (dict): Mapping from paper identifiers to their full text file paths.\n",
    "        \"\"\"\n",
    "        for paperID, paper in enumerate(self.papers):\n",
    "            if paper in paper_full_text:\n",
    "                with open(paper_full_text[paper], \"r\", encoding=\"utf8\") as f:\n",
    "                    text = f.read().lower()\n",
    "                    for word, wordID in self.words.items():\n",
    "                        if not self.word_occurrences_in_papers[paperID][wordID]:\n",
    "                            self.find_and_add_word_occurrences(\n",
    "                                paperID, wordID, word, text\n",
    "                            )\n",
    "            else:\n",
    "                print(f\"Paper {paper} has no full text available.\")\n",
    "\n",
    "    def find_and_add_word_occurrences(self, paperID, wordID, word, text):\n",
    "        \"\"\"\n",
    "        Finds and adds the occurrences of a word in a paper's text to the internal data structure.\n",
    "\n",
    "        Parameters:\n",
    "        - paperID (int): The index of the paper in the internal list.\n",
    "        - wordID (int): The index of the word in the internal list.\n",
    "        - word (str): The word to find occurrences of.\n",
    "        - text (str): The full text of the paper.\n",
    "        \"\"\"\n",
    "        pos = text.find(word)\n",
    "        while pos != -1:\n",
    "            # self.word_occurrences_in_papers[paperID][wordID].add(pos)\n",
    "            self.word_occurrences_in_papers[paperID][wordID].append([pos, wordID])\n",
    "            pos = text.find(word, pos + 1)\n",
    "\n",
    "    @time_function\n",
    "    def optimize_data(self):\n",
    "        \"\"\"\n",
    "        Optimizes the internal data structures for faster access and smaller memory footprint.\n",
    "        \"\"\"\n",
    "        # self.word_combination_index_literal_literal = np.array(self.word_combination_index_literal_literal, dtype=int)\n",
    "        for paperID in range(len(self.papers)):\n",
    "            for wordID in range(len(self.words)):\n",
    "                # self.word_occurrences_in_papers[paperID][wordID] = SortedSet(self.word_occurrences_in_papers[paperID][wordID])\n",
    "                if not self.word_occurrences_in_papers[paperID][wordID]:\n",
    "                    continue\n",
    "                if isinstance(self.word_occurrences_in_papers[paperID][wordID][0], int):\n",
    "                    self.word_occurrences_in_papers[paperID][wordID] = [\n",
    "                        (x, wordID)\n",
    "                        for x in self.word_occurrences_in_papers[paperID][wordID]\n",
    "                    ]\n",
    "                    print(\n",
    "                        f\"Optimizing {list(self.words.keys())[wordID]} in paper {paperID}\"\n",
    "                    )\n",
    "                for occurrence in self.word_occurrences_in_papers[paperID][wordID]:\n",
    "                    if isinstance(occurrence, int):\n",
    "                        self.word_occurrences_in_papers[paperID][wordID] = [\n",
    "                            (occurrence, wordID)\n",
    "                        ]\n",
    "                        # break\n",
    "                        raise Exception(\"This should not happen\")\n",
    "\n",
    "    def save_to_csv(self, config: Config = None, path=None, name=\"pos_in_paper\"):\n",
    "        if path is None:\n",
    "            path = config.get_output_path()\n",
    "\n",
    "        # save min_distances to csv\n",
    "        # dump self.min_distances to csv, with self.papers as row headers and self.word_combinations as column headers\n",
    "        filepath = os.path.join(path, name + \"_min_distances.csv\")\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            word_combinations = [\n",
    "                \"_\".join(sorted(froz, key=len, reverse=True))\n",
    "                for froz in self.word_combinations.keys()\n",
    "            ]\n",
    "            f.write(\n",
    "                \"papers\"\n",
    "                + config.csv_separator\n",
    "                + config.csv_separator.join(word_combinations)\n",
    "                + \"\\n\"\n",
    "            )\n",
    "            for i, paper in enumerate(self.papers):\n",
    "                f.write(\n",
    "                    paper\n",
    "                    + config.csv_separator\n",
    "                    + config.csv_separator.join(map(str, self.min_distances[i]))\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "\n",
    "    @time_function\n",
    "    def save_to_file(\n",
    "        self,\n",
    "        config,\n",
    "        path=None,\n",
    "        name=\"pos_in_paper\",\n",
    "        check_size=False,\n",
    "        min_distance_to_csv=False,\n",
    "        backup=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Saves the internal data structures to files for persistence.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str, optional): The base path for the output files. Defaults to \"pos_in_paper\".\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = config.get_output_path()\n",
    "        filepath = os.path.join(path, name + \".json\")\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        for key, value in self.__dict__.items():\n",
    "            # if key == \"min_distances\" or key == \"word_combination_index_literal_literal\":\n",
    "            if (\n",
    "                value.__class__.__name__ == \"ndarray\"\n",
    "            ):  # min_distances, word_combination_index_literal_literal\n",
    "                data[key] = value.tolist()\n",
    "            # if key == \"min_distances\":\n",
    "            #     data[key] = value.tolist()\n",
    "            elif key == \"word_combinations\":\n",
    "                data[key] = {\n",
    "                    \"_\".join(key): value\n",
    "                    for key, value in self.word_combinations.items()\n",
    "                }\n",
    "            else:\n",
    "                data[key] = value\n",
    "            if check_size:\n",
    "                # Construct the file name for each sub-dictionary\n",
    "                filepath = os.path.join(path, f\"{name}_{key}.json\")\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(data[key], f, ensure_ascii=False)\n",
    "            pass\n",
    "\n",
    "        if min_distance_to_csv:\n",
    "            self.save_to_csv(config, path, name)\n",
    "\n",
    "        if backup:\n",
    "            backup_path = os.path.join(path, name + \"_backup\" + \".json\")\n",
    "            if os.path.exists(filepath):\n",
    "                existing_is_healthy = True\n",
    "                try:\n",
    "                    self.load_from_file(config, path, name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Overwriting existing save\")\n",
    "                    existing_is_healthy = False\n",
    "                if existing_is_healthy:\n",
    "                    if os.path.exists(backup_path):\n",
    "                        os.remove(backup_path)\n",
    "                    os.rename(filepath, backup_path)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "    @time_function\n",
    "    def load_from_file(self, config, path=None, name=\"pos_in_paper\", backup=True):\n",
    "        \"\"\"\n",
    "        Loads the internal data structures from files.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str, optional): The base path for the input files. Defaults to \"pos_in_paper\".\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = config.get_output_path()\n",
    "        filepath = os.path.join(path, name + \".json\")\n",
    "\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {filepath}: {e}\")\n",
    "            if backup:\n",
    "                backup_path = os.path.join(path, name + \"_backup\" + \".json\")\n",
    "                if os.path.exists(backup_path):\n",
    "                    print(f\"Trying to load backup file {backup_path}\")\n",
    "                    with open(backup_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)\n",
    "                else:\n",
    "                    raise Exception(f\"No backup file found at {backup_path}\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        for key, value in data.items():\n",
    "            if key == \"word_combinations\":\n",
    "                setattr(\n",
    "                    self,\n",
    "                    key,\n",
    "                    {\n",
    "                        frozenset(split_string(sub_key)): i\n",
    "                        for i, sub_key in enumerate(value)\n",
    "                    },\n",
    "                )\n",
    "            elif (\n",
    "                key == \"min_distances\"\n",
    "                or key == \"word_combination_index_literal_literal\"\n",
    "            ):\n",
    "                setattr(self, key, np.array(value))\n",
    "            else:\n",
    "                try:\n",
    "                    setattr(self, key, value)\n",
    "                except Exception as e:\n",
    "                    raise Exception(f\"Error loading pos_in_paper attribute {key}: {e}\")\n",
    "\n",
    "        self.setup_data_structures()\n",
    "\n",
    "    @time_function\n",
    "    def calculate_all_possible(self):\n",
    "        \"\"\"\n",
    "        Calculates the minimum distances between all possible combinations of literals in all papers.\n",
    "        \"\"\"\n",
    "        save_every = None\n",
    "        if len(self.papers) > 300:\n",
    "            print(\n",
    "                \"Warning: This operation is computationally expensive and may take a long time.\"\n",
    "            )\n",
    "            save_every = 300\n",
    "        for p in range(len(self.papers)):\n",
    "            if save_every and p % save_every == 0:\n",
    "                print(f\"Processing paper {p} of {len(self.papers)}\")\n",
    "                self.save_to_file(config)\n",
    "            for w in range(len(self.word_combinations)):\n",
    "                self.find_min_distance_by_id(p, w)\n",
    "            # for i in range(len(self.literals)):\n",
    "            #     for j in range(i + 1, len(self.literals)):\n",
    "            #         # get word_combination_index_literal_literal\n",
    "            # self.find_min_distance_by_id(p, self.word_combination_index_literal_literal[i][j])\n",
    "\n",
    "    def find_min_distance_by_id(self, paperID, wcID):\n",
    "        \"\"\"\n",
    "        Finds the minimum distance between occurrences of literals in a paper.\n",
    "\n",
    "        Parameters:\n",
    "        - paper (str): The identifier for the paper.\n",
    "        - literals (list): A list of literals for which the distance is to be found.\n",
    "        - allow_call (bool): Flag to allow recursive call to get_min_distance.\n",
    "\n",
    "        Returns:\n",
    "        - int: The minimum distance between occurrences of the literals.\n",
    "        \"\"\"\n",
    "        distance = self.min_distances[paperID][wcID]\n",
    "\n",
    "        if distance == -1:\n",
    "            # word combination not found in paper\n",
    "            return -1\n",
    "        if distance == -2:\n",
    "            # calculate distance\n",
    "            pass\n",
    "        else:\n",
    "            return distance\n",
    "\n",
    "        list_ids = self.word_combination_lists[wcID]\n",
    "        # since we have attached global Word IDs to the occurrences, we need to map to their local position\n",
    "        list_ids_map = {list_ids[i]: i for i in range(len(list_ids))}\n",
    "        # literals = [list(self.words)[i] for i in list_ids]\n",
    "\n",
    "        # TODO: It should be possible to remove smaller words from the list,\n",
    "        # if a larger word contains it:\n",
    "        # e.g. remove \"engine\" if \"engineer\" is in the list\n",
    "        ## The following implementation works, but is not used for now. Reasons:\n",
    "        ## 1. It could be slower than the current implementation\n",
    "        ## 2. It might be beneficial to future use-cases to not remove smaller words\n",
    "        # literals = [list(self.words)[key] for key in list_ids_map]\n",
    "        # # check if any literal is a substring of another\n",
    "        # for i, lit1 in enumerate(literals):\n",
    "        #     for j, lit2 in enumerate(literals):\n",
    "        #         if i != j and lit1 in lit2:\n",
    "        #             # if lit1 is a substring of lit2, remove it from the list\n",
    "        #             list_ids_map.pop(list_ids[i])\n",
    "        #             break\n",
    "\n",
    "        lit_len = [self.word_len[i] for i in list_ids]\n",
    "\n",
    "        for i in list_ids:\n",
    "            if not self.word_occurrences_in_papers[paperID][i]:\n",
    "                self.min_distances[paperID][wcID] = -1\n",
    "                return -1\n",
    "        # Outsourced to optimize\n",
    "        # inputs = [[(x, i) for x in self.word_occurrences_in_papers[paperID][wordID]] for i, wordID in enumerate(list_ids)]\n",
    "        inputs = [\n",
    "            self.word_occurrences_in_papers[paperID][wordID] for wordID in list_ids\n",
    "        ]\n",
    "\n",
    "        indices = [lst[0][0] for lst in inputs]\n",
    "        best = float(\"inf\")\n",
    "\n",
    "        for item in sorted(sum(inputs, [])):\n",
    "            if item[0] not in indices:\n",
    "                continue\n",
    "            indices[list_ids_map[item[1]]] = item[0]\n",
    "            arr_min = min(indices)\n",
    "            best = min(max(indices) - arr_min - lit_len[indices.index(arr_min)], best)\n",
    "            if best <= 0:\n",
    "                best = 0\n",
    "                break\n",
    "        self.min_distances[paperID][wcID] = best\n",
    "\n",
    "        return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_in_paper = PosInPaper()\n",
    "\n",
    "# config.recalculate_pos_in_paper = True # May be used for debug purposes\n",
    "\n",
    "if not config.recalculate_pos_in_paper:\n",
    "    try:\n",
    "        pos_in_paper.load_from_file(config)\n",
    "    # print exception\n",
    "    except Exception as e:\n",
    "        # if config.debug:\n",
    "        #     raise e\n",
    "        print(e)\n",
    "        print(\"Starting from scratch.\")\n",
    "\n",
    "        config.recalculate_pos_in_paper = True\n",
    "\n",
    "# TODO:\n",
    "# raise NotImplementedError(\"Implement a check that compares the loaded instances and papers with the current ones\")\n",
    "\n",
    "# if config.recalculate_pos_in_paper:\n",
    "pos_in_paper.populate(\n",
    "    config,\n",
    "    list(director.papers.keys()),\n",
    "    list(director.instances.keys()),\n",
    "    paper_full_text,\n",
    ")\n",
    "# pos_in_paper.save_to_file(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.recalculate_pos_in_paper or -2 in pos_in_paper.min_distances:\n",
    "    pos_in_paper.calculate_all_possible()\n",
    "    # Info: This method is extremely slow. requires more testing, which is currently done in a side project:\n",
    "    ## scripts\\SLR\\MVP\\test_case.ipynb\n",
    "    pos_in_paper.save_to_file(config)\n",
    "    config.recalculate_pos_in_paper = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # intermediate stitching:\n",
    "# papers = list(director.papers.keys())\n",
    "# literals = list(director.instances.keys())\n",
    "# paper_instance_occurrence_matrix = director.paper_instance_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "director.paper_full_text = paper_full_text\n",
    "\n",
    "director.builder[\"error_matrix_builder\"] = ErrorMatrixBuilder(director, pos_in_paper)\n",
    "director.builder[\"error_matrix_builder\"].build()\n",
    "director.builder[\"error_matrix_builder\"].save()\n",
    "director.sort_instances()\n",
    "director.builder[\"occurrence_matrix\"].save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance_instance Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_instance_co_occurrence_matrix = np.dot(\n",
    "    director.builder[\"occurrence_matrix\"].matrix.T, director.builder[\"occurrence_matrix\"].matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_instance_relative_co_occurrence_matrix = (\n",
    "    instance_instance_co_occurrence_matrix\n",
    "    / np.diag(instance_instance_co_occurrence_matrix)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize timeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def visualize_timeline(\n",
    "    config: Config,\n",
    "    year_instance_occurrence_matrix,\n",
    "    year_papers,\n",
    "    instances,\n",
    "    instance_types_dicts,\n",
    "    name=\"some_timeline\",\n",
    "    path=None,\n",
    "    recursion_depth=0,\n",
    "    start_index=0,\n",
    "    error_matrix=None,\n",
    "    error_instances=None,\n",
    "):\n",
    "    if not path:\n",
    "        path = config.get_output_path(path, visualization=True)\n",
    "    years = list(year_papers.keys())\n",
    "    max_papers = max([len(year_papers[year]) for year in years])\n",
    "    yearly_papers = [len(year_papers[year]) for year in years]\n",
    "\n",
    "    ALPHA_ERROR_LINE = 0.3\n",
    "    ALPHA_ERROR_ZONE = 0.2\n",
    "    ALPHA_PAPER_BAR = 0.3\n",
    "\n",
    "    for type in instance_types_dicts:\n",
    "        use = [instance in instance_types_dicts[type] for instance in instances]\n",
    "        type_instances = [\n",
    "            instance for instance, use_flag in zip(instances, use) if use_flag\n",
    "        ]\n",
    "        total_occurrences = [\n",
    "            np.sum(year_instance_occurrence_matrix[:, instances.index(instance)])\n",
    "            for instance in type_instances\n",
    "        ]\n",
    "        type_instances_sorted = [\n",
    "            x\n",
    "            for _, x in sorted(\n",
    "                zip(total_occurrences, type_instances),\n",
    "                key=lambda pair: pair[0],\n",
    "                reverse=True,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        PARTITION_SIZE = 10\n",
    "        # if error_instances is not None:\n",
    "        #     PARTITION_SIZE = int(0.5 * PARTITION_SIZE)\n",
    "\n",
    "        type_matrix = year_instance_occurrence_matrix[\n",
    "            :, [instances.index(instance) for instance in type_instances_sorted]\n",
    "        ]\n",
    "        factor = 1\n",
    "        size_x = (2 + len(years) / 6) * factor\n",
    "        size_y = (2 + max_papers / 15) * factor\n",
    "        size_y_2 = (2 + PARTITION_SIZE / 2) * factor\n",
    "        size_y = max(size_y, size_y_2)\n",
    "        fig, ax = plt.subplots(figsize=(size_x, size_y), dpi=300)\n",
    "\n",
    "        ax.set_xticks(range(len(years)))\n",
    "        years_labels = [year if len(year_papers[year]) > 0 else \"\" for year in years]\n",
    "        ax.set_xticklabels(years_labels, fontsize=10, rotation=90)\n",
    "\n",
    "        step_size = max(1, math.ceil(max_papers / 10))\n",
    "        ax.set_yticks(np.arange(0, max_papers + 1, step=step_size))\n",
    "        ax.set_yticklabels(\n",
    "            [str(int(x)) for x in np.arange(0, max_papers + 1, step=step_size)],\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "        # set y axis label\n",
    "        ax.set_ylabel(\"absolute\", fontsize=10)\n",
    "\n",
    "        plt.bar(\n",
    "            range(len(years)),\n",
    "            yearly_papers,\n",
    "            color=\"black\",\n",
    "            alpha=ALPHA_PAPER_BAR,\n",
    "            label=f\"Total papers ({sum(yearly_papers)})\",\n",
    "            zorder=0,\n",
    "        )\n",
    "\n",
    "        line_count = 0\n",
    "        i = start_index\n",
    "        while line_count < PARTITION_SIZE and i < len(type_instances_sorted):\n",
    "            instance = type_instances_sorted[i]\n",
    "            yearly_occurrences = type_matrix[:, i]\n",
    "            i_total_occurrences = yearly_occurrences.sum()\n",
    "            label = f\"{instance} ({i_total_occurrences})\"\n",
    "            values = yearly_occurrences\n",
    "            line = plt.plot(range(len(years)), values, label=label, zorder=3)[0]\n",
    "            line_count += 1\n",
    "            if error_matrix is not None and instance in error_instances:\n",
    "                color = line.get_color()\n",
    "                errors = error_matrix[:, error_instances.index(instance)]\n",
    "                errors_plus = yearly_occurrences + errors\n",
    "                line.set_label(f\"{instance} ({i_total_occurrences}-{sum(errors_plus)})\")\n",
    "                # Plot the error as a half transparent line on top of the normal line\n",
    "                plt.plot(\n",
    "                    range(len(years)),\n",
    "                    errors_plus,\n",
    "                    color=color,\n",
    "                    alpha=ALPHA_ERROR_LINE,\n",
    "                    label=f\"{instance} (w/o proximity)\",\n",
    "                    zorder=2,\n",
    "                )\n",
    "                line_count += 1\n",
    "                # color in the area between the normal line and the error line\n",
    "                plt.fill_between(\n",
    "                    range(len(years)),\n",
    "                    yearly_occurrences,\n",
    "                    errors_plus,\n",
    "                    color=color,\n",
    "                    alpha=ALPHA_ERROR_ZONE,\n",
    "                    zorder=1,\n",
    "                )\n",
    "            i += 1\n",
    "\n",
    "            # plt.scatter(range(len(years)), errors, color='red', label=f\"{instance} (error)\", zorder=1)\n",
    "        stop_index = i\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        plt.title(\n",
    "            f\"Number of papers covering {type} instances (#{start_index+1} to #{stop_index} of {len(type_instances_sorted)})\"\n",
    "        )\n",
    "\n",
    "        # Inset for relative values\n",
    "        fig.canvas.draw()\n",
    "        x_lim = ax.get_xlim()  # Get the current x-axis limits from the main plot\n",
    "\n",
    "        bbox = ax.get_position()\n",
    "        bb_left, bb_bottom = bbox.x0, bbox.y0\n",
    "        bb_width, bb_height = bbox.width, bbox.height\n",
    "\n",
    "        ax_inset = plt.axes(\n",
    "            [bb_left, 0.05, bb_width, 0.15],\n",
    "            alpha=ALPHA_PAPER_BAR,\n",
    "            facecolor=\"lightgrey\",\n",
    "        )\n",
    "        for i, instance in enumerate(\n",
    "            type_instances_sorted[start_index:stop_index], start=start_index\n",
    "        ):\n",
    "            yearly_occurrences = type_matrix[:, i]\n",
    "            values_relative = [\n",
    "                occurrences / papers if papers > 0 else 0\n",
    "                for occurrences, papers in zip(yearly_occurrences, yearly_papers)\n",
    "            ]\n",
    "            line_relative = ax_inset.plot(\n",
    "                range(len(years)),\n",
    "                values_relative,\n",
    "                label=f\"{instance} (relative)\",\n",
    "                zorder=3,\n",
    "            )[0]\n",
    "\n",
    "            # add the error part\n",
    "            if error_matrix is not None and instance in error_instances:\n",
    "                color = line_relative.get_color()\n",
    "                errors = error_matrix[:, error_instances.index(instance)]\n",
    "                errors_plus = yearly_occurrences + errors\n",
    "                errors_relative = [\n",
    "                    error / papers if papers > 0 else 0\n",
    "                    for error, papers in zip(errors_plus, yearly_papers)\n",
    "                ]\n",
    "                if max(errors_relative) > 1:\n",
    "                    print(f\"Error: {instance} has a relative error > 1\")\n",
    "                    # throw an exception because this should never be the case:\n",
    "                    # raise Exception(f\"Error: relative {instance} occurrence + error > 1\")\n",
    "\n",
    "                ax_inset.plot(\n",
    "                    range(len(years)),\n",
    "                    errors_relative,\n",
    "                    alpha=ALPHA_ERROR_LINE,\n",
    "                    color=color,\n",
    "                    label=f\"{instance} (error, relative)\",\n",
    "                    zorder=2,\n",
    "                )\n",
    "                # color in the area between the normal line and the error line\n",
    "                ax_inset.fill_between(\n",
    "                    range(len(years)),\n",
    "                    values_relative,\n",
    "                    errors_relative,\n",
    "                    alpha=ALPHA_ERROR_ZONE,\n",
    "                    color=color,\n",
    "                    zorder=1,\n",
    "                )\n",
    "\n",
    "        ax_inset.set_xlim(x_lim)\n",
    "\n",
    "        ax_inset.set_xticks([])\n",
    "        ax_inset.set_yticks(np.arange(0, 1.1, step=0.5))\n",
    "        ax_inset.set_yticklabels(\n",
    "            [f\"{int(x*100)}%\" for x in np.arange(0, 1.1, step=0.5)], fontsize=8\n",
    "        )\n",
    "\n",
    "        # set y axis label\n",
    "        ax_inset.set_ylabel(\"relative\", fontsize=10)\n",
    "\n",
    "        plt.subplots_adjust(bottom=0.3)\n",
    "\n",
    "        start_string = f\"{start_index+1}\"\n",
    "        stop_string = f\"{stop_index}\"\n",
    "\n",
    "        # fill up with 0 to have a constant length\n",
    "        start_string = \"0\" * (3 - len(start_string)) + start_string\n",
    "        stop_string = \"0\" * (3 - len(stop_string)) + stop_string\n",
    "\n",
    "        part_appendix = f\"{start_string}_to_{stop_string}\"\n",
    "        filepath = os.path.join(path, name)\n",
    "        plt.savefig(f\"{filepath}_{type.replace(' ', '_')}_{part_appendix}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        start_index = stop_index\n",
    "        if start_index < len(type_instances_sorted):\n",
    "            # if recursion_depth > 0:\n",
    "            #     break\n",
    "            visualize_timeline(\n",
    "                config,\n",
    "                year_instance_occurrence_matrix,\n",
    "                year_papers,\n",
    "                instances,\n",
    "                {type: instance_types_dicts[type]},\n",
    "                name,\n",
    "                path=path,\n",
    "                recursion_depth=recursion_depth + 1,\n",
    "                start_index=start_index,\n",
    "                error_matrix=error_matrix,\n",
    "                error_instances=error_instances,\n",
    "            )\n",
    "        start_index = 0\n",
    "\n",
    "\n",
    "# if config.visualize:\n",
    "#     yearly_error_matrix, year_error_papers = create_year_paper_occurrence_matrix(\n",
    "#         papers_metadata, error_matrix, error_papers, is_error_matrix=True\n",
    "#     )\n",
    "#     visualize_timeline(\n",
    "#         config,\n",
    "#         year_instance_occurrence_matrix,\n",
    "#         year_papers,\n",
    "#         instances,\n",
    "#         instance_types_dicts,\n",
    "#         name=\"year_instance_occurrence_matrix\",\n",
    "#         error_matrix=yearly_error_matrix,\n",
    "#         error_instances=error_instances,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year_paper_occurrence_matrix\n",
    "class YearPaperOccurrenceMatrixBuilder(MatrixBuilder):\n",
    "    def __init__(self, director, papers = None, paper_instance_occurrence_matrix = None, is_error_matrix=False, ):\n",
    "        super().__init__(director)\n",
    "\n",
    "        # self.papers_metadata = papers_metadata\n",
    "        self.papers:dict[str,Instance] = papers or director.papers\n",
    "        self.paper_instance_occurrence_matrix = paper_instance_occurrence_matrix or director.builder[\"occurrence_matrix\"].matrix\n",
    "        self.is_error_matrix = is_error_matrix\n",
    "        self.year_papers:dict[int,dict[str,Instance]] = {}\n",
    "\n",
    "\n",
    "\n",
    "    def build_matrix(self, paper_instance_occurrence_matrix = None, papers = None, is_error_matrix=False):\n",
    "        paper_instance_occurrence_matrix = paper_instance_occurrence_matrix or self.paper_instance_occurrence_matrix\n",
    "        papers = papers or self.papers\n",
    "        # self.matrix, self.year_papers = create_year_paper_occurrence_matrix(\n",
    "        #     papers_metadata, paper_instance_occurrence_matrix, papers, is_error_matrix\n",
    "        # )\n",
    "\n",
    "        # def create_year_paper_occurrence_matrix(\n",
    "        #     papers_metadata, paper_instance_occurrence_matrix, papers, is_error_matrix=False\n",
    "        # ):\n",
    "        indexed_papers = {paper: i for i, paper in enumerate(papers)}\n",
    "        for paper, instance in self.papers.items():\n",
    "            if hasattr(instance, \"year\"):\n",
    "                year = int(getattr(instance, \"year\"))\n",
    "                if year not in self.year_papers:\n",
    "                    self.year_papers[year] = {}\n",
    "                self.year_papers[year][paper] = instance\n",
    "\n",
    "        earliest = min(self.year_papers)\n",
    "        latest = max(self.year_papers)\n",
    "        span = latest - earliest + 1\n",
    "\n",
    "        for year in range(earliest, latest):\n",
    "            if year not in self.year_papers:\n",
    "                self.year_papers[year] = []\n",
    "\n",
    "        self.year_papers = {\n",
    "            k: v for k, v in sorted(self.year_papers.items(), key=lambda item: item[0])\n",
    "        }\n",
    "\n",
    "        if is_error_matrix:\n",
    "            # convert any value != 0 to 1\n",
    "            paper_instance_occurrence_matrix = np.where(\n",
    "                paper_instance_occurrence_matrix != 0, 1, 0\n",
    "            )\n",
    "\n",
    "        # create a year_instance_occurrence matrix from the paper_instance_occurrence_matrix\n",
    "        year_instance_occurrence_matrix = np.zeros(\n",
    "            (span, paper_instance_occurrence_matrix.shape[1]), dtype=int\n",
    "        )\n",
    "        for yearID, year in enumerate(self.year_papers):\n",
    "            for paper in self.year_papers[year]:\n",
    "                if paper in papers:\n",
    "                    paperID = indexed_papers[paper]\n",
    "                    year_instance_occurrence_matrix[\n",
    "                        yearID\n",
    "                    ] += paper_instance_occurrence_matrix[paperID]\n",
    "\n",
    "    def build(self):\n",
    "        self.build_matrix()\n",
    "\n",
    "\n",
    "director.builder['year_instance_occurrence_matrix'] = YearPaperOccurrenceMatrixBuilder(director)\n",
    "director.builder['year_instance_occurrence_matrix'].build()\n",
    "\n",
    "# year_instance_occurrence_matrix, year_papers = create_year_paper_occurrence_matrix(\n",
    "#     papers_metadata, paper_instance_occurrence_matrix, papers\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Complete\n",
    "\n",
    "We now have:\n",
    "\n",
    "| Variable                          | Type    | Size         | Comments |\n",
    "|-----------------------------------|---------|--------------|----------|\n",
    "| error_instances                   | list    | 165          | Comments |\n",
    "| error_matrix                      | ndarray | (999, 165)   | Comments |\n",
    "| error_papers                      | list    | 999          | Comments |\n",
    "| gap_too_large_threshold           | int     | n.a.         | Comments |\n",
    "| instance_piece_gap                | dict    | 151          | Comments |\n",
    "| instance_types_dicts              | dict    | 5            | Comments |\n",
    "| instances                         | list    | 315          | Comments |\n",
    "| paper_full_text                   | dict    | 1029         | Comments |\n",
    "| paper_instance_occurrence_matrix  | ndarray | (1003, 315)  | Comments |\n",
    "| papers                            | list    | 1003         | Comments |\n",
    "| pos_in_paper                      | dict    | 1003         | Comments |\n",
    "\n",
    "Consisting of:\n",
    "* The paper_instance_occurrence_matrix, binary listing if a term (instance) is present in a paper\n",
    "  * papers x instances\n",
    "* The error_matrix, of all instances that were dropped from the paper_instance_occurrence_matrix\n",
    "  * error_papers x error_instances\n",
    "\n",
    "And some leftover variables:\n",
    "* instance_types_dicts, listing all instance types (\"process\", \"software\", ...) and their respective instance sets (\"Curation\", \"Knowledge Work\", ...)\n",
    "* paper_full_text, containing each papers full text\n",
    "  * pos_in_paper, listing for each paper: for each instance: each position of that instance in that papers full text.\n",
    "* instance_piece_gap, a dict listing all instances made up from compound words (e.g. \"Knowledge Work\", and their minimum distance in each papers full text)\n",
    "  * gap_too_large_threshold, defining how far appart a finding of \"Knowledge\" and \"Work\" would qualify as \"Knowledge Work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~3 min | {( len(papers) * len(instances) ) / (3 * 1000) }seconds  compare proximity of all instances with one antoher\n",
    "# ~8 min right now.\n",
    "# 3 min 30 sec with 164 papers and 339 instances\n",
    "class ProximityMatrixBuilder(MatrixBuilder):\n",
    "    def __init__(self, director:Director, instances = None, papers = None, pos_in_paper = None, mode = \"sqrt\"):\n",
    "        super().__init__(director)\n",
    "\n",
    "        self.instances:dict[str,Instance] = instances or director.instances\n",
    "        self.papers:dict[str,Instance] = papers or director.papers\n",
    "        self.pos_in_paper:PosInPaper = pos_in_paper or director.pos_in_paper\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "    def build_matrix(self, instances = None, papers = None, pos_in_paper = None):\n",
    "        instances = instances or self.instances\n",
    "        papers = papers or self.papers\n",
    "        pos_in_paper = pos_in_paper or self.pos_in_paper\n",
    "\n",
    "        # self.matrix, self.proximity_instances = calculate_proximity_matrix(\n",
    "        #     self.config, pos_in_paper, instances, mode=\"sqrt\"\n",
    "        # )\n",
    "\n",
    "    def build(self):\n",
    "        self.build_matrix()\n",
    "        self.remove_zeros()\n",
    "        self.instances = self.handle_deletions(self.instances)\n",
    "\n",
    "    @time_function\n",
    "    def build_matrix(self,\n",
    "        # config: Config,\n",
    "        # pos_in_paper: PosInPaper,\n",
    "        # instances,\n",
    "        # mode=\"sqrt\",\n",
    "        try_to_save_time=False,\n",
    "    ):\n",
    "        # TODO: Optimize this function.\n",
    "        # each instance needs to have it's occurrences as pieces clustered together, so that only those below max distance are considered\n",
    "\n",
    "        # create a np zeros matrix of size instances x instances\n",
    "        indexed_instances = {instance: i for i, instance in enumerate(self.instances)}\n",
    "\n",
    "        self.matrix = np.zeros(\n",
    "            (len(self.instances), len(self.instances)), dtype=float\n",
    "        )\n",
    "\n",
    "        # alternatives are:\n",
    "        # \"sqrt\" - 1 / (square root of the distance)\n",
    "        # \"linear\" - 1 / distance\n",
    "        # \"binary\" - 1 if distance < MAX_GAP_THRESHOLD, 0 otherwise\n",
    "        # \"log\" - 1 / log(distance)\n",
    "\n",
    "        # There is a chance that pos_in_paper papers and instances are out of sync with the current papers and instances\n",
    "        paperIDs = [\n",
    "            paperID for paperID, name in enumerate(pos_in_paper.papers) if name in self.papers\n",
    "        ]\n",
    "        lID_map = {\n",
    "            indexed_instances[name]: instanceID\n",
    "            for instanceID, name in enumerate(pos_in_paper.literals)\n",
    "            if name in self.instances\n",
    "        }\n",
    "\n",
    "        for id1 in range(len(self.instances)):\n",
    "            # print (f\"Processing {id1} of {len(instances)}: {instance1}\")\n",
    "            for id2 in range(id1 + 1, len(self.instances)):\n",
    "                # FIXME: this resulted in a matrix which was not symmetric.\n",
    "                # That hints at a problem with the calclulation, [id1][id2] and [id2][id1] should be the same\n",
    "                wcID = pos_in_paper.word_combination_index_literal_literal[lID_map[id1]][\n",
    "                    lID_map[id2]\n",
    "                ]\n",
    "                for paperID in paperIDs:\n",
    "                    distance = pos_in_paper.find_min_distance_by_id(paperID, wcID)\n",
    "\n",
    "                    if distance < 0:\n",
    "                        # print(f\"Error: {instance1} and {instance2} not found in {paper}\")\n",
    "                        continue\n",
    "                    result = 0.0\n",
    "                    if distance == 0:\n",
    "                        result = 1\n",
    "                    elif distance == 1:\n",
    "                        result = 1\n",
    "                    elif self.mode == \"sqrt\":\n",
    "                        result = 1 / np.sqrt(distance)\n",
    "                    elif self.mode == \"linear\":\n",
    "                        result = 1 / distance\n",
    "                    elif self.mode == \"binary\":\n",
    "                        result = 1 if distance < config.gap_too_large_threshold else 0\n",
    "                    elif self.mode == \"log\":\n",
    "                        result = 1 / np.log(distance)\n",
    "                    else:\n",
    "                        print(\"Error: unknown mode\")\n",
    "                        break\n",
    "                    if result > 0.0:\n",
    "                        self.matrix[id1][id2] += result\n",
    "                        self.matrix[id2][id1] += result\n",
    "\n",
    "        # TODO rest doesnt seem to work, short fix implemented:\n",
    "        # create a copy of labels that only contains instances that are in the proximity matrix\n",
    "\n",
    "        # instance_instance_proximity_matrix, deletions = remove_zeros(\n",
    "        #     instance_instance_proximity_matrix\n",
    "        # )\n",
    "        # proximity_instances = handle_deletions(instances, deletions, rows=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "director.pos_in_paper = pos_in_paper\n",
    "director.builder['proximity_matrix'] = ProximityMatrixBuilder(director)\n",
    "director.builder['proximity_matrix'].build()\n",
    "# instance_instance_proximity_matrix, proximity_instances = calculate_proximity_matrix(\n",
    "#     config, pos_in_paper, instances\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "\n",
    "def get_rules(matrix, columns):\n",
    "    # AttributeError: 'numpy.ndarray' object has no attribute 'dtypes'\n",
    "    dataframe = pd.DataFrame(matrix, columns=columns).astype(bool)\n",
    "\n",
    "    # for each process:\n",
    "    # create one res\n",
    "\n",
    "    res = apriori(dataframe, min_support=0.4, use_colnames=True, max_len=2)\n",
    "\n",
    "    # visualize res\n",
    "    res = res.sort_values(by=\"support\", ascending=False)\n",
    "    res = res.reset_index(drop=True)\n",
    "    # res\n",
    "\n",
    "    rules = association_rules(res)\n",
    "    # sort rules by confidence\n",
    "    # rules = rules.sort_values(by='confidence', ascending=False)\n",
    "    rules = rules.sort_values(by=\"lift\", ascending=False)  # (propably most important)\n",
    "    # rules = rules.sort_values(by='leverage', ascending=False)\n",
    "    # export rules to csv\n",
    "    return rules\n",
    "\n",
    "\n",
    "rules = get_rules(director.builder[\"occurrence_matrix\"].matrix, list(director.instances.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules\n",
    "process_dataframe(config, rules, \"rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_cross_type_rules(rules, director:Director):\n",
    "    cross_type = [False] * len(rules)\n",
    "\n",
    "    for i, antecentent in enumerate(rules.antecedents):\n",
    "        if not isinstance(antecentent, str):\n",
    "            (antecentent,) = antecentent\n",
    "        consequent = rules.iloc[i].consequents\n",
    "        if not isinstance(consequent, str):\n",
    "            (consequent,) = consequent\n",
    "        type1, type2 = None, None\n",
    "        type1 = director.instances.get(antecentent, {}).get(\"instance_of\", [None])[0]\n",
    "        type2 = director.instances.get(consequent, {}).get(\"instance_of\", [None])[0]\n",
    "        # for type in director.classes:\n",
    "        #     if antecentent in instance_types_dicts[type]:\n",
    "        #         type1 = type\n",
    "        #     if consequent in instance_types_dicts[type]:\n",
    "        #         type2 = type\n",
    "        #     if type1 and type2:\n",
    "        #         break\n",
    "        if type1 and type2 and type1 != type2:\n",
    "            cross_type[i] = True\n",
    "            # print(rules.iloc[i])\n",
    "\n",
    "    # create a copy for all rules that are cross type\n",
    "    rules_cross_type = rules[cross_type].copy()\n",
    "    return rules_cross_type\n",
    "\n",
    "\n",
    "rules_cross_type = identify_cross_type_rules(rules, director)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_dataframe(config:Config, input_df, name = \"some_df\", path=None):\n",
    "#     if path is None:\n",
    "#         path = config.get_output_path()\n",
    "#     filepath = os.path.join(path, name)\n",
    "\n",
    "#     # convert all froensets to strings\n",
    "#     for col in input_df.columns:\n",
    "#         if isinstance(col[0], frozenset):\n",
    "#             # input_df[col] = input_df[col].apply(lambda x: \"_\".join(x))\n",
    "#             # input_df[col] = input_df[col].apply(lambda x: \"_\".join(x))\n",
    "#             input_df[col] = input_df[col].apply(lambda x: x + \"_HI!\")\n",
    "#             pass\n",
    "\n",
    "#     input_df.to_csv(filepath + '.csv', sep=config.csv_separator, decimal=config.csv_decimal)\n",
    "#     show(input_df)\n",
    "\n",
    "# rules_cross_type = identify_cross_type_rules(rules)\n",
    "\n",
    "process_dataframe(config, rules_cross_type, \"rules_cross_type\")\n",
    "# cross_type_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_done = False\n",
    "\n",
    "\n",
    "def print_kg_dict(config: Config, kg_dict, header):\n",
    "    filepath = os.path.join(config.get_output_path(), \"instance_relations.csv\")\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(header + \"\\n\")\n",
    "        total_comma = len(kg_dict) - 1\n",
    "        for pos1, type1 in enumerate(kg_dict):\n",
    "            preamble = \",\" * pos1\n",
    "            for pos2, type2 in enumerate(kg_dict[type1]):\n",
    "                intermediate = \",\" * (pos2 + 1)\n",
    "                rest_comma = \",\" * (total_comma - pos1 - pos2)\n",
    "                for i1, i2 in kg_dict[type1][type2]:\n",
    "                    f.write(preamble + i1 + intermediate + i2 + rest_comma + \"\\n\")\n",
    "\n",
    "\n",
    "def knowledge_graph_population_cross_type_rules(\n",
    "    config: Config, rules: association_rules, instance_types_dicts\n",
    "):\n",
    "    header = config.csv_separator.join(instance_types_dicts.keys())\n",
    "    # Triangular dict\n",
    "    dummy_dict = {}\n",
    "    for instance_type in instance_types_dicts:\n",
    "        dummy_dict[instance_type] = {}\n",
    "        for type in instance_types_dicts:\n",
    "            if type not in dummy_dict:\n",
    "                dummy_dict[instance_type][type] = []\n",
    "    for i, antecentent in enumerate(rules.antecedents):\n",
    "        (antecentent,) = antecentent\n",
    "        (consequent,) = rules.iloc[i].consequents\n",
    "        first_type = None\n",
    "        second_type = None\n",
    "        for type in instance_types_dicts:\n",
    "            if antecentent in instance_types_dicts[type]:\n",
    "                # type1 = type\n",
    "                if not first_type:\n",
    "                    first_type = type\n",
    "                    first_instance = antecentent\n",
    "                else:\n",
    "                    second_type = type\n",
    "                    second_instance = antecentent\n",
    "            if consequent in instance_types_dicts[type]:\n",
    "                if not first_type:\n",
    "                    first_type = type\n",
    "                    first_instance = consequent\n",
    "                else:\n",
    "                    second_type = type\n",
    "                    second_instance = consequent\n",
    "            if first_type and second_type:\n",
    "                break\n",
    "        if first_type != second_type:\n",
    "            dummy_dict[first_type][second_type].append(\n",
    "                (first_instance, second_instance)\n",
    "            )\n",
    "\n",
    "    print_kg_dict(config, dummy_dict, header)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "## Disabled. likely not needed anymore\n",
    "# try:\n",
    "#     kg_done = knowledge_graph_population_cross_type_rules(\n",
    "#         config, rules_cross_type, director\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     if config.debug:\n",
    "#         raise e\n",
    "#     else:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare csv file again\n",
    "# process,software,data item,data model,data format specification,interchange format,data visualization,data validation,inference,source\n",
    "\n",
    "\n",
    "@time_function\n",
    "def knowledge_graph_population(\n",
    "    config: Config,\n",
    "    instance_types_dicts,\n",
    "    property_types_dicts,\n",
    "    instance_instance_proximity_matrix,\n",
    "    proximity_instances,\n",
    "):\n",
    "    columns = list(instance_types_dicts.keys())\n",
    "    # columns += list(property_types_dicts.keys())\n",
    "    # columns = ['process', 'software', 'data item', 'data model', 'data format specification', 'data visualization', 'data validation', 'inference']\n",
    "\n",
    "    rows = []\n",
    "    for c_ID, column in enumerate(columns):\n",
    "        for instance in instance_types_dicts[column]:\n",
    "            # add the instance to the csv with each of their relations\n",
    "            if instance not in proximity_instances:\n",
    "                continue\n",
    "            instance_index = proximity_instances.index(instance)\n",
    "            for oc_ID, other_column in enumerate(columns):\n",
    "                if other_column not in instance_types_dicts:\n",
    "                    if other_column in property_types_dicts:\n",
    "                        # TODO: handle properties specially\n",
    "                        continue\n",
    "                    continue\n",
    "                if other_column != column:\n",
    "                    other_column_instances = instance_types_dicts[other_column]\n",
    "                    for other_instance in other_column_instances:\n",
    "                        if other_instance not in proximity_instances:\n",
    "                            continue\n",
    "                        other_instance_index = proximity_instances.index(other_instance)\n",
    "                        if (\n",
    "                            instance_instance_proximity_matrix[instance_index][\n",
    "                                other_instance_index\n",
    "                            ]\n",
    "                            > config.proximity_min_value\n",
    "                        ):\n",
    "                            # build row column by column\n",
    "                            row = [\"\"] * len(columns)\n",
    "                            row[c_ID] = instance\n",
    "                            row[oc_ID] = other_instance\n",
    "                            rows.append(row)\n",
    "\n",
    "    # write to csv\n",
    "    filepath = os.path.join(config.get_output_path(), \"instance_relations.csv\")\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(config.csv_separator.join(columns) + \"\\n\")\n",
    "        for row in rows:\n",
    "            f.write(config.csv_separator.join(row) + \"\\n\")\n",
    "    return True\n",
    "\n",
    "## Disabled. likely not needed anymore\n",
    "# if not kg_done:\n",
    "#     kg_done = knowledge_graph_population(\n",
    "#         config,\n",
    "#         instance_types_dicts,\n",
    "#         property_types_dicts,\n",
    "#         instance_instance_proximity_matrix,\n",
    "#         proximity_instances,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "import types\n",
    "\n",
    "\n",
    "# process,software,data item,data model,data format specification,interchange format,data visualization,data validation,inference,source\n",
    "# process,software,data item,data model,data format specification\n",
    "\n",
    "\n",
    "def save_as_owl(config: Config, path=None):\n",
    "    onto_path = config.ontology_path\n",
    "    df_cl = pd.read_csv(os.path.join(onto_path, \"classes.csv\"))\n",
    "    df_re = pd.read_csv(os.path.join(onto_path, \"relations.csv\"))\n",
    "    df_re = df_re.set_index(\"Domain\\Range\")\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    data_path = os.path.join(path, \"instance_relations.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    # df = pd.read_csv('data.csv')\n",
    "\n",
    "    onto = get_ontology(\"http://tib.eu/slr\")\n",
    "\n",
    "    df_contributions = pd.read_csv(\n",
    "        os.path.join(path, \"paper_instance_occurrence_matrix.csv\")\n",
    "    )\n",
    "    df_rules = pd.read_csv(os.path.join(path, \"rules_cross_type.csv\"))\n",
    "\n",
    "    with open(os.path.join(path, \"instance_types_dicts.json\")) as file:\n",
    "        inst_data = json.load(file)\n",
    "\n",
    "    onto = get_ontology(\"http://tib.eu/slr\")\n",
    "\n",
    "    with onto:\n",
    "\n",
    "        # Classes\n",
    "        for ind, row in df_cl.iterrows():\n",
    "            cl = types.new_class(row[\"URI\"], (Thing,))\n",
    "            cl.label = row[\"Label\"]\n",
    "            re = types.new_class(\n",
    "                f'has{row[\"Label\"].title().replace(\" \", \"\")}', (ObjectProperty,)\n",
    "            )\n",
    "            re.label = f'has {row[\"Label\"]}'\n",
    "\n",
    "        # Instances\n",
    "        for key, value in inst_data.items():\n",
    "            cl = onto.search_one(label=key)\n",
    "            if cl:\n",
    "                for item in value:\n",
    "                    inst = cl()\n",
    "                    inst.label = item\n",
    "\n",
    "        # Statements\n",
    "        Contribution = types.new_class(\"Contribution\", (Thing,))\n",
    "        mentions = types.new_class(\"mentions\", (ObjectProperty,))\n",
    "        mentions.label = \"mentions\"\n",
    "        for ind, row in df_contributions.iterrows():\n",
    "            contrib_inst = Contribution()\n",
    "            contrib_inst.label = row[0]\n",
    "            for col in df_contributions.columns:\n",
    "                if row[col]:\n",
    "                    inst = onto.search_one(label=col)\n",
    "                    if inst:\n",
    "                        contrib_inst.mentions.append(inst)\n",
    "\n",
    "        # Rules\n",
    "        for ind, row in df_rules.iterrows():\n",
    "            subj_inst = onto.search_one(label=row[\"antecedents\"])\n",
    "            obj_inst = onto.search_one(label=row[\"consequents\"])\n",
    "            if subj_inst and obj_inst:\n",
    "                obj_cl = obj_inst.is_a[0]\n",
    "                rel_label = f\"has {str(obj_cl.label[0])}\"\n",
    "                rel = onto.search_one(label=rel_label)\n",
    "                if rel:\n",
    "                    rel[subj_inst].append(obj_inst)\n",
    "\n",
    "    output_path = os.path.join(onto_path, \"onto.owl\")\n",
    "    # onto.save('onto.owl')\n",
    "    onto.save(output_path)\n",
    "    onto.destroy()\n",
    "\n",
    "\n",
    "# save_as_owl(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for ORKG\n",
    "# header:\n",
    "# paper:title,paper:authors,paper:publication_month,paper:publication_year,paper:published_in,paper:research_field,paper:doi,paper:url,contribution:research_problem,contribution:extraction_method,Property 1,Property 2\n",
    "\n",
    "\n",
    "def flatten_nested_properties(data, pefix=\"\"):\n",
    "    res = {}\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            res.update(flatten_nested_properties(value, f\"{pefix}{key}:\"))\n",
    "        else:\n",
    "            res[f\"{pefix}{key}\"] = value\n",
    "    return res\n",
    "\n",
    "\n",
    "class Paper:\n",
    "    order = [\n",
    "        \"title\",\n",
    "        \"authors\",\n",
    "        \"publication_month\",\n",
    "        \"publication_year\",\n",
    "        \"published_in\",\n",
    "        \"research_field\",\n",
    "        \"doi\",\n",
    "        \"url\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, paperID, data={}):\n",
    "        self.paperID: str = paperID\n",
    "        self.title: str = data.get(\"title\", \"\")\n",
    "        ## now handled later\n",
    "        # if self.title:\n",
    "        #     self.title = '\"' + self.title + '\"'\n",
    "        if self.title and \"{\" in self.title or \"}\" in self.title:\n",
    "            self.title = self.title.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "        self.authors: list[str] = data.get(\"author\", \"\")\n",
    "        if isinstance(self.authors, str):\n",
    "            authors = self.authors.split(\"and \")\n",
    "            if authors:\n",
    "                for i, author in enumerate(authors):\n",
    "                    name = author.split(\",\")\n",
    "                    if len(name) > 1:\n",
    "                        name = f\"{name[1].strip()} {name[0].strip()}\"\n",
    "                    else:\n",
    "                        name = name[0].strip()\n",
    "                    authors[i] = name\n",
    "            self.authors = \"; \".join(authors)\n",
    "        self.publication_month: int = data.get(\"publication_month\", \"\")\n",
    "        self.publication_year: int = data.get(\"year\", \"\")\n",
    "        self.published_in = \"\"\n",
    "        for key in [\"journal\", \"conference\", \"journal\"]:\n",
    "            if key in data:\n",
    "                self.published_in = data[key]\n",
    "                break\n",
    "        self.research_field: str = data.get(\"research_field\", \"\")\n",
    "        if not self.research_field:\n",
    "            # TODO: find a way to get the research field\n",
    "            self.research_field = \"R195\"\n",
    "        self.doi: str = data.get(\"doi\", \"\")\n",
    "        self.url: str = data.get(\"url\", \"\")\n",
    "\n",
    "\n",
    "class Contribution:\n",
    "    def __init__(self, paperID, properties={}):\n",
    "        self.paperID: str = paperID\n",
    "        self.properties: dict = flatten_nested_properties(properties)\n",
    "\n",
    "\n",
    "class ORKGComparison:\n",
    "    def __init__(self):\n",
    "        self.papers = {}  # paperID:paper data\n",
    "        self.contibutions: dict[str:Contribution] = {}  # paperID:contribution data\n",
    "        self.properties = {}\n",
    "\n",
    "    def populate(\n",
    "        self,\n",
    "        config: Config,\n",
    "        papers,\n",
    "        instances,\n",
    "        instance_types_dicts,\n",
    "        paper_instance_occurrence_matrix,\n",
    "        papers_metadata,\n",
    "    ):\n",
    "        # Create a dictionary to hold the count of existing values for each property\n",
    "        property_ranges = {\n",
    "            property: sum(\n",
    "                value in instances for value in values\n",
    "            )  # Count how many values exist in 'instances'\n",
    "            for property, values in instance_types_dicts.items()  # Iterate over each property and its values\n",
    "        }\n",
    "        # floor = 0\n",
    "        # for prop, value in property_ranges.items():\n",
    "        #     property_ranges[prop] += floor\n",
    "        #     floor += value\n",
    "\n",
    "        for paperID, paper in enumerate(papers):\n",
    "            paper_data = papers_metadata.get(paper, {})\n",
    "            self.papers[paperID] = Paper(paper, paper_data)\n",
    "            properties = {prop: [] for prop in property_ranges}\n",
    "            floor = 0\n",
    "            for prop, prop_range in property_ranges.items():\n",
    "                for i in range(floor, floor + prop_range):\n",
    "                    if paper_instance_occurrence_matrix[paperID][i] == 1:\n",
    "                        properties[prop].append(instances[i])\n",
    "                floor += prop_range\n",
    "            self.contibutions[paperID] = Contribution(paper, properties)\n",
    "        return self\n",
    "\n",
    "    def populate_properties(self):\n",
    "        for contribution in self.contibutions.values():\n",
    "            for prop, value in contribution.properties.items():\n",
    "                len_values = len(value) if isinstance(value, list) else 1\n",
    "                if prop not in self.properties or self.properties[prop] < len_values:\n",
    "                    self.properties[prop] = len_values\n",
    "        return self.properties\n",
    "\n",
    "    def get(self, key):\n",
    "        if key == \"properties\" and not self.properties:\n",
    "            self.populate_properties()\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def save(self, config: Config, path=None, name=\"orkg_comparison\"):\n",
    "        if path is None:\n",
    "            path = config.orkg_path\n",
    "        filepath = os.path.join(path, name)\n",
    "        if not filepath.endswith(\".csv\"):\n",
    "            filepath += \".csv\"\n",
    "\n",
    "        rows = []\n",
    "        row = [\"paper:\" + prop for prop in Paper.order]\n",
    "        for prop, count in self.get(\"properties\").items():\n",
    "            # row += [f\"contribution:{prop}\"] * count\n",
    "            row += [prop] * count\n",
    "        rows.append(row)\n",
    "\n",
    "        for paperID, contribution in self.contibutions.items():\n",
    "            paper = self.papers[paperID]\n",
    "            row = [getattr(paper, key, \"\") for key in Paper.order]\n",
    "            for prop, count in self.properties.items():\n",
    "                value = contribution.properties.get(prop, \"\")\n",
    "                if not isinstance(value, list):\n",
    "                    value = [value]\n",
    "                len_taken = len(value)\n",
    "                if len_taken < count:\n",
    "                    value += [\"\"] * (count - len_taken)\n",
    "                row += value\n",
    "            rows.append(row)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for row in rows:\n",
    "                for id, item in enumerate(row):\n",
    "                    if config.csv_separator in item:\n",
    "                        if item.startswith('\"') and item.endswith('\"'):\n",
    "                            continue\n",
    "                        row[id] = '\"' + item + '\"'\n",
    "                f.write(config.csv_separator.join(row) + \"\\n\")\n",
    "\n",
    "\n",
    "# orkg_comparison = ORKGComparison()\n",
    "# orkg_comparison.populate(\n",
    "#     config,\n",
    "#     papers,\n",
    "#     instances,\n",
    "#     instance_types_dicts,\n",
    "#     paper_instance_occurrence_matrix,\n",
    "#     papers_metadata,\n",
    "# )\n",
    "# orkg_comparison.save(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, builder in director.builder.items():\n",
    "    if not hasattr(builder, \"matrix\"):\n",
    "        continue\n",
    "    if name in [\"year_instance_occurrence_matrix\"]:\n",
    "        continue\n",
    "    rows = []\n",
    "    candidates = [\"papers\", \"instances\"]\n",
    "    for candidate in candidates:\n",
    "        if hasattr(builder, candidate):\n",
    "            rows = getattr(builder, candidate)\n",
    "            break\n",
    "    if not rows:\n",
    "        raise Exception(f\"Could not find rows for {name}\")\n",
    "    if isinstance(rows, dict):\n",
    "        rows = list(rows.keys())\n",
    "    \n",
    "    cols = []\n",
    "    candidates = [\"instances\", \"literals\"]\n",
    "    for candidate in candidates:\n",
    "        if hasattr(builder, candidate):\n",
    "            cols = getattr(builder, candidate)\n",
    "            break\n",
    "    if not cols:\n",
    "        raise Exception(f\"Could not find cols for {name}\")\n",
    "    if isinstance(cols, dict):\n",
    "        cols = list(cols.keys())\n",
    "    \n",
    "    builder.save()\n",
    "    # process_matrix(director.config, builder.matrix, rows, cols, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.visualize = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_list(config, instances, \"instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Dicts: instance_types_dicts, papers_metadata, instance_piece_gap\n",
    "# process_dict(config, instance_types_dicts, \"instance_types_dicts\")\n",
    "# process_dict(config, papers_metadata, \"papers_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     paper_instance_occurrence_matrix,\n",
    "#     rows=papers,\n",
    "#     columns=instances,\n",
    "#     name=\"paper_instance_occurrence_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     error_matrix,\n",
    "#     rows=error_papers,\n",
    "#     columns=error_instances,\n",
    "#     name=\"error_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     instance_instance_co_occurrence_matrix,\n",
    "#     rows=instances,\n",
    "#     columns=instances,\n",
    "#     name=\"instance_instance_co_occurrence_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     instance_instance_relative_co_occurrence_matrix,\n",
    "#     rows=instances,\n",
    "#     columns=instances,\n",
    "#     name=\"instance_instance_relative_co_occurrence_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     instance_instance_proximity_matrix,\n",
    "#     rows=proximity_instances,\n",
    "#     columns=proximity_instances,\n",
    "#     name=\"instance_instance_proximity_matrix\",\n",
    "#     instance_types_dicts=instance_types_dicts,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "## Pre-Processing\n",
    "Using Completion Rating in %\n",
    "\n",
    "### 80 %: Full Text extraction\n",
    "* lacking noise removal (Headings, page numbers, ...)\n",
    "* lacking line-break mending\n",
    "\n",
    "### 100 %: Bag of Words\n",
    "* The problem with BoW that the words are looked at seperatly and correlation is not really clear.\n",
    "\n",
    "\n",
    "### 99 %: TF-IDF\n",
    "* tf-idf only on terms\n",
    "\n",
    "### ? %: Part Of Speech (POS) Tagging, Named Entity Recognition (NER) \n",
    "* ready, but not used currently\n",
    "\n",
    "## Visualize\n",
    "\n",
    "### 85 % Matrix\n",
    "* CSV and Dataframe dumps work fine\n",
    "* Visualization as PNG or SVG are extremely large.\n",
    "  * DPI regulation works to somewhat keep this in check, but images still reach 20 MB\n",
    "* An interactive matrix would be preferred.\n",
    "  * If you hover on a cell, it shows you the x and y label and it's value.\n",
    "\n",
    "### 100 % Timeline\n",
    "* arrange the papers on a timeline and identify the flow of:\n",
    "  * Processes\n",
    "  * File formats\n",
    "  * software\n",
    "  * ...\n",
    "* Additional ideas:\n",
    "  * Compare this to goolge trends\n",
    "\n",
    "### GraphDB\n",
    "* Visualize\n",
    "\n",
    "## Future Work\n",
    "Using Difficulty ranked (DR) solutions:\n",
    "\n",
    "### Step 0: Look it up\n",
    "\n",
    "#### Wikidata linking & more\n",
    "* https://openrefine.org/\n",
    "\n",
    "#### More visualization\n",
    "* https://github.com/JasonKessler/scattertext \n",
    "* https://pypi.org/project/yellowbrick/\n",
    "\n",
    "#### NLP Pipelines:\n",
    "https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "\n",
    "#### BLAST: Basic Local Alignment Search Tool\n",
    "  * starting point: https://academic.oup.com/bioinformatics/article/39/12/btad716/7450067\n",
    "\n",
    "#### AMIE 3\n",
    "  * https://luisgalarraga.de/docs/amie3.pdf\n",
    "  * https://github.com/dig-team/amie\n",
    "\n",
    "### Step 1: Low hanging fruits\n",
    "\n",
    "#### 1/5 DR: multi-word detection (n-gram)\n",
    "Tools:  nltk, spaCy, etc.\n",
    "\n",
    "### Step 2: Not-to-tricky follow-up\n",
    "\n",
    "#### 3/5 DR: Acronym Expansion\n",
    "Tools: spaCy - https://spacy.io/universe/project/neuralcoref\n",
    "\n",
    "#### 3/5 DR: CoReference resolution\n",
    "Tools: spaCy - https://spacy.io/universe/project/neuralcoref or https://huggingface.co/coref/ (you can use the model out of the box)\n",
    "\n",
    "### Step 3: Vector-magic\n",
    "\n",
    "#### 2-4/5 DR: Word embedding\n",
    "* Find out, that jpeg and png are similar\n",
    "\n",
    "(depending on your needs) - Tools: gensim - https://www.analyticsvidhya.com/blog/2023/07/step-by-step-guide-to-word2vec-with-gensim/\n",
    "\n",
    "#### 3/5 DR: document embedding\n",
    "Tools: gensim - https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "I would also check graph embeddings, sentence embeddings, and recently there is LLM2Vec\n",
    "\n",
    "### Step 3.1: Reaping the vector-rewards\n",
    "\n",
    "#### 1/5 DR: clustering\n",
    "Tools: sklearn\n",
    "\n",
    "Requirements: Need to have data as numbers first. This is quite possible after generating embeddings\n",
    "\n",
    "### Step 9: Won't be happening in this paper\n",
    "* Paper classes\n",
    "* Subclasses of paper classes\n",
    "* model which process is a subprocess of another process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
