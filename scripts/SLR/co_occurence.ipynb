{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and modify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, for_git=True):\n",
    "        self.for_git = for_git\n",
    "        self.visualize = not for_git\n",
    "        self.csv_separator = ',' if for_git else ';'\n",
    "        self.csv_decimal = '.' if for_git else ','\n",
    "        self.only_included_papers = True\n",
    "        self.properties = ['source']\n",
    "        self.proximity_mode = \"sqrt\"  # \"log\" mode is untested/unsafe, prefer \"sqrt\"\n",
    "        self.base_path = 'data/'  # Default base path\n",
    "        self.subset_path = 'data_subset/'\n",
    "        self.visualization_path = 'visualization/'\n",
    "        self.papers_path = 'G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/02_nlp'\n",
    "        self.review_path = 'G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/03_notes'\n",
    "        self.csv_file = 'C:/workspace/borgnetzwerk/tools/scripts/SLR/data.csv'\n",
    "        self.gap_too_large_threshold = 1000\n",
    "        self.savetime_on_fulltext = False   # If True, operations on fulltext will be kept to a minimum\n",
    "        self.try_to_save_time = False\n",
    "        self.recalculate_pos_in_paper = False # while the calculation is inefficient, just load from file\n",
    "\n",
    "\n",
    "    def get_output_path(self, path=\"\", visualization=False):\n",
    "        \"\"\"\n",
    "        Determine the output path based on the configuration and parameters.\n",
    "        \"\"\"\n",
    "        if not path:\n",
    "            path = self.subset_path if self.only_included_papers else self.base_path\n",
    "        if visualization:\n",
    "            path = os.path.join(path, self.visualization_path)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        return path\n",
    "\n",
    "# Usage\n",
    "config = Config(for_git=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug test\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "def run_debug_test(config: Config, instances:list[str] = None, papers:list[str] = None, paper_instance_occurrence_matrix:np.ndarray = None):\n",
    "    if \"ikewiki\" in instances:\n",
    "        ike_index = instances.index(\"ikewiki\")\n",
    "        sum_ikewiki = np.sum(paper_instance_occurrence_matrix[:,ike_index])\n",
    "        if sum_ikewiki > 1:\n",
    "            raise Exception(f\"Only one paper should contain 'ikewiki', but {sum_ikewiki} do.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_function(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        appendix = \"\"\n",
    "        # if instances in args:\n",
    "        if \"instances\" in kwargs:\n",
    "            # append len of instances\n",
    "            appendix = f\"({len(kwargs['instances'])} instances\"\n",
    "        if \"papers\" in kwargs:\n",
    "            if appendix:\n",
    "                appendix += \", \"\n",
    "            appendix += f\"{len(kwargs['papers'])} papers\"\n",
    "        if appendix:\n",
    "            appendix += \")\"\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{func.__name__} executed in {end_time - start_time} seconds\" + appendix)\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful functions\n",
    "def index_all(input_list, item):\n",
    "    return [i for i, x in enumerate(input_list) if x == item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Paper Metadata\n",
    "from bnw_tools.extract import util_zotero\n",
    "\n",
    "\n",
    "def get_paper_nlp_paths(papers_path):\n",
    "    paper_nlp_paths = {}\n",
    "    for file in os.listdir(papers_path):\n",
    "        if file.endswith(\".json\"):\n",
    "            paper_nlp_paths[file[:-5]] = os.path.join(papers_path, file)\n",
    "    return paper_nlp_paths\n",
    "\n",
    "def reduce_to_reviewed_papers(papers, review_path):\n",
    "    # todo: sort by review score + average rank\n",
    "    included_papers = []\n",
    "    excluded_papers = []\n",
    "    included_identifier = [\"review_score:: 3\", \"review_score:: 4\", \"review_score:: 5\"]\n",
    "    excluded_identifier = [\"review_score:: 2\", \"review_score:: 1\", \"review_score:: 0\"]\n",
    "    for file in os.listdir(review_path):\n",
    "        if file.endswith(\".md\"):\n",
    "            paper_name = file[:-3]\n",
    "            if paper_name in papers:\n",
    "                # check if file contains \"reviewed\"ArithmeticError\n",
    "                with open(os.path.join(review_path, file), 'r', encoding=\"utf8\") as f:\n",
    "                    content = f.read()\n",
    "                    for id in included_identifier:\n",
    "                        if id in content:\n",
    "                            included_papers.append(paper_name)\n",
    "                            break\n",
    "                    for id in excluded_identifier:\n",
    "                        if id in content:\n",
    "                            excluded_papers.append(paper_name)\n",
    "                            break\n",
    "    return included_papers, excluded_papers\n",
    "\n",
    "@time_function\n",
    "def get_paper_metadata(papers, path):\n",
    "    papers_metadata = {}\n",
    "\n",
    "    bib_resources = util_zotero.BibResources(path)\n",
    "\n",
    "    for paper in papers:\n",
    "        for entry in bib_resources.entries:\n",
    "            if hasattr(bib_resources.entries[entry], 'file') and paper in bib_resources.entries[entry].file:\n",
    "                papers_metadata[paper] = bib_resources.entries[entry].get_dict()\n",
    "                del bib_resources.entries[entry]\n",
    "                break\n",
    "\n",
    "\n",
    "    print(f\"{len(papers_metadata)} out of {len(papers)} papers have metadata.\")\n",
    "\n",
    "    return papers_metadata\n",
    "\n",
    "paper_nlp_paths = get_paper_nlp_paths(config.papers_path)\n",
    "\n",
    "papers = list(paper_nlp_paths.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only included papers are considered. Excluded or not reviewed papers are removed.\n"
     ]
    }
   ],
   "source": [
    "def exclude_papers(paper_nlp_paths, papers, included_papers, only_included_papers):\n",
    "    if only_included_papers:\n",
    "        deletions = [False] * len(papers)\n",
    "        for p_ID, paper in enumerate(paper_nlp_paths):\n",
    "            if paper not in included_papers:\n",
    "                deletions[p_ID] = True\n",
    "        for p_ID, deletion in enumerate(deletions):\n",
    "            if deletion:\n",
    "                paper_nlp_paths.pop(papers[p_ID])\n",
    "        papers = included_papers\n",
    "        print(\"Only included papers are considered. Excluded or not reviewed papers are removed.\")\n",
    "    return paper_nlp_paths, papers\n",
    "\n",
    "included_papers, excluded_papers = reduce_to_reviewed_papers(papers, config.review_path)\n",
    "\n",
    "paper_nlp_paths, papers = exclude_papers(paper_nlp_paths, papers, included_papers, only_included_papers = config.only_included_papers)\n",
    "\n",
    "# Free memory\n",
    "del included_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no new Zotero export at G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR:\n",
      "There should be a folder called 'files'\n",
      "We now have 1035 PDFs stored at G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR\\00_PDFs\n",
      "164 out of 164 papers have metadata.\n",
      "get_paper_metadata executed in 16.63345980644226 seconds\n"
     ]
    }
   ],
   "source": [
    "# extract the matadata from bibtex\n",
    "papers_metadata = get_paper_metadata(papers, 'G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort papers by year\n",
    "papers = sorted(papers, key=lambda x: papers_metadata[x]['year'] if 'year' in papers_metadata[x] else \"9999\")\n",
    "# sort paper_nlp_paths and papers_metadata accordingly\n",
    "paper_nlp_paths = {k: paper_nlp_paths[k] for k in papers}\n",
    "papers_metadata = {k: papers_metadata[k] for k in papers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: find occurrences of instances in bag of words of papers\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_string(input_string, delimiters = [\" \", \"-\", \"_\"]):\n",
    "    for delimiter in delimiters:\n",
    "        input_string = \" \".join(input_string.split(delimiter))\n",
    "    return input_string.split()\n",
    "\n",
    "def csv_to_dict_of_sets(csv_file, config: Config, prune_nan = True):\n",
    "    dict_of_sets = {}\n",
    "    # try:\n",
    "    #     df = pd.read_csv(csv_file)\n",
    "    # except pd.errors.ParserError:\n",
    "    #     print(\"Error parsing CSV file. Trying again with 'error_bad_lines=False'\")\n",
    "    # TODO: Specify modular separator and decimal here as well \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, on_bad_lines='warn', delimiter=config.csv_separator,  encoding=\"utf-8\")\n",
    "    except:\n",
    "        print(\"Error parsing CSV file. Trying again with 'encoding=ISO-8859-1'\")\n",
    "        df = pd.read_csv(csv_file, on_bad_lines='warn', delimiter=config.csv_separator, encoding='ISO-8859-1')\n",
    "    for column in df.columns:\n",
    "        dict_of_sets[column] = set(df[column].str.lower())\n",
    "        if prune_nan and np.nan in dict_of_sets[column]:\n",
    "            dict_of_sets[column].remove(np.nan)\n",
    "    # saved_column = df['process'] #you can also use df['column_name']\n",
    "    # delete all that exists in two or more columns\n",
    "    for key in dict_of_sets:\n",
    "        for other_key in dict_of_sets:\n",
    "            if key != other_key:\n",
    "                dict_of_sets[key] = dict_of_sets[key].difference(dict_of_sets[other_key])\n",
    "    return dict_of_sets\n",
    "\n",
    "def prune_properties(instance_types_dicts, properties_to_prune = [], prune_empty = True, prune_x = True):\n",
    "    properties = {}\n",
    "\n",
    "    \n",
    "    # merge \"interchange format\" into \"data format specification\"\n",
    "    if 'interchange format' in instance_types_dicts:\n",
    "        pruned = []\n",
    "        for key in instance_types_dicts['interchange format']:\n",
    "            if len(key) > 1:\n",
    "                instance_types_dicts['data format specification'].add(key)\n",
    "                pruned.append(key)\n",
    "        for key in pruned:\n",
    "            instance_types_dicts['interchange format'].remove(key)\n",
    "\n",
    "    for instance_type in instance_types_dicts:\n",
    "        prune = False\n",
    "        if instance_type in properties_to_prune:\n",
    "            prune = True\n",
    "        elif prune_empty and len(instance_types_dicts[instance_type]) == 0:\n",
    "            # prune empty sets\n",
    "            prune = True\n",
    "        elif prune_x and len(max(instance_types_dicts[instance_type], key=len)) < 2:\n",
    "            # prune sets with only one character entries\n",
    "            prune = True\n",
    "        \n",
    "        if prune:\n",
    "            properties[instance_type] = instance_types_dicts[instance_type]\n",
    "\n",
    "    for instance_type in properties:\n",
    "        instance_types_dicts.pop(instance_type)           \n",
    "\n",
    "    return instance_types_dicts, properties\n",
    "\n",
    "def count_occurrences(papers, instances):\n",
    "    occurrences = np.zeros((len(papers), len(instances)), dtype=int)\n",
    "\n",
    "    for p, paperpath in enumerate(papers.values()):\n",
    "        with open(paperpath, 'r', encoding=\"utf8\") as f:\n",
    "            paper = json.load(f)\n",
    "            for i, instance in enumerate(instances):\n",
    "                present = True\n",
    "                pieces = split_string(instance)\n",
    "                for piece in pieces:\n",
    "                    if piece.lower() not in paper['bag_of_words']:\n",
    "                        present = False\n",
    "                        break\n",
    "                    \n",
    "                # if instance == \"system integration\":\n",
    "                #     if \"Liu und Hu - 2013 - A reuse oriented representation model for capturin\" in paperpath:\n",
    "                #         print(present)\n",
    "                if present:\n",
    "                    occurrences[p][i] = 1\n",
    "    return occurrences\n",
    "\n",
    "# ---------------------- Variables ----------------------\n",
    "\n",
    "## instances: A list of all instances, regardless of their type\n",
    "# first all type 1, then all type 2, etc.\n",
    "# if possible, instance sare ordered by their occurrence\n",
    "\n",
    "## instances_dicts: A dictionary of all different types (columns) of instances\n",
    "#\n",
    "# types:\n",
    "#  - process\n",
    "#  - software\n",
    "#  - data item\n",
    "#  - data model\n",
    "#  - data format specification\n",
    "#  - interchange format\n",
    "#  - source\n",
    "#\n",
    "# instances_dicts['process']: A set of all instances of the type 'process'\n",
    "#\n",
    "instance_types_dicts = {}\n",
    "\n",
    "## paper_nlp_dict: A dictionary of all papers and their NLP data (as dict)\n",
    "\n",
    "## occurrences: A matrix of binary occurrences of instances in papers\n",
    "#\n",
    "# rows: papers\n",
    "# columns: instances\n",
    "# cells: 1 if instance is present in paper, 0 otherwise\n",
    "#\n",
    "paper_instance_occurrence_matrix = np.zeros((), dtype=int)\n",
    "\n",
    "\n",
    "# ---------------------- Main ----------------------\n",
    "\n",
    "# Usage example\n",
    "\n",
    "instance_types_dicts = csv_to_dict_of_sets(config.csv_file, config)\n",
    "\n",
    "# Extract instance types that are actually property types\n",
    "instance_types_dicts, property_types_dicts = prune_properties(instance_types_dicts, properties_to_prune=config.properties)\n",
    "\n",
    "def get_instances_list(instance_types_dicts):\n",
    "    instances = []\n",
    "    # merge all sets into one set\n",
    "    for instance_type in instance_types_dicts:\n",
    "        instances += (instance_types_dicts[instance_type])\n",
    "    return instances\n",
    "\n",
    "instances = get_instances_list(instance_types_dicts)\n",
    "\n",
    "paper_instance_occurrence_matrix = count_occurrences(paper_nlp_paths, instances)\n",
    "\n",
    "# free unneeded memory\n",
    "del paper_nlp_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "run_debug_test(config, instances, papers, paper_instance_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_instances(config: Config,matrix, instances, instance_types_dict=None,):\n",
    "    # total occurrences of each instance\n",
    "    instance_occurrences = {}\n",
    "    \n",
    "    for i, instance in enumerate(instances):\n",
    "        instance_occurrences[instance] = matrix[:, i].sum()\n",
    "    sorted_instances = {k: float(v) for k, v in sorted(instance_occurrences.items(), key=lambda item: item[1], reverse=True) if v > 0}\n",
    "    filepath = os.path.join(config.get_output_path(),'instance_occurrences')\n",
    "    with open(filepath + '.json', 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(sorted_instances, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    sorted_instance_list = list(sorted_instances.keys())\n",
    "\n",
    "\n",
    "    if instance_types_dict is not None:\n",
    "        # Instances should be sorted by their type\n",
    "        type_lists = [[] for _ in range(len(instance_types_dict))]\n",
    "        for instance in sorted_instance_list:\n",
    "            for type_ID, instance_type in enumerate(instance_types_dict):\n",
    "                if instance in instance_types_dict[instance_type]:\n",
    "                    type_lists[type_ID].append(instance)\n",
    "        type_sorted_instances = [item for sublist in type_lists for item in sublist]\n",
    "\n",
    "    new_order = [0] * len(sorted_instance_list)\n",
    "    for i, instance in enumerate(type_sorted_instances):\n",
    "        new_order[i] = instances.index(instance)\n",
    "\n",
    "    return type_sorted_instances, new_order\n",
    "\n",
    "def remove_zeros(matrix, columns=True, rows=True, row_lists=None, column_lists=None):\n",
    "    # remove all columns that are all zeros\n",
    "    if columns:\n",
    "        deleted_columns = np.all(matrix == 0, axis=0)\n",
    "        matrix = matrix[:, ~np.all(matrix == 0, axis=0)]\n",
    "\n",
    "    # remove all rows that are all zeros\n",
    "    if rows:\n",
    "        deleted_rows = np.all(matrix == 0, axis=1)\n",
    "        matrix = matrix[~np.all(matrix == 0, axis=1)]\n",
    "\n",
    "    \n",
    "    return matrix, [deleted_columns, deleted_rows]\n",
    "\n",
    "def update_instances(matrix, instances, instance_types_dict=None):\n",
    "    instances, new_order = sort_instances(config, matrix, instances, instance_types_dict)\n",
    "\n",
    "    new_order = np.array(new_order)\n",
    "    matrix = matrix[:, new_order]\n",
    "    \n",
    "    matrix, deletions = remove_zeros(matrix)\n",
    "    return matrix, instances, deletions\n",
    "\n",
    "paper_instance_occurrence_matrix, instances, deletions = update_instances(paper_instance_occurrence_matrix, instances, instance_types_dicts)\n",
    "\n",
    "def handle_deletions(input, deletions, rows = True):\n",
    "    \"\"\"\n",
    "    input: list, dict or np.ndarray\n",
    "    deletions: list of bools\n",
    "    rows: if True, deletions[1] is used, else deletions[0]\n",
    "    \"\"\"\n",
    "    delID = 1 if rows else 0\n",
    "\n",
    "    if deletions[delID].any():\n",
    "        # rows were deleted, in this case: papers\n",
    "        if isinstance(input, list):\n",
    "            input = [item for i, item in enumerate(input) if not deletions[delID][i]]\n",
    "        elif isinstance(input, dict):\n",
    "            input = {key: item for i, (key, item) in enumerate(input.items()) if not deletions[delID][i]}\n",
    "        elif isinstance(input, np.ndarray):\n",
    "            input = input[~deletions[delID]]\n",
    "    return input\n",
    "\n",
    "papers = handle_deletions(papers, deletions)\n",
    "# free unneeded memory\n",
    "del deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_debug_test(config, instances, papers, paper_instance_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all text files\n",
    "def get_paper_full_text(directory):\n",
    "    paper_full_text = {}\n",
    "    for folder in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    paper_full_text[file[:-4]] = file_path\n",
    "                    break\n",
    "\n",
    "    return paper_full_text\n",
    "\n",
    "paper_full_text = get_paper_full_text('G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/00_PDFs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: find occurrences of instances in full text of papers\n",
    "import sys\n",
    "from bisect import bisect_left\n",
    "from sortedcontainers import SortedSet\n",
    "\n",
    "\n",
    "class PosInPaper:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "        self.min_distances = {}\n",
    "        \"\"\"\n",
    "        Initialize the PosInPaper class with empty dictionaries for storing data and minimum distances.\n",
    "        \"\"\"\n",
    "\n",
    "    def save_to_file(self, path=None):\n",
    "        if path is None:\n",
    "            path = \"pos_in_paper\"\n",
    "        data_to_save = {paper: {literal: list(positions) for literal, positions in literals.items()}\n",
    "                        for paper, literals in self.data.items()}\n",
    "        with open(path + \".json\", 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(data_to_save, f, ensure_ascii=False)\n",
    "        data_to_save = {outer_key: {str(inner_key): value for inner_key, value in inner_dict.items()}\n",
    "                        for outer_key, inner_dict in self.min_distances.items()}\n",
    "\n",
    "        with open(path + \"_min_distances.json\", 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(data_to_save, f, ensure_ascii=False)\n",
    "\n",
    "    def load_from_file(self, path=None):\n",
    "        if path is None:\n",
    "            path = \"pos_in_paper\"\n",
    "        with open(path + \".json\", 'r', encoding=\"utf-8\") as f:\n",
    "            self.data = json.load(f)\n",
    "        with open(path + \"_min_distances.json\", 'r', encoding=\"utf-8\") as f:\n",
    "            min_distances_str_keys = json.load(f)\n",
    "            # Convert string keys back to frozensets\n",
    "            self.min_distances = {outer_key: {frozenset(eval(inner_key)): value for inner_key, value in inner_dict.items()}\n",
    "                                for outer_key, inner_dict in min_distances_str_keys.items()}\n",
    "        self.optimize_data()\n",
    "        \n",
    "    def add_occurrence(self, paper, literal, pos):\n",
    "        \"\"\"\n",
    "        Add an occurrence of a literal in a paper.\n",
    "\n",
    "        Parameters:\n",
    "        - paper (str): The identifier for the paper.\n",
    "        - literal (hashable): A literal that occurs in the paper. string or frozenset\n",
    "        - pos (int): The position of the literal's occurrence in the paper.\n",
    "        \"\"\"\n",
    "        if paper not in self.data:\n",
    "            self.data[paper] = {}\n",
    "        if literal not in self.data[paper]:\n",
    "            self.data[paper][literal] = []\n",
    "        self.data[paper][literal].append(pos)\n",
    "\n",
    "    def optimize_data(self):\n",
    "        \"\"\"\n",
    "        Optimize the data structure for memory efficiency.\n",
    "        \"\"\"\n",
    "        for paper in self.data:\n",
    "            for literal, positions in self.data[paper].items():\n",
    "                # self.data[paper][literal] = np.array(self.data[paper][literal], dtype=int)\n",
    "                self.data[paper][literal] = SortedSet(positions)\n",
    "\n",
    "    @time_function\n",
    "    def find(self, config:Config, papers, paper_full_text, instances, paper_instance_occurrence_matrix, optimize = True):\n",
    "        \"\"\"\n",
    "        Find all occurrences of instances in the full text of papers.\n",
    "\n",
    "        Parameters:\n",
    "        - config (Config): Configuration object with settings.\n",
    "        - papers (list): A list of paper identifiers.\n",
    "        - paper_full_text (dict): A mapping from paper identifiers to their full text file paths.\n",
    "        - instances (list): A list of instances to find in the papers.\n",
    "        - paper_instance_occurrence_matrix (list of lists): A matrix indicating whether an instance is in a paper.\n",
    "        \"\"\"\n",
    "        # find all occurrences of instances in text files\n",
    "        for paperID, paper in enumerate(papers):\n",
    "            if paperID % 100 == 0:\n",
    "                # print(f\"Processing paper {paperID} of {len(papers)}\")\n",
    "                pass\n",
    "            if paper in paper_full_text:\n",
    "                # Full text of paper is available\n",
    "                with open(paper_full_text[paper], 'r', encoding=\"utf8\") as f:\n",
    "                    text = f.read().lower()\n",
    "                    for i, instance in enumerate(instances):\n",
    "                        # if this instance is not in this document, move on.\n",
    "                        if config.savetime_on_fulltext:\n",
    "                            if not paper_instance_occurrence_matrix[paperID][i]:\n",
    "                                # assume instance is not in this paper\n",
    "                                continue\n",
    "                            \n",
    "                        pieces = split_string(instance) \n",
    "                        for piece in pieces:\n",
    "                            piece = piece.lower()\n",
    "                            pos = text.find(piece)\n",
    "                            while pos != -1:\n",
    "                                self.add_occurrence(paper, piece, pos)\n",
    "                                pos = text.find(piece, pos + 1)\n",
    "                                # Idea: store the sentence in which the instance was found\n",
    "                if not self.data.get(paper):\n",
    "                    print(f\"Paper {paper} has no instances in full text.\")\n",
    "            else:\n",
    "                print(f\"Paper {paper} has no full text available.\")\n",
    "        if optimize:\n",
    "            self.optimize_data()\n",
    "   \n",
    "    def set_min_distance(self, paper, literals, distance):\n",
    "        \"\"\"\n",
    "        Set the minimum distance between occurrences of literals in a paper.\n",
    "\n",
    "        Parameters:\n",
    "        - paper (str): The identifier for the paper.\n",
    "        - literals (list): A list of literals for which the distance is calculated.\n",
    "        - distance (int): The calculated minimum distance.\n",
    "        \"\"\"\n",
    "        key = frozenset(literals)\n",
    "        if paper not in self.min_distances:\n",
    "            self.min_distances[paper] = {}\n",
    "        self.min_distances[paper][key] = distance\n",
    "\n",
    "    def get_min_distance(self, paper, literals, allow_call = True):\n",
    "        \"\"\"\n",
    "        Get the minimum distance between occurrences of literals in a paper, if previously calculated.\n",
    "\n",
    "        Parameters:\n",
    "        - paper (str): The identifier for the paper.\n",
    "        - literals (list): A list of literals to find the minimum distance between.\n",
    "        - allow_call (bool): Flag to allow recursive call to find_min_distance.\n",
    "\n",
    "        Returns:\n",
    "        - int or None: The minimum distance if found, otherwise None.\n",
    "        \"\"\"\n",
    "        key = frozenset(literals)\n",
    "        if paper in self.min_distances and key in self.min_distances[paper]:\n",
    "            return self.min_distances[paper][key]\n",
    "        if allow_call:\n",
    "            return self.find_min_distance(paper, literals, allow_call = False)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def find_min_distance(self, paper, literals, allow_call = True):\n",
    "        if allow_call:\n",
    "            min_distance = self.get_min_distance(paper, literals, allow_call = False)\n",
    "            if min_distance:\n",
    "                return min_distance\n",
    "        literals = sorted(literals, key=len, reverse=True)\n",
    "        added = []\n",
    "        for literal in literals:\n",
    "            if literal in added:\n",
    "                continue\n",
    "            added.append(literal)\n",
    "            if literal not in self.data[paper]:\n",
    "                self.set_min_distance(paper, literals, -1)\n",
    "                return -1  # Literal not found in paper\n",
    "        lit_len = [len(literal) for literal in added]\n",
    "            \n",
    "        inputs = [[(x, i) for x in self.data[paper][literal]] for i, literal in enumerate(added)]\n",
    "\n",
    "        indices = [lst[0][0] for lst in inputs]\n",
    "        best = float('inf')\n",
    "\n",
    "        for item in sorted(sum(inputs, [])):\n",
    "            indices[item[1]] = item[0]\n",
    "            arr_min = min(indices)\n",
    "            best = min(max(indices) - arr_min - lit_len[indices.index(arr_min)], best)\n",
    "        self.set_min_distance(paper, literals, best)\n",
    "        return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_in_paper = PosInPaper()\n",
    "\n",
    "if not config.recalculate_pos_in_paper:\n",
    "    try:\n",
    "        pos_in_paper.load_from_file(os.path.join(config.get_output_path(), 'pos_in_paper'))\n",
    "    # print exception\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Starting from scratch.\")\n",
    "        config.recalculate_pos_in_paper = True\n",
    "if config.recalculate_pos_in_paper:\n",
    "    pos_in_paper.find(config, papers, paper_full_text, instances, paper_instance_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: find the gap between the pieces of an instance\n",
    "import sys\n",
    "\n",
    "def find_min_distance_keep(lists, literals = [], keep_close_matches = False):\n",
    "\n",
    "    #TODO: currently, this does not consider stemmed words\n",
    "    # implemented: consider the length of the words for measuring the distance.\n",
    "    ## e.g. \"four\" will always have a distance of at least 5 (4 characters + 1 space) to any consecutive word\n",
    "    ## We must subtract the length of the words from the distance later.\n",
    "    positions = []\n",
    "\n",
    "    # Initialize pointers for each of the lists\n",
    "    pointers = [0] * len(lists)\n",
    "    min_distance = sys.maxsize\n",
    "    \n",
    "    pointer_map = [i for i in range(len(lists))]\n",
    "    has_lists = []\n",
    "    for i, item in enumerate(lists):\n",
    "        if not item:\n",
    "            # There are cases where e.g. \"system integration\" is not found in full text\n",
    "            # This happens when NLP converts e.g. \"integrated\" to \"integration\"\n",
    "            # example:\n",
    "            # \"Liu und Hu - 2013 - A reuse oriented representation model for capturin\"\n",
    "            # \"system integration\" -> \"integration\" is not found in the full text\n",
    "            return -1\n",
    "        \n",
    "        if isinstance(item[0], list):\n",
    "            has_lists.append(i)\n",
    "            pointer_map = pointer_map[:i] + [pointer_map[i]] * len(item[0]) + pointer_map[i+1:]\n",
    "            break\n",
    "\n",
    "    # preprocess list to see if duplicates are present\n",
    "    deletions = []\n",
    "    checked = []\n",
    "    for lit in literals:\n",
    "        if lit in checked:\n",
    "            continue\n",
    "        checked.append(lit)\n",
    "        if literals.count(lit) > 1:\n",
    "            lit_index = index_all(literals, lit)\n",
    "            respective_lists_index = [pointer_map[index] for index in lit_index]\n",
    "            counts = [pointer_map.count(index) for index in respective_lists_index]\n",
    "            if min(counts) == 1:\n",
    "                # get index of min count\n",
    "                min_index = counts.index(1)\n",
    "                deletions.append(respective_lists_index[min_index])\n",
    "            else:\n",
    "                # be careful, all duplicate literals are part of merged instances\n",
    "                pass\n",
    "    for i in deletions:\n",
    "        # safe deletion. list is not used anymore\n",
    "        literals.pop(i)\n",
    "        lists.pop(i)\n",
    "        mapped_list = pointer_map.pop(i)\n",
    "        pointers.pop(i)\n",
    "        if mapped_list not in pointer_map:\n",
    "            # the list is gone, we need to adjust the pointer map\n",
    "            for j in range(i, len(pointer_map)):\n",
    "                pointer_map[j] -= 1\n",
    "        else:\n",
    "            # handle the specific use case that a part of a nested list needs to be removed.\n",
    "            # since we do not do that yet, we just pass\n",
    "            # When this is ever implemented, insert the code here\n",
    "            pass\n",
    "        \n",
    "    # flattened_lists = []\n",
    "    # for lst in lists:\n",
    "    #     flattened_list = []\n",
    "    #     for item in lst:\n",
    "    #         if isinstance(item, list):\n",
    "    #             flattened_list.extend(item)\n",
    "    #         else:\n",
    "    #             flattened_list.append(item)\n",
    "    #     flattened_lists.append(flattened_list)\n",
    "\n",
    "    while True:\n",
    "        # Get the current elements from the lists\n",
    "        current_elements = [lists[i][pointers[i]] for i in range(len(lists))]\n",
    "        # if one of the elements is a list, add both elements to the current elements\n",
    "        \n",
    "        for i in has_lists:\n",
    "            # insert the current elements into the list, but keep the order\n",
    "            current_elements = current_elements[:i] + current_elements[i] + current_elements[i+1:]\n",
    "        \n",
    "        # Calculate the current distance\n",
    "        current_min = min(current_elements)\n",
    "        current_max = max(current_elements)\n",
    "        current_distance = current_max - current_min\n",
    "        \n",
    "        # reduce the distance by the length of the first word\n",
    "        if literals:\n",
    "            min_element_index = current_elements.index(current_min)\n",
    "            current_distance -= len(literals[min_element_index]) + 1 # +1 for the space\n",
    "        \n",
    "        if keep_close_matches:\n",
    "            # Keep track of matches that are close to each other:\n",
    "            if current_distance < config.gap_too_large_threshold:\n",
    "                positions.append(current_elements)\n",
    "\n",
    "        # Update the minimum distance\n",
    "        if current_distance < min_distance:\n",
    "            min_distance = current_distance\n",
    "            if min_distance <= 0:\n",
    "                min_distance = 0\n",
    "                if not keep_close_matches:\n",
    "                    return min_distance\n",
    "            \n",
    "        # Check if we can move forward in the list containing the minimum element\n",
    "        min_index = pointer_map[current_elements.index(current_min)]\n",
    "\n",
    "        # If the pointer exceeds its list length, exit the loop\n",
    "        for i in range(len(lists)):\n",
    "            if pointers[i] < len(lists[i]) - 1:\n",
    "                break\n",
    "        if pointers[min_index] + 1 >= len(lists[min_index]):\n",
    "            break\n",
    "\n",
    "        # Otherwise, increment the pointer\n",
    "        pointers[min_index] += 1\n",
    "\n",
    "    if keep_close_matches:\n",
    "        return min_distance, positions\n",
    "    else:\n",
    "        return min_distance\n",
    "\n",
    "\n",
    "# # Test the function with the given lists\n",
    "# lists = [[1, 2, 3, 2, 1000], [50, 1001], [100, 1002, 10000]]\n",
    "# print(find_min_distance(lists))\n",
    "\n",
    "\n",
    "@time_function\n",
    "def find_instance_piece_gap(config:Config, papers, paper_full_text, instances, paper_instance_occurrence_matrix, pos_in_paper:PosInPaper):\n",
    "    error_matrix = np.zeros(paper_instance_occurrence_matrix.shape, dtype=float)\n",
    "    instance_piece_gap = {}\n",
    "    for paperID, paper in enumerate(papers):\n",
    "        if paperID % 100 == 0:\n",
    "            # print(f\"Processing paper {paperID} of {len(papers)}\")\n",
    "            continue\n",
    "        if paper in paper_full_text:\n",
    "            for i, instance in enumerate(instances):\n",
    "                # if this instance is not in this document, move on.\n",
    "                #TODO This does not work\n",
    "                if not paper_instance_occurrence_matrix[paperID][i]:\n",
    "                    continue\n",
    "\n",
    "                pieces = split_string(instance)\n",
    "\n",
    "                if len(pieces) > 1:\n",
    "                    # print(f\"Processing {instance} in {paper}\")\n",
    "                    # candidate_postions = []\n",
    "                    # for piece in pieces:\n",
    "                    #     if piece not in pos_in_paper[paper]:\n",
    "                    #         # print(f\"{piece} not found in {paper}\")\n",
    "                    #         candidate_postions = []\n",
    "                    #         break\n",
    "                    #     key = frozenset(piece)\n",
    "                    #     candidate_postions.append(pos_in_paper[paper][key])\n",
    "                    # if candidate_postions:\n",
    "                    min_distance = pos_in_paper.get_min_distance(paper, pieces)\n",
    "                    # if config.try_to_save_time:\n",
    "                    #     min_distance_keep, positions = find_min_distance_keep(candidate_postions, pieces, keep_close_matches = True)\n",
    "                    #     if min_distance != min_distance_keep:\n",
    "                    #         raise Exception(f\"Error: {min_distance} != {min_distance_keep}\")\n",
    "                    #     pos_in_paper[paper][instance] = positions\n",
    "                    # else:\n",
    "                    #     min_distance = -1\n",
    "\n",
    "                    # min_distance_nested = find_min_distance_nested(candidate_postions)\n",
    "                    # print(f\"{instance}: {min_distance} vs {min_distance_nested}\")\n",
    "                    # if min_distance != min_distance_nested:\n",
    "                    #     print(f\"Error: {min_distance} != {min_distance_nested}\")\n",
    "\n",
    "                    # Pieces to far apart are not counted\n",
    "                    if min_distance is None:\n",
    "                        pass\n",
    "                    if min_distance > config.gap_too_large_threshold:\n",
    "                        # print(f\"Gap for {instance} in {paper} ({min_distance} > {GAP_TOO_LARGE_THRESHOLD})\")\n",
    "                        paper_instance_occurrence_matrix[paperID][i] = 0\n",
    "                        # get log base 10 of min distance\n",
    "                        error_matrix[paperID][i] = round(np.log10(min_distance), 1)\n",
    "                    \n",
    "                    # Some pieces may not be found in the full text\n",
    "                    if min_distance == -1:\n",
    "                        # print(f\"{instance} not found in {paper} at all\")\n",
    "                        paper_instance_occurrence_matrix[paperID][i] = 0\n",
    "                        error_matrix[paperID][i] = min_distance\n",
    "                        # for these, we do not store the gap                    \n",
    "                        continue\n",
    "\n",
    "                    if instance not in instance_piece_gap:\n",
    "                        instance_piece_gap[instance] = {}\n",
    "                    instance_piece_gap[instance][paper] = min_distance\n",
    "    return instance_piece_gap, error_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find_instance_piece_gap executed in 0.058472394943237305 seconds\n"
     ]
    }
   ],
   "source": [
    "instance_piece_gap, error_matrix = find_instance_piece_gap(config, papers, paper_full_text, instances, paper_instance_occurrence_matrix, pos_in_paper)\n",
    "\n",
    "error_matrix, has_error = remove_zeros(error_matrix)\n",
    "error_papers = handle_deletions(papers, has_error)\n",
    "error_instances = handle_deletions(instances, has_error, rows = False)\n",
    "\n",
    "paper_instance_occurrence_matrix, instances, deletions = update_instances(paper_instance_occurrence_matrix, instances, instance_types_dicts)\n",
    "\n",
    "papers = handle_deletions(papers, deletions)\n",
    "# pos_in_paper = handle_deletions(pos_in_paper, deletions)\n",
    "\n",
    "instance_instance_co_occurrence_matrix = np.dot(paper_instance_occurrence_matrix.T, paper_instance_occurrence_matrix)\n",
    "\n",
    "# free unneeded memory\n",
    "del deletions, has_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year_paper_occurrence_matrix\n",
    "def create_year_paper_occurrence_matrix(papers_metadata, paper_instance_occurrence_matrix, papers, is_error_matrix=False):\n",
    "    year_papers = {}\n",
    "\n",
    "    for paper in papers_metadata:\n",
    "        if 'year' in papers_metadata[paper]:\n",
    "            year = int(papers_metadata[paper]['year'])\n",
    "            if year not in year_papers:\n",
    "                year_papers[year] = []\n",
    "            year_papers[year].append(paper)\n",
    "\n",
    "\n",
    "    earliest = min(year_papers)\n",
    "    latest = max(year_papers)\n",
    "    span = latest-earliest+1\n",
    "\n",
    "    for year in range(earliest, latest):\n",
    "        if year not in year_papers:\n",
    "            year_papers[year] = []\n",
    "\n",
    "    year_papers = {k: v for k, v in sorted(year_papers.items(), key=lambda item: item[0])}\n",
    "\n",
    "    if is_error_matrix:\n",
    "        # convert any value != 0 to 1\n",
    "        paper_instance_occurrence_matrix = np.where(paper_instance_occurrence_matrix != 0, 1, 0)\n",
    "\n",
    "    # create a year_instance_occurence matrix from the paper_instance_occurrence_matrix\n",
    "    year_instance_occurrence_matrix = np.zeros((span, paper_instance_occurrence_matrix.shape[1]), dtype=int)\n",
    "    for yearID, year in enumerate(year_papers):\n",
    "        for paper in year_papers[year]:\n",
    "            if paper in papers:\n",
    "                paperID = papers.index(paper)\n",
    "                year_instance_occurrence_matrix[yearID] += paper_instance_occurrence_matrix[paperID]\n",
    "    \n",
    "    return year_instance_occurrence_matrix, year_papers\n",
    "\n",
    "year_instance_occurrence_matrix, year_papers = create_year_paper_occurrence_matrix(papers_metadata, paper_instance_occurrence_matrix, papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Complete\n",
    "\n",
    "We now have:\n",
    "\n",
    "| Variable                          | Type    | Size         | Comments |\n",
    "|-----------------------------------|---------|--------------|----------|\n",
    "| error_instances                   | list    | 165          | Comments |\n",
    "| error_matrix                      | ndarray | (999, 165)   | Comments |\n",
    "| error_papers                      | list    | 999          | Comments |\n",
    "| gap_too_large_threshold           | int     | n.a.         | Comments |\n",
    "| instance_piece_gap                | dict    | 151          | Comments |\n",
    "| instance_types_dicts              | dict    | 5            | Comments |\n",
    "| instances                         | list    | 315          | Comments |\n",
    "| paper_full_text                   | dict    | 1029         | Comments |\n",
    "| paper_instance_occurrence_matrix  | ndarray | (1003, 315)  | Comments |\n",
    "| papers                            | list    | 1003         | Comments |\n",
    "| pos_in_paper                      | dict    | 1003         | Comments |\n",
    "\n",
    "Consisting of:\n",
    "* The paper_instance_occurrence_matrix, binary listing if a term (instance) is present in a paper\n",
    "  * papers x instances\n",
    "* The error_matrix, of all instances that were dropped from the paper_instance_occurrence_matrix\n",
    "  * error_papers x error_instances\n",
    "\n",
    "And some leftover variables:\n",
    "* instance_types_dicts, listing all instance types (\"process\", \"software\", ...) and their respective instance sets (\"Curation\", \"Knowledge Work\", ...)\n",
    "* paper_full_text, containing each papers full text\n",
    "  * pos_in_paper, listing for each paper: for each instance: each position of that instance in that papers full text.\n",
    "* instance_piece_gap, a dict listing all instances made up from compound words (e.g. \"Knowledge Work\", and their minimum distance in each papers full text)\n",
    "  * gap_too_large_threshold, defining how far appart a finding of \"Knowledge\" and \"Work\" would qualify as \"Knowledge Work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~3 min | {( len(papers) * len(instances) ) / (3 * 1000) }seconds  compare proximity of all instances with one antoher\n",
    "# ~8 min right now.\n",
    "# 3 min 30 sec with 164 papers and 339 instances\n",
    "@time_function\n",
    "def calculate_proximity_matrix(config:Config, pos_in_paper:PosInPaper, instances, mode = \"sqrt\", try_to_save_time = False):\n",
    "    # TODO: Optimize this function.\n",
    "    # each instance needs to have it's occurences as pieces clustered together, so that only those below max distance are considered\n",
    "\n",
    "    # create a np zeros matrix of size instances x instances\n",
    "    instance_instance_proximity_matrix = np.zeros((len(instances), len(instances)), dtype=float)\n",
    "    \n",
    "    # alternatives are:\n",
    "    # \"sqrt\" - 1 / (square root of the distance)\n",
    "    # \"linear\" - 1 / distance\n",
    "    # \"binary\" - 1 if distance < MAX_GAP_THRESHOLD, 0 otherwise\n",
    "    # \"log\" - 1 / log(distance) \n",
    "\n",
    "    for paperID, paper in enumerate(pos_in_paper.data):\n",
    "        # print(f\"Processing {paper}\")\n",
    "        # if paperID % 50 == 0:\n",
    "        print(f\"Processing paper {paperID} of {len(pos_in_paper.data)}\")\n",
    "        for id1, instance1 in enumerate(instances):\n",
    "            # print (f\"Processing {id1} of {len(instances)}: {instance1}\")\n",
    "            for id2, instance2 in enumerate(instances):\n",
    "                # FIXME: this resulted in a matrix which was not symmetric.\n",
    "                # That hints at a problem with the calclulation, [id1][id2] and [id2][id1] should be the same\n",
    "                if id1 < id2:\n",
    "                    continue\n",
    "                if instance1 != instance2:\n",
    "                    # valid_combination = True\n",
    "                    literals = []\n",
    "                    for instance in [instance1, instance2]:\n",
    "                        pieces = split_string(instance)\n",
    "                        literals += pieces\n",
    "                    #     if try_to_save_time and instance in pos_in_paper[paper]:\n",
    "                    #         instance_positions = pos_in_paper[paper][instance]\n",
    "                    #         positions.append(instance_positions)\n",
    "                    #     else:\n",
    "                    #         for piece in pieces:\n",
    "                    #             if piece not in literals:\n",
    "                    #                 literals.append(piece)\n",
    "                    #             else:\n",
    "                    #                 continue\n",
    "                    #         positions = get_pos_in_paper(paper, piece, pos_in_paper)\n",
    "\n",
    "                    \n",
    "                    # if valid_combination:\n",
    "                    #     if try_to_save_time:\n",
    "                    #         distance, combination_positions = find_min_distance_keep(positions, literals, keep_close_matches = True)\n",
    "                    #         pos_in_paper[paper][' '.join(literals)] = combination_positions\n",
    "                    #     else:\n",
    "\n",
    "                    distance = pos_in_paper.get_min_distance(paper, literals)\n",
    "                    if distance < 0:\n",
    "                        # print(f\"Error: {instance1} and {instance2} not found in {paper}\")\n",
    "                        continue\n",
    "                    result = 0.0\n",
    "                    if distance == 0:\n",
    "                        result = 1\n",
    "                    elif distance == 1:\n",
    "                        result = 1\n",
    "                    elif mode == \"sqrt\":\n",
    "                        result = 1 / np.sqrt(distance)\n",
    "                    elif mode == \"linear\":\n",
    "                        result = 1 / distance\n",
    "                    elif mode == \"binary\":\n",
    "                        result = 1 if distance < config.gap_too_large_threshold else 0\n",
    "                    elif mode == \"log\":\n",
    "                        result = 1 / np.log(distance)\n",
    "                    else:\n",
    "                        print(\"Error: unknown mode\")\n",
    "                        break\n",
    "                    if result > 0.0:\n",
    "                        instance_instance_proximity_matrix[id1][id2] += result\n",
    "                        instance_instance_proximity_matrix[id2][id1] += result\n",
    "\n",
    "    #TODO rest doesnt seem to work, short fix implemented:\n",
    "    # create a copy of labels that only contains instances that are in the proximity matrix\n",
    "\n",
    "    instance_instance_proximity_matrix, deletions = remove_zeros(instance_instance_proximity_matrix)\n",
    "    proximity_instances = handle_deletions(instances, deletions, rows=False)\n",
    "    \n",
    "    return instance_instance_proximity_matrix, proximity_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing paper 0 of 164\n",
      "Processing paper 1 of 164\n",
      "Processing paper 2 of 164\n",
      "Processing paper 3 of 164\n",
      "Processing paper 4 of 164\n",
      "Processing paper 5 of 164\n",
      "Processing paper 6 of 164\n",
      "Processing paper 7 of 164\n",
      "Processing paper 8 of 164\n",
      "Processing paper 9 of 164\n",
      "Processing paper 10 of 164\n",
      "Processing paper 11 of 164\n",
      "Processing paper 12 of 164\n",
      "Processing paper 13 of 164\n",
      "Processing paper 14 of 164\n",
      "Processing paper 15 of 164\n",
      "Processing paper 16 of 164\n",
      "Processing paper 17 of 164\n",
      "Processing paper 18 of 164\n",
      "Processing paper 19 of 164\n",
      "Processing paper 20 of 164\n",
      "Processing paper 21 of 164\n",
      "Processing paper 22 of 164\n",
      "Processing paper 23 of 164\n",
      "Processing paper 24 of 164\n",
      "Processing paper 25 of 164\n",
      "Processing paper 26 of 164\n",
      "Processing paper 27 of 164\n",
      "Processing paper 28 of 164\n",
      "Processing paper 29 of 164\n",
      "Processing paper 30 of 164\n",
      "Processing paper 31 of 164\n",
      "Processing paper 32 of 164\n",
      "Processing paper 33 of 164\n",
      "Processing paper 34 of 164\n",
      "Processing paper 35 of 164\n",
      "Processing paper 36 of 164\n",
      "Processing paper 37 of 164\n",
      "Processing paper 38 of 164\n",
      "Processing paper 39 of 164\n",
      "Processing paper 40 of 164\n",
      "Processing paper 41 of 164\n",
      "Processing paper 42 of 164\n",
      "Processing paper 43 of 164\n",
      "Processing paper 44 of 164\n",
      "Processing paper 45 of 164\n",
      "Processing paper 46 of 164\n",
      "Processing paper 47 of 164\n",
      "Processing paper 48 of 164\n",
      "Processing paper 49 of 164\n",
      "Processing paper 50 of 164\n",
      "Processing paper 51 of 164\n",
      "Processing paper 52 of 164\n",
      "Processing paper 53 of 164\n",
      "Processing paper 54 of 164\n",
      "Processing paper 55 of 164\n",
      "Processing paper 56 of 164\n",
      "Processing paper 57 of 164\n",
      "Processing paper 58 of 164\n",
      "Processing paper 59 of 164\n",
      "Processing paper 60 of 164\n",
      "Processing paper 61 of 164\n",
      "Processing paper 62 of 164\n",
      "Processing paper 63 of 164\n",
      "Processing paper 64 of 164\n",
      "Processing paper 65 of 164\n",
      "Processing paper 66 of 164\n",
      "Processing paper 67 of 164\n",
      "Processing paper 68 of 164\n",
      "Processing paper 69 of 164\n",
      "Processing paper 70 of 164\n",
      "Processing paper 71 of 164\n",
      "Processing paper 72 of 164\n",
      "Processing paper 73 of 164\n",
      "Processing paper 74 of 164\n",
      "Processing paper 75 of 164\n",
      "Processing paper 76 of 164\n",
      "Processing paper 77 of 164\n",
      "Processing paper 78 of 164\n",
      "Processing paper 79 of 164\n",
      "Processing paper 80 of 164\n",
      "Processing paper 81 of 164\n",
      "Processing paper 82 of 164\n",
      "Processing paper 83 of 164\n",
      "Processing paper 84 of 164\n",
      "Processing paper 85 of 164\n",
      "Processing paper 86 of 164\n",
      "Processing paper 87 of 164\n",
      "Processing paper 88 of 164\n",
      "Processing paper 89 of 164\n",
      "Processing paper 90 of 164\n",
      "Processing paper 91 of 164\n",
      "Processing paper 92 of 164\n",
      "Processing paper 93 of 164\n",
      "Processing paper 94 of 164\n",
      "Processing paper 95 of 164\n",
      "Processing paper 96 of 164\n",
      "Processing paper 97 of 164\n",
      "Processing paper 98 of 164\n",
      "Processing paper 99 of 164\n",
      "Processing paper 100 of 164\n",
      "Processing paper 101 of 164\n",
      "Processing paper 102 of 164\n",
      "Processing paper 103 of 164\n",
      "Processing paper 104 of 164\n",
      "Processing paper 105 of 164\n",
      "Processing paper 106 of 164\n",
      "Processing paper 107 of 164\n",
      "Processing paper 108 of 164\n",
      "Processing paper 109 of 164\n",
      "Processing paper 110 of 164\n",
      "Processing paper 111 of 164\n",
      "Processing paper 112 of 164\n",
      "Processing paper 113 of 164\n",
      "Processing paper 114 of 164\n",
      "Processing paper 115 of 164\n",
      "Processing paper 116 of 164\n",
      "Processing paper 117 of 164\n",
      "Processing paper 118 of 164\n",
      "Processing paper 119 of 164\n",
      "Processing paper 120 of 164\n",
      "Processing paper 121 of 164\n",
      "Processing paper 122 of 164\n",
      "Processing paper 123 of 164\n",
      "Processing paper 124 of 164\n",
      "Processing paper 125 of 164\n",
      "Processing paper 126 of 164\n",
      "Processing paper 127 of 164\n",
      "Processing paper 128 of 164\n",
      "Processing paper 129 of 164\n",
      "Processing paper 130 of 164\n",
      "Processing paper 131 of 164\n",
      "Processing paper 132 of 164\n",
      "Processing paper 133 of 164\n",
      "Processing paper 134 of 164\n",
      "Processing paper 135 of 164\n",
      "Processing paper 136 of 164\n",
      "Processing paper 137 of 164\n",
      "Processing paper 138 of 164\n",
      "Processing paper 139 of 164\n",
      "Processing paper 140 of 164\n",
      "Processing paper 141 of 164\n",
      "Processing paper 142 of 164\n",
      "Processing paper 143 of 164\n",
      "Processing paper 144 of 164\n",
      "Processing paper 145 of 164\n",
      "Processing paper 146 of 164\n",
      "Processing paper 147 of 164\n",
      "Processing paper 148 of 164\n",
      "Processing paper 149 of 164\n",
      "Processing paper 150 of 164\n",
      "Processing paper 151 of 164\n",
      "Processing paper 152 of 164\n",
      "Processing paper 153 of 164\n",
      "Processing paper 154 of 164\n",
      "Processing paper 155 of 164\n",
      "Processing paper 156 of 164\n",
      "Processing paper 157 of 164\n",
      "Processing paper 158 of 164\n",
      "Processing paper 159 of 164\n",
      "Processing paper 160 of 164\n",
      "Processing paper 161 of 164\n",
      "Processing paper 162 of 164\n",
      "Processing paper 163 of 164\n",
      "calculate_proximity_matrix executed in 23.936680555343628 seconds\n"
     ]
    }
   ],
   "source": [
    "instance_instance_proximity_matrix, proximity_instances = calculate_proximity_matrix(config, pos_in_paper, instances)\n",
    "# Info: This method is extremely slow. requires more testing, which is currently done in a side project:\n",
    "## scripts\\SLR\\MVP\\test_case.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while we do not have a fast method to calculate this, we load the data from the file\n",
    "if config.recalculate_pos_in_paper:\n",
    "    pos_in_paper.save_to_file(os.path.join(config.get_output_path(), 'pos_in_paper'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare csv file again\n",
    "# process,software,data item,data model,data format specification,interchange format,data visualization,data validation,inference,source\n",
    "\n",
    "@time_function\n",
    "def knowledge_graph_population(config:Config, instance_types_dicts, property_types_dicts, instance_instance_proximity_matrix, proximity_instances):\n",
    "    columns = list(instance_types_dicts.keys()) \n",
    "    # columns += list(property_types_dicts.keys())\n",
    "    # columns = ['process', 'software', 'data item', 'data model', 'data format specification', 'data visualization', 'data validation', 'inference']\n",
    "\n",
    "\n",
    "\n",
    "    rows = []\n",
    "    for c_ID, column in enumerate(columns):\n",
    "        for instance in instance_types_dicts[column]:\n",
    "            # add the instance to the csv with each of their relations\n",
    "            if instance not in proximity_instances:\n",
    "                continue\n",
    "            instance_index = proximity_instances.index(instance)\n",
    "            for oc_ID, other_column in enumerate(columns):\n",
    "                if other_column not in instance_types_dicts:\n",
    "                    if other_column in property_types_dicts:\n",
    "                        #TODO: handle properties specially\n",
    "                        continue\n",
    "                    continue\n",
    "                if other_column != column:\n",
    "                    other_column_instances = instance_types_dicts[other_column]\n",
    "                    for other_instance in other_column_instances:\n",
    "                        if other_instance not in proximity_instances:\n",
    "                            continue\n",
    "                        other_instance_index = proximity_instances.index(other_instance)\n",
    "                        if instance_instance_proximity_matrix[instance_index][other_instance_index] > 0:\n",
    "                            # build row column by column\n",
    "                            row = [''] * len(columns)\n",
    "                            row[c_ID] = instance\n",
    "                            row[oc_ID] = other_instance\n",
    "                            rows.append(row)\n",
    "\n",
    "    # write to csv\n",
    "    filepath = os.path.join(config.get_output_path(),'instance_relations.csv')\n",
    "    with open(filepath, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(config.csv_separator.join(columns) + '\\n')\n",
    "        for row in rows:\n",
    "            f.write(config.csv_separator.join(row) + '\\n')\n",
    "    \n",
    "knowledge_graph_population(config, instance_types_dicts, property_types_dicts, instance_instance_proximity_matrix, proximity_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "def get_rules(matrix, columns):\n",
    "    # AttributeError: 'numpy.ndarray' object has no attribute 'dtypes'\n",
    "    dataframe = pd.DataFrame(matrix, columns=columns).astype(bool)\n",
    "\n",
    "    # for each process:\n",
    "    # create one res\n",
    "\n",
    "    res = apriori(dataframe, min_support=0.4, use_colnames=True, max_len=2)\n",
    "\n",
    "    # visualize res\n",
    "    res = res.sort_values(by='support', ascending=False)\n",
    "    res = res.reset_index(drop=True)\n",
    "    # res\n",
    "\n",
    "    rules = association_rules(res)\n",
    "    # sort rules by confidence\n",
    "    # rules = rules.sort_values(by='confidence', ascending=False)\n",
    "    rules = rules.sort_values(by='lift', ascending=False) # (propably most important)\n",
    "    # rules = rules.sort_values(by='leverage', ascending=False)\n",
    "    # export rules to csv\n",
    "    return rules\n",
    "\n",
    "rules = get_rules(paper_instance_occurrence_matrix, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_cross_type_rules(rules):\n",
    "    cross_type = [False] * len(rules)\n",
    "\n",
    "    for i, antecentent in enumerate(rules.antecedents):\n",
    "        antecentent, = antecentent\n",
    "        consequent, = rules.iloc[i].consequents\n",
    "        type1, type2 = None, None\n",
    "        for type in instance_types_dicts:\n",
    "            if antecentent in instance_types_dicts[type]:\n",
    "                type1 = type\n",
    "            if consequent in instance_types_dicts[type]:\n",
    "                type2 = type\n",
    "            if type1 and type2:\n",
    "                break\n",
    "        if type1 != type2:\n",
    "            cross_type[i] = True\n",
    "            # print(rules.iloc[i])\n",
    "\n",
    "    # create a copy for all rules that are cross type\n",
    "    rules_cross_type = rules[cross_type].copy()\n",
    "    return rules_cross_type\n",
    "\n",
    "rules_cross_type = identify_cross_type_rules(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent a dict\n",
    "import csv\n",
    "import os\n",
    "from itables import init_notebook_mode, show\n",
    "\n",
    "# better represent dataframes\n",
    "if not config.for_git:\n",
    "    init_notebook_mode(all_interactive=True)\n",
    "\n",
    "\n",
    "def prep_dict(input_dict):\n",
    "    changes_needed = {}\n",
    "    # get ONLY THE FIRST key and value   \n",
    "    key, value = next(iter(input_dict.items()))     \n",
    "    key_change = \"\"\n",
    "    value_change = \"\"\n",
    "    if isinstance(key, frozenset):\n",
    "        key_change = \"str\"\n",
    "    if isinstance(value, set):\n",
    "        value_change = \"list\"\n",
    "    if isinstance(value, dict):\n",
    "        value_change = prep_dict(value)\n",
    "    changes_needed[key_change] = value_change\n",
    "    return changes_needed\n",
    "    \n",
    "def change_dict(input_dict, changes_needed):\n",
    "    for key, value in changes_needed.items():\n",
    "        if key == \"str\" and value == \"list\":\n",
    "            input_dict = {str(key): list(value) for key, value in input_dict.items()}\n",
    "        elif key == \"str\":\n",
    "            input_dict = {str(key): value for key, value in input_dict.items()}\n",
    "        elif value == \"list\":\n",
    "            input_dict = {key: list(value) for key, value in input_dict.items()}\n",
    "        elif key == \"str\" and value == \"dict\":\n",
    "            input_dict = {str(key): change_dict(value, changes_needed[\"str\"]) for key, value in input_dict.items()}\n",
    "        elif value == \"dict\":\n",
    "            input_dict = {key: change_dict(value, changes_needed[\"\"]) for key, value in input_dict.items()}\n",
    "    return input_dict\n",
    "\n",
    "def process_list(config:Config, input_list:list, filename=\"some_list\", path=None):\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    filepath = os.path.join(path, filename)\n",
    "    with open(filepath + '.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for item in input_list:\n",
    "            f.write(f\"{item}\\n\")\n",
    "\n",
    "def process_dict(config:Config, input_dict:dict, filename=\"some_dict\", path=None):\n",
    "    # convert all sets to lists\n",
    "    changes_needed = prep_dict(input_dict)\n",
    "    processed_dict = change_dict(input_dict, changes_needed)\n",
    "\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    filepath = os.path.join(path, filename)\n",
    "    with open(filepath + '.json', 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(processed_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # TODO: Only do this for dicts that need statistical analysis\n",
    "    requires_analysis = False\n",
    "    value = list(processed_dict.values())[0]  # Convert dict_values to list to make it subscriptable\n",
    "    if isinstance(value, dict):\n",
    "        value = list(value.values())[0]  # Convert dict_values to list to make it subscriptable\n",
    "        if isinstance(value, int) or isinstance(value, float):\n",
    "            requires_analysis = True\n",
    "    if not requires_analysis:\n",
    "        return\n",
    "    \n",
    "    container = [\n",
    "        [\"Instance\", \"Min\", \"Max\", \"Mean\", \"Median\", \"Std\"]\n",
    "    ]\n",
    "\n",
    "    for instance, papers in input_dict.items():\n",
    "\n",
    "        # print(f\"Instance: {instance}\")\n",
    "        gaps = papers.values()\n",
    "        # generate all kinds of statistical values\n",
    "        min_gap = min(gaps)\n",
    "        max_gap = max(gaps)\n",
    "        mean_gap = sum(gaps) / len(gaps)\n",
    "        median_gap = np.median(list(gaps))\n",
    "        std_gap = np.std(list(gaps))\n",
    "        container.append([instance, min_gap, max_gap, mean_gap, median_gap, std_gap])\n",
    "\n",
    "    filepath = os.path.join(path, filename)\n",
    "\n",
    "    # TODO: Handle CSV separator\n",
    "    # if not for_git:\n",
    "    # Function to convert a single value\n",
    "    # def convert_decimal_delimiter(value, decimal=CSV_DECIMAL):\n",
    "    #     if isinstance(value, float):\n",
    "    #         return f\"{value}\".replace('.', decimal)\n",
    "    #     return value\n",
    "\n",
    "    # # Convert all floats in your container to strings with the desired decimal delimiter\n",
    "    # container = [[convert_decimal_delimiter(value) for value in row] for row in container]\n",
    "\n",
    "    # write to csv\n",
    "    with open(filepath + \".csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=config.csv_separator)\n",
    "        writer.writerows(container)   \n",
    "\n",
    "def process_dataframe(config:Config, input_df, name = \"some_df\", path=None):\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    filepath = os.path.join(path, name)\n",
    "    \n",
    "    input_df.to_csv(filepath + '.csv', sep=config.csv_separator, decimal=config.csv_decimal)\n",
    "    show(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize co-occurrences\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import math\n",
    "\n",
    "def visualize_matrix(matrix: np.ndarray, rows: list[str], columns: list[str] = None, name: str = 'some_matrix', format = '.png', path = None) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes a matrix as a heatmap.\n",
    "    matrix: The matrix to visualize\n",
    "    rows: The labels for the rows\n",
    "    columns: The labels for the columns\n",
    "    name: The name of the file to save\n",
    "    format: The format of the file to save (default: '.png', also accepts '.svg' and '.pdf', also accepts a list of formats)\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = rows\n",
    "\n",
    "    if path is None:\n",
    "        path = get_output_path(visualization=True)\n",
    "\n",
    "    ## Calculate the maximum size of the plot\n",
    "    dpi = 300\n",
    "    max_dpi = 600\n",
    "    if for_git:\n",
    "        dpi = 96\n",
    "        max_dpi = 200\n",
    "    max_pixel = 2**16  # Maximum size in any direction\n",
    "    max_size = max_pixel / dpi  # Maximum size in any direction\n",
    "    max_size_total = max_size * max_size # Maximum size in total\n",
    "    max_size_total *= 0.05 # produce smaller files\n",
    "\n",
    "    # Experience value of space required per cell\n",
    "    factor = 0.18\n",
    "    size_x: float = 2 + len(columns) * factor\n",
    "    size_y: float = 3 + len(rows) * 0.8 * factor\n",
    "\n",
    "    while size_x * size_y < max_size_total and dpi < max_dpi:\n",
    "        dpi /= 0.95 \n",
    "        max_size_total *= 0.95\n",
    "\n",
    "    if dpi > max_dpi:\n",
    "        dpi = max_dpi\n",
    "\n",
    "    while size_x * size_y > max_size_total:\n",
    "        dpi *= 0.95 \n",
    "        max_size_total /= 0.95\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(size_x, size_y), dpi=dpi)\n",
    "\n",
    "    cax = ax.matshow(matrix, cmap='viridis')\n",
    "\n",
    "    # use labels from instance_occurrences\n",
    "    ax.set_xticks(range(len(columns)))\n",
    "    ax.set_xticklabels(list(columns), fontsize=10, rotation=90)\n",
    "    ax.set_yticks(range(len(rows)))\n",
    "    ax.set_yticklabels(list(rows), fontsize=10)\n",
    "\n",
    "    # # adjust the spacing between the labels\n",
    "    # plt.gca().tick_params(axis='x', which='major', pad=15)\n",
    "    # plt.gca().tick_params(axis='y', which='major', pad=15)\n",
    "\n",
    "    # show the number of co-occurrences in each cell, if greater than 0\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[i])):\n",
    "            if matrix[i, j] == 0:\n",
    "                continue\n",
    "            # if co_occurrences[i, j] > 100:\n",
    "            #     continue\n",
    "            \n",
    "            # make sure the text is at most 3 digits and a dot\n",
    "            decimals = 2\n",
    "            if matrix[i, j] > 99:\n",
    "                decimals = 0\n",
    "            elif matrix[i, j] > 9:\n",
    "                decimals = 1\n",
    "            cell_text = round(matrix[i, j], decimals)\n",
    "            if decimals == 0:\n",
    "                cell_text = int(cell_text)\n",
    "            plt.text(j, i, cell_text, ha='center', va='center', color='white', fontsize=4)\n",
    "\n",
    "\n",
    "    # plt.show()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # title\n",
    "    plt.title(name)\n",
    "\n",
    "    if isinstance(format, list):\n",
    "        for f in format:\n",
    "            if f[0] != '.':\n",
    "                f = '.' + f\n",
    "            filepath = os.path.join(path, name + f)\n",
    "            fig.savefig(filepath)\n",
    "    else:\n",
    "        if format[0] != '.':\n",
    "            format = '.' + format\n",
    "        filepath = os.path.join(path, name + format)\n",
    "        fig.savefig(filepath)\n",
    "\n",
    "def visualize_matrix_graph(matrix, instances, instance_types_dicts, name='some_matrix_graph', path=None, node_size_mode = \"sqrt\", raise_mode = \"prune\"):\n",
    "    path = get_output_path(path, visualization=True)\n",
    "\n",
    "    SEED = 17\n",
    "    K_SPRRING = 18\n",
    "    MIN_VALUE = 0.01\n",
    "\n",
    "    scale = len(instances) * .12\n",
    "    # Create a new figure\n",
    "    x = scale / 10 * 16\n",
    "    y = scale / 10 * 9\n",
    "    fig = plt.figure(figsize=(x, y))\n",
    "\n",
    "    # normalize the proximity matrix\n",
    "    matrix = matrix / matrix.max()\n",
    "\n",
    "    \n",
    "    # Make sure the matrix is not completely stretched out\n",
    "    if matrix.min() < MIN_VALUE:\n",
    "        if raise_mode == \"prune\":\n",
    "            # remove every value that is below MIN_VALUE\n",
    "            matrix = np.where(matrix < MIN_VALUE, 0, matrix)\n",
    "        elif raise_mode == \"sqrt\":\n",
    "            while np.min(matrix[np.nonzero(matrix)]) < MIN_VALUE:\n",
    "                matrix = np.sqrt(matrix)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown raise mode\")\n",
    "\n",
    "    # alternatives are:\n",
    "    # \"linear\" - take proximity as is\n",
    "    # \"sqrt\" - sqrt(proximity)\n",
    "    # \"log\" - log(proximity)\n",
    "    if node_size_mode == \"log\":\n",
    "        # TODO: see how this works with log(1)\n",
    "        nodesize_map = [np.log(matrix[:, i].sum() + 1) for i in range(len(instances))]\n",
    "    elif node_size_mode == \"sqrt\":\n",
    "        nodesize_map = [np.sqrt(matrix[:, i].sum()) for i in range(len(instances))]\n",
    "    elif node_size_mode == \"linear\":\n",
    "        nodesize_map = [matrix[:, i].sum()for i in range(len(instances))]\n",
    "    else:\n",
    "        nodesize_map = [matrix[:, i].sum() for i in range(len(instances))]\n",
    "        \n",
    "    # print(max(nodesize_map))\n",
    "    # print(min(nodesize_map))\n",
    "\n",
    "    nodesize_map = np.array(nodesize_map) / max(nodesize_map) * 1000\n",
    "\n",
    "    # print(max(nodesize_map))\n",
    "    # print(min(nodesize_map))\n",
    "\n",
    "\n",
    "    # Create a graph from the proximity matrix\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "\n",
    "    # Specify the layout\n",
    "    pos = nx.spring_layout(G, seed=SEED, k=K_SPRRING/math.sqrt(G.order()))  # Seed for reproducibility\n",
    "\n",
    "    color_map = []\n",
    "\n",
    "    color = {\n",
    "        \"process\": \"#1f77b4\",  # muted blue\n",
    "        \"software\": \"#ff7f0e\",  # safety orange\n",
    "        \"data item\": \"#2ca02c\",  # cooked asparagus green\n",
    "        \"data model\": \"#d62728\",  # brick red\n",
    "        \"data format specification\": \"#9467bd\",  # muted purple\n",
    "        \"interchange format\": \"#8c564b\",  # chestnut brown\n",
    "        # \"source\": \"#e377c2\",  # raspberry yogurt pink\n",
    "    }\n",
    "\n",
    "    for instance in instances:\n",
    "        added = False\n",
    "        for instance_type in instance_types_dicts:\n",
    "            if instance in instance_types_dicts[instance_type]:\n",
    "                color_map.append(color[instance_type])\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            color_map.append(\"grey\")\n",
    "\n",
    "    # Draw the graph\n",
    "    options = {\n",
    "        \"edge_color\": \"grey\",\n",
    "        \"linewidths\": 0.5,\n",
    "        \"width\": 0.5,\n",
    "        \"with_labels\": True,  # This will add labels to the nodes\n",
    "        \"labels\": {i: label for i, label in enumerate(instances)},\n",
    "        \"node_color\": color_map,\n",
    "        \"node_size\": nodesize_map,\n",
    "        # \"edge_color\": \"white\",\n",
    "        # \"alpha\": 0.9,\n",
    "    }\n",
    "\n",
    "    # print(nx.is_weighted(G))\n",
    "\n",
    "\n",
    "    # nx.set_edge_attributes(G, values = 1, name = 'weight')\n",
    "\n",
    "    nx.draw(G, pos, **options, ax=fig.add_subplot(111))\n",
    "\n",
    "    # Make the graph more spacious\n",
    "    fig.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.9)\n",
    "\n",
    "    # Create a patch for each color\n",
    "    patches = [mpatches.Patch(color=color[key], label=key) for key in color]\n",
    "\n",
    "    # Add the legend to the graph\n",
    "    plt.legend(handles=patches, loc='upper right', fontsize='x-large')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # save plot to file\n",
    "    filepath = os.path.join(path, name)\n",
    "    fig.savefig(filepath + '.png')\n",
    "    fig.savefig(filepath + '.svg')\n",
    "\n",
    "    # nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "def sankey(matrix, instances, instance_types_dicts, name='some_sankey', path = None):\n",
    "    #TODO: Implement a method to create one graph per Process\n",
    "    path = get_output_path(path, visualization=True)\n",
    "    # Convert the proximity matrix into a list of source nodes, target nodes, and values\n",
    "    sources = []\n",
    "    targets = []\n",
    "    values = []\n",
    "\n",
    "    x_pos=[0] * len(instances)\n",
    "    y_pos=[0] * len(instances)\n",
    "    color_map=[0] * len(instances)\n",
    "\n",
    "    max_types = len(instance_types_dicts)\n",
    "    type_positions = [0.1 + (i / max_types) * 0.8 for i in range(max_types)]\n",
    "\n",
    "    color = {\n",
    "        \"process\": \"#1f77b4\",  # muted blue\n",
    "        \"software\": \"#ff7f0e\",  # safety orange\n",
    "        \"data item\": \"#2ca02c\",  # cooked asparagus green\n",
    "        \"data model\": \"#d62728\",  # brick red\n",
    "        \"data format specification\": \"#9467bd\",  # muted purple\n",
    "        \"interchange format\": \"#8c564b\",  # chestnut brown\n",
    "        # \"source\": \"#e377c2\",  # raspberry yogurt pink\n",
    "    }\n",
    "    color = list(color.values())\n",
    "\n",
    "    space = {}\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        source_type = None\n",
    "\n",
    "        for j in range(matrix.shape[1]):\n",
    "            target_type = None\n",
    "            \n",
    "            for type_depth, type in enumerate(instance_types_dicts):\n",
    "                if instances[i] in instance_types_dicts[type]:\n",
    "                    source_type = type_depth\n",
    "                if instances[j] in instance_types_dicts[type]:\n",
    "                    target_type = type_depth\n",
    "\n",
    "            # only keep directly forward moving connections\n",
    "            if target_type - source_type != 1:\n",
    "                continue\n",
    "\n",
    "            # only keep forward moving connections\n",
    "            if target_type - source_type <= 0:\n",
    "                continue\n",
    "\n",
    "            if source_type not in space:\n",
    "                space[source_type] = {}\n",
    "            if i not in space[source_type]:\n",
    "                space[source_type][i] = 0\n",
    "            space[source_type][i] += matrix[i][j]\n",
    "            \n",
    "            if target_type not in space:\n",
    "                space[target_type] = {}\n",
    "            if j not in space[target_type]:\n",
    "                space[target_type][j] = 0\n",
    "            space[target_type][j] += matrix[i][j]\n",
    "\n",
    "            x_pos[i] = type_positions[source_type]\n",
    "            x_pos[j] = type_positions[target_type]\n",
    "            color_map[i] = color[source_type]\n",
    "            color_map[j] = color[target_type]\n",
    "            if matrix[i][j] > 0.0:  # Ignore zero values\n",
    "                sources.append(i)\n",
    "                targets.append(j)\n",
    "                values.append(matrix[i][j])\n",
    "\n",
    "    for type in space:\n",
    "        sum_values = sum(space[type].values())\n",
    "        space[type] = {k: v/sum_values for k, v in sorted(space[type].items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # assign each instance a proper y position\n",
    "    for type in space:\n",
    "        bottom = 0.1\n",
    "        for i, instance in enumerate(space[type]):\n",
    "            y_pos[instance] = bottom\n",
    "            bottom += space[type][instance] * 0.8\n",
    "\n",
    "    nodes = dict(\n",
    "        # pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=instances,\n",
    "        color=color_map,\n",
    "        x=x_pos,\n",
    "        y=y_pos,\n",
    "        align=\"right\",\n",
    "    )\n",
    "\n",
    "    # Create a Sankey diagram\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=nodes,\n",
    "        link=dict(\n",
    "            source=sources,\n",
    "            target=targets,\n",
    "            value=values\n",
    "        )\n",
    "    )])\n",
    "\n",
    "\n",
    "    fig.update_layout(width=1920, height=1080)\n",
    "\n",
    "\n",
    "    fig.update_layout(title_text=\"Sankey Diagram\", font_size=10)\n",
    "    # fig.show()\n",
    "    \n",
    "    filepath = os.path.join(path, name)\n",
    "    fig.write_image(filepath + '.png')\n",
    "    fig.write_image(filepath + '.svg')\n",
    "    fig.write_html(filepath + '.html')\n",
    "\n",
    "# Represent a matrix\n",
    "def process_matrix(config:Config, matrix, rows, columns=None, name = 'some_matrix', visualize = True, path = None, instance_types_dicts = None, mode = \"sqrt\"):\n",
    "    if columns is None:\n",
    "        columns = rows\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    df = pd.DataFrame(matrix, columns=columns, index=rows)\n",
    "    filepath = os.path.join(path, name)\n",
    "    df.to_csv(filepath + '.csv', sep=config.csv_separator, decimal=config.csv_decimal)\n",
    "    path = config.get_output_path(path, visualization=True)\n",
    "    if visualize:\n",
    "        if instance_types_dicts:\n",
    "            sankey(matrix, rows, instance_types_dicts, name + '_sankey', path=path)\n",
    "            if mode:\n",
    "                visualize_matrix_graph(matrix, rows, instance_types_dicts, name + '_graph', path=path, node_size_mode=config.proximity_mode)\n",
    "            else:\n",
    "                visualize_matrix_graph(matrix, rows, instance_types_dicts, name + '_graph', path=path)\n",
    "        visualize_matrix(matrix, rows, columns, name, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize timeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def visualize_timeline(config:Config, year_instance_occurrence_matrix, year_papers, instances, instance_types_dicts, name='some_timeline', path=None, recursion_depth=0, start_index=0, error_matrix=None, error_instances=None):\n",
    "    path = config.get_output_path(path, visualization=True)\n",
    "    years = list(year_papers.keys())\n",
    "    max_papers = max([len(year_papers[year]) for year in years])\n",
    "    yearly_papers = [len(year_papers[year]) for year in years]\n",
    "\n",
    "\n",
    "    ALPHA_ERROR_LINE = 0.3\n",
    "    ALPHA_ERROR_ZONE = 0.2\n",
    "    ALPHA_PAPER_BAR = 0.3\n",
    "\n",
    "\n",
    "    for type in instance_types_dicts:\n",
    "        use = [instance in instance_types_dicts[type] for instance in instances]\n",
    "        type_instances = [instance for instance, use_flag in zip(instances, use) if use_flag]\n",
    "        total_occurrences = [np.sum(year_instance_occurrence_matrix[:, instances.index(instance)]) for instance in type_instances]\n",
    "        type_instances_sorted = [x for _, x in sorted(zip(total_occurrences, type_instances), key=lambda pair: pair[0], reverse=True)]\n",
    "        \n",
    "        PARTITION_SIZE = 10\n",
    "        # if error_instances is not None:\n",
    "        #     PARTITION_SIZE = int(0.5 * PARTITION_SIZE)\n",
    "        \n",
    "        type_matrix = year_instance_occurrence_matrix[:, [instances.index(instance) for instance in type_instances_sorted]]\n",
    "        factor = 1\n",
    "        size_x = (2 + len(years) / 6) * factor\n",
    "        size_y = (2 + max_papers / 15) * factor\n",
    "        size_y_2 = (2 + PARTITION_SIZE / 2) * factor\n",
    "        size_y = max(size_y, size_y_2)\n",
    "        fig, ax = plt.subplots(figsize=(size_x, size_y), dpi=300)\n",
    "\n",
    "        ax.set_xticks(range(len(years)))\n",
    "        years_labels = [year if len(year_papers[year]) > 0 else '' for year in years]\n",
    "        ax.set_xticklabels(years_labels, fontsize=10, rotation=90)\n",
    "        \n",
    "        step_size = max(1, math.ceil(max_papers / 10))\n",
    "        ax.set_yticks(np.arange(0, max_papers + 1, step=step_size))\n",
    "        ax.set_yticklabels([str(int(x)) for x in np.arange(0, max_papers + 1, step=step_size)], fontsize=10)\n",
    "        \n",
    "        # set y axis label\n",
    "        ax.set_ylabel('absolute', fontsize=10)\n",
    "\n",
    "        plt.bar(range(len(years)), yearly_papers, color='black', alpha=ALPHA_PAPER_BAR, label=f\"Total papers ({sum(yearly_papers)})\", zorder=0)\n",
    "\n",
    "        line_count = 0\n",
    "        i = start_index\n",
    "        while line_count < PARTITION_SIZE and i < len(type_instances_sorted):\n",
    "            instance = type_instances_sorted[i]\n",
    "            yearly_occurrences = type_matrix[:, i]\n",
    "            i_total_occurrences = yearly_occurrences.sum()\n",
    "            label = f\"{instance} ({i_total_occurrences})\"\n",
    "            values = yearly_occurrences\n",
    "            line = plt.plot(range(len(years)), values, label=label, zorder=3)[0]\n",
    "            line_count += 1\n",
    "            if error_matrix is not None and instance in error_instances:\n",
    "                color = line.get_color()\n",
    "                errors = error_matrix[:, error_instances.index(instance)]\n",
    "                errors_plus = yearly_occurrences + errors\n",
    "                line.set_label(f\"{instance} ({i_total_occurrences}-{sum(errors_plus)})\")\n",
    "                # Plot the error as a half transparent line on top of the normal line\n",
    "                plt.plot(range(len(years)), errors_plus, color=color, alpha=ALPHA_ERROR_LINE, label=f\"{instance} (w/o proximity)\", zorder=2)\n",
    "                line_count += 1\n",
    "                # color in the area between the normal line and the error line\n",
    "                plt.fill_between(range(len(years)), yearly_occurrences, errors_plus, color=color, alpha=ALPHA_ERROR_ZONE, zorder=1)\n",
    "            i += 1\n",
    "                \n",
    "                # plt.scatter(range(len(years)), errors, color='red', label=f\"{instance} (error)\", zorder=1)\n",
    "        stop_index = i\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        plt.title(f\"Number of papers covering {type} instances (#{start_index+1} to #{stop_index} of {len(type_instances_sorted)})\")\n",
    "\n",
    "        # Inset for relative values\n",
    "        fig.canvas.draw()\n",
    "        x_lim = ax.get_xlim()  # Get the current x-axis limits from the main plot\n",
    "\n",
    "        bbox = ax.get_position()\n",
    "        bb_left, bb_bottom = bbox.x0, bbox.y0\n",
    "        bb_width, bb_height = bbox.width, bbox.height\n",
    "\n",
    "        ax_inset = plt.axes([bb_left, 0.05, bb_width, 0.15], alpha=ALPHA_PAPER_BAR, facecolor='lightgrey')\n",
    "        for i, instance in enumerate(type_instances_sorted[start_index:stop_index], start=start_index):\n",
    "            yearly_occurrences = type_matrix[:, i]\n",
    "            values_relative = [occurrences / papers if papers > 0 else 0 for occurrences, papers in zip(yearly_occurrences, yearly_papers)]\n",
    "            line_relative = ax_inset.plot(range(len(years)), values_relative, label=f\"{instance} (relative)\", zorder=3)[0]\n",
    "\n",
    "            # add the error part\n",
    "            if error_matrix is not None and instance in error_instances:\n",
    "                color = line_relative.get_color()\n",
    "                errors = error_matrix[:, error_instances.index(instance)]\n",
    "                errors_plus = yearly_occurrences + errors\n",
    "                errors_relative = [error / papers if papers > 0 else 0 for error, papers in zip(errors_plus, yearly_papers)]\n",
    "                if max(errors_relative) > 1:\n",
    "                    print(f\"Error: {instance} has a relative error > 1\")\n",
    "                    # throw an exception because this should never be the case:\n",
    "                    # raise Exception(f\"Error: relative {instance} occurence + error > 1\")\n",
    "\n",
    "\n",
    "                ax_inset.plot(range(len(years)), errors_relative, alpha=ALPHA_ERROR_LINE, color=color, label=f\"{instance} (error, relative)\", zorder=2)\n",
    "                # color in the area between the normal line and the error line\n",
    "                ax_inset.fill_between(range(len(years)), values_relative, errors_relative, alpha=ALPHA_ERROR_ZONE, color=color, zorder=1)\n",
    "        \n",
    "        ax_inset.set_xlim(x_lim)\n",
    "\n",
    "        ax_inset.set_xticks([])\n",
    "        ax_inset.set_yticks(np.arange(0, 1.1, step=0.5))\n",
    "        ax_inset.set_yticklabels([f\"{int(x*100)}%\" for x in np.arange(0, 1.1, step=0.5)], fontsize=8)\n",
    "\n",
    "        # set y axis label\n",
    "        ax_inset.set_ylabel('relative', fontsize=10)\n",
    "\n",
    "        plt.subplots_adjust(bottom=0.3)\n",
    "\n",
    "        start_string = f\"{start_index+1}\"\n",
    "        stop_string = f\"{stop_index}\"\n",
    "\n",
    "        # fill up with 0 to have a constant length\n",
    "        start_string = \"0\" * (3 - len(start_string)) + start_string\n",
    "        stop_string = \"0\" * (3 - len(stop_string)) + stop_string\n",
    "\n",
    "        part_appendix = f\"{start_string}_to_{stop_string}\"\n",
    "        filepath = os.path.join(path, name)\n",
    "        plt.savefig(f\"{filepath}_{type.replace(' ', '_')}_{part_appendix}.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        start_index = stop_index\n",
    "        if start_index < len(type_instances_sorted):\n",
    "            # if recursion_depth > 0:\n",
    "            #     break\n",
    "            visualize_timeline(config, year_instance_occurrence_matrix, year_papers, instances, {type: instance_types_dicts[type]}, name, path=path, recursion_depth=recursion_depth + 1, start_index=start_index, error_matrix=error_matrix, error_instances=error_instances)\n",
    "        start_index = 0\n",
    "            \n",
    "if config.visualize:\n",
    "    yearly_error_matrix, year_error_papers = create_year_paper_occurrence_matrix(papers_metadata, error_matrix, error_papers, is_error_matrix=True)\n",
    "    visualize_timeline(config, year_instance_occurrence_matrix, year_papers, instances, instance_types_dicts, name=\"year_instance_occurrence_matrix\", error_matrix=yearly_error_matrix, error_instances=error_instances) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_list(config, instances, \"instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Dicts: instance_types_dicts, papers_metadata, instance_piece_gap\n",
    "process_dict(config, instance_types_dicts, 'instance_types_dicts')\n",
    "process_dict(config, papers_metadata, 'papers_metadata')\n",
    "process_dict(config, instance_piece_gap, 'instance_piece_gaps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataframe(config, rules, 'rules')\n",
    "# rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataframe(config, rules_cross_type, 'cross_type_rules')\n",
    "# cross_type_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_matrix(config, paper_instance_occurrence_matrix, rows=papers, columns=instances, name='paper_instance_occurrence_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_matrix(config, error_matrix, rows=error_papers, columns=error_instances, name='error_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_matrix(config, instance_instance_co_occurrence_matrix, rows=instances, columns=instances, name='instance_instance_co_occurrence_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_matrix(config, instance_instance_proximity_matrix, rows=proximity_instances, columns=proximity_instances, name='instance_instance_proximity_matrix', instance_types_dicts=instance_types_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "## Pre-Processing\n",
    "Using Completion Rating in %\n",
    "\n",
    "### 80 %: Full Text extraction\n",
    "* lacking noise removal (Headings, page numbers, ...)\n",
    "* lacking line-break mending\n",
    "\n",
    "### 100 %: Bag of Words\n",
    "* The problem with BoW that the words are looked at seperatly and correlation is not really clear.\n",
    "\n",
    "\n",
    "### 99 %: TF-IDF\n",
    "* tf-idf only on terms\n",
    "\n",
    "### ? %: Part Of Speech (POS) Tagging, Named Entity Recognition (NER) \n",
    "* ready, but not used currently\n",
    "\n",
    "## Visualize\n",
    "\n",
    "### 85 % Matrix\n",
    "* CSV and Dataframe dumps work fine\n",
    "* Visualization as PNG or SVG are extremely large.\n",
    "  * DPI regulation works to somewhat keep this in check, but images still reach 20 MB\n",
    "\n",
    "### 100 % Timeline\n",
    "* arrange the papers on a timeline and identify the flow of:\n",
    "  * Processes\n",
    "  * File formats\n",
    "  * software\n",
    "  * ...\n",
    "* Additional ideas:\n",
    "  * Compare this to goolge trends\n",
    "\n",
    "## Future Work\n",
    "Using Difficulty ranked (DR) solutions:\n",
    "\n",
    "### Step 0: Look it up\n",
    "\n",
    "#### More visualization\n",
    "* https://github.com/JasonKessler/scattertext \n",
    "* https://pypi.org/project/yellowbrick/\n",
    "\n",
    "#### NLP Pipelines:\n",
    "https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "\n",
    "#### BLAST: Basic Local Alignment Search Tool\n",
    "  * starting point: https://academic.oup.com/bioinformatics/article/39/12/btad716/7450067\n",
    "\n",
    "#### AMIE 3\n",
    "  * https://luisgalarraga.de/docs/amie3.pdf\n",
    "  * https://github.com/dig-team/amie\n",
    "\n",
    "### Step 1: Low hanging fruits\n",
    "\n",
    "#### 1/5 DR: multi-word detection (n-gram)\n",
    "Tools:  nltk, spaCy, etc.\n",
    "\n",
    "### Step 2: Not-to-tricky follow-up\n",
    "\n",
    "#### 3/5 DR: Acronym Expansion\n",
    "Tools: spaCy - https://spacy.io/universe/project/neuralcoref\n",
    "\n",
    "#### 3/5 DR: CoReference resolution\n",
    "Tools: spaCy - https://spacy.io/universe/project/neuralcoref or https://huggingface.co/coref/ (you can use the model out of the box)\n",
    "\n",
    "### Step 3: Vector-magic\n",
    "\n",
    "#### 2-4/5 DR: Word embedding\n",
    "* Find out, that jpeg and png are similar\n",
    "\n",
    "(depending on your needs) - Tools: gensim - https://www.analyticsvidhya.com/blog/2023/07/step-by-step-guide-to-word2vec-with-gensim/\n",
    "\n",
    "#### 3/5 DR: document embedding\n",
    "Tools: gensim - https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "I would also check graph embeddings, sentence embeddings, and recently there is LLM2Vec\n",
    "\n",
    "### Step 3.1: Reaping the vector-rewards\n",
    "\n",
    "#### 1/5 DR: clustering\n",
    "Tools: sklearn\n",
    "\n",
    "Requirements: Need to have data as numbers first. This is quite possible after generating embeddings\n",
    "\n",
    "### Step 9: Won't be happening in this paper\n",
    "* Paper classes\n",
    "* Subclasses of paper classes\n",
    "* model which process is a subprocess of another process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
