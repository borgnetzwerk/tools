{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Instance occurrence in Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, for_git=True):\n",
    "        self.for_git = for_git\n",
    "        self.visualize = not for_git\n",
    "        # self.visualize = True\n",
    "        self.csv_separator = \",\" if for_git else \";\"\n",
    "        self.csv_decimal = \".\" if for_git else \",\"\n",
    "        self.only_included_papers = True\n",
    "        self.properties = [\"source\"]\n",
    "        self.proximity_mode = \"sqrt\"  # \"log\" mode is untested/unsafe, prefer \"sqrt\"\n",
    "        self.base_path = \"data/\"  # Default base path\n",
    "        self.subset_path = \"data_subset/\"\n",
    "        self.visualization_path = \"visualization/\"\n",
    "        self.ontology_path = \"ontology/\"\n",
    "        self.orkg_path = \"ORKG/\"\n",
    "        self.folder_path = \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR\"\n",
    "        self.papers_path = (\n",
    "            \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/02_nlp\"\n",
    "        )\n",
    "        self.review_path = (\n",
    "            \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/03_notes\"\n",
    "        )\n",
    "        self.csv_file = \"C:/workspace/borgnetzwerk/tools/scripts/SLR/data.csv\"\n",
    "        self.obsidian_path = \"ontology/obsidian/\"\n",
    "        self.gap_too_large_threshold = 1000\n",
    "        self.savetime_on_fulltext = (\n",
    "            False  # If True, operations on fulltext will be kept to a minimum\n",
    "        )\n",
    "        self.try_to_save_time = False\n",
    "        self.recalculate_pos_in_paper = (\n",
    "            False  # while the calculation is inefficient, just load from file\n",
    "        )\n",
    "        self.debug = True\n",
    "\n",
    "        self.wikidata_query_limit = 20\n",
    "\n",
    "        self.proximity_seed = 17\n",
    "        self.proximity_k_spring = 18\n",
    "        self.proximity_min_value = 0.1\n",
    "\n",
    "    def get_output_path(self, path=\"\", visualization=False):\n",
    "        \"\"\"\n",
    "        Determine the output path based on the configuration and parameters.\n",
    "        \"\"\"\n",
    "        if not path:\n",
    "            path = self.subset_path if self.only_included_papers else self.base_path\n",
    "        if visualization and not path.endswith(self.visualization_path):\n",
    "            path = os.path.join(path, self.visualization_path)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        return path\n",
    "\n",
    "\n",
    "# Usage\n",
    "config = Config(for_git=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug test\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "\n",
    "\n",
    "def run_debug_test(\n",
    "    config: Config,\n",
    "    instances: list[str] = None,\n",
    "    papers: list[str] = None,\n",
    "    paper_instance_occurrence_matrix: np.ndarray = None,\n",
    "):\n",
    "    if \"ikewiki\" in instances:\n",
    "        ike_index = instances.index(\"ikewiki\")\n",
    "        sum_ikewiki = np.sum(paper_instance_occurrence_matrix[:, ike_index])\n",
    "        if sum_ikewiki > 1:\n",
    "            raise Exception(\n",
    "                f\"Only one paper should contain 'ikewiki', but {sum_ikewiki} do.\"\n",
    "            )\n",
    "\n",
    "\n",
    "def time_function(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        appendix = \"\"\n",
    "        # if instances in args:\n",
    "        if \"instances\" in kwargs:\n",
    "            # append len of instances\n",
    "            appendix = f\"({len(kwargs['instances'])} instances\"\n",
    "        if \"papers\" in kwargs:\n",
    "            if appendix:\n",
    "                appendix += \", \"\n",
    "            appendix += f\"{len(kwargs['papers'])} papers\"\n",
    "        if appendix:\n",
    "            appendix += \")\"\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{func.__name__} executed in {end_time - start_time} seconds\" + appendix)\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful functions\n",
    "def index_all(input_list, item):\n",
    "    return [i for i, x in enumerate(input_list) if x == item]\n",
    "\n",
    "\n",
    "def split_string(input_string, delimiters=[\" \", \"-\", \"_\"]):\n",
    "    for delimiter in delimiters:\n",
    "        input_string = \" \".join(input_string.split(delimiter))\n",
    "    return input_string.split()\n",
    "\n",
    "def path_cleaning(text):\n",
    "    mapping = {\n",
    "        \"\\\\\": \"/\",\n",
    "    }\n",
    "    for key, value in mapping.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent a dict\n",
    "import csv\n",
    "import os\n",
    "from itables import init_notebook_mode, show\n",
    "import json\n",
    "\n",
    "# better represent dataframes\n",
    "if not config.for_git:\n",
    "    init_notebook_mode(all_interactive=True)\n",
    "\n",
    "\n",
    "def prep_dict(input_dict):\n",
    "    changes_needed = {}\n",
    "    # get ONLY THE FIRST key and value\n",
    "    key, value = next(iter(input_dict.items()))\n",
    "    key_change = \"\"\n",
    "    value_change = \"\"\n",
    "    if isinstance(key, frozenset):\n",
    "        key_change = \"str\"\n",
    "    if isinstance(value, set):\n",
    "        value_change = \"list\"\n",
    "    if isinstance(value, dict):\n",
    "        value_change = prep_dict(value)\n",
    "    changes_needed[key_change] = value_change\n",
    "    return changes_needed\n",
    "\n",
    "\n",
    "def change_dict(input_dict, changes_needed):\n",
    "    for key, value in changes_needed.items():\n",
    "        if key == \"str\" and value == \"list\":\n",
    "            input_dict = {str(key): list(value) for key, value in input_dict.items()}\n",
    "        elif key == \"str\":\n",
    "            input_dict = {str(key): value for key, value in input_dict.items()}\n",
    "        elif value == \"list\":\n",
    "            input_dict = {key: list(value) for key, value in input_dict.items()}\n",
    "        elif key == \"str\" and value == \"dict\":\n",
    "            input_dict = {\n",
    "                str(key): change_dict(value, changes_needed[\"str\"])\n",
    "                for key, value in input_dict.items()\n",
    "            }\n",
    "        elif value == \"dict\":\n",
    "            input_dict = {\n",
    "                key: change_dict(value, changes_needed[\"\"])\n",
    "                for key, value in input_dict.items()\n",
    "            }\n",
    "    return input_dict\n",
    "\n",
    "\n",
    "def process_list(config: Config, input_list: list, filename=\"some_list\", path=None):\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    filepath = os.path.join(path, filename)\n",
    "    with open(filepath + \".txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in input_list:\n",
    "            f.write(f\"{item}\\n\")\n",
    "\n",
    "\n",
    "def process_dict(config: Config, input_dict: dict, filename=\"some_dict\", path=None):\n",
    "    # convert all sets to lists\n",
    "    changes_needed = prep_dict(input_dict)\n",
    "    processed_dict = change_dict(input_dict, changes_needed)\n",
    "\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    filepath = os.path.join(path, filename)\n",
    "    with open(filepath + \".json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(processed_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # TODO: Only do this for dicts that need statistical analysis\n",
    "    requires_analysis = False\n",
    "    value = list(processed_dict.values())[\n",
    "        0\n",
    "    ]  # Convert dict_values to list to make it subscriptable\n",
    "    if isinstance(value, dict):\n",
    "        value = list(value.values())[\n",
    "            0\n",
    "        ]  # Convert dict_values to list to make it subscriptable\n",
    "        if isinstance(value, int) or isinstance(value, float):\n",
    "            requires_analysis = True\n",
    "    if not requires_analysis:\n",
    "        return\n",
    "\n",
    "    container = [[\"Instance\", \"Min\", \"Max\", \"Mean\", \"Median\", \"Std\"]]\n",
    "\n",
    "    for instance, papers in input_dict.items():\n",
    "\n",
    "        # print(f\"Instance: {instance}\")\n",
    "        gaps = papers.values()\n",
    "        # generate all kinds of statistical values\n",
    "        min_gap = min(gaps)\n",
    "        max_gap = max(gaps)\n",
    "        mean_gap = sum(gaps) / len(gaps)\n",
    "        median_gap = np.median(list(gaps))\n",
    "        std_gap = np.std(list(gaps))\n",
    "        container.append([instance, min_gap, max_gap, mean_gap, median_gap, std_gap])\n",
    "\n",
    "    filepath = os.path.join(path, filename)\n",
    "\n",
    "    # TODO: Handle CSV separator\n",
    "    # if not for_git:\n",
    "    # Function to convert a single value\n",
    "    # def convert_decimal_delimiter(value, decimal=CSV_DECIMAL):\n",
    "    #     if isinstance(value, float):\n",
    "    #         return f\"{value}\".replace('.', decimal)\n",
    "    #     return value\n",
    "\n",
    "    # # Convert all floats in your container to strings with the desired decimal delimiter\n",
    "    # container = [[convert_decimal_delimiter(value) for value in row] for row in container]\n",
    "\n",
    "    # write to csv\n",
    "    with open(filepath + \".csv\", \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file, delimiter=config.csv_separator)\n",
    "        writer.writerows(container)\n",
    "\n",
    "\n",
    "def process_dataframe(config: Config, input_df, name=\"some_df\", path=None):\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    filepath = os.path.join(path, name)\n",
    "\n",
    "    # convert all froensets to strings\n",
    "    for col in input_df.columns:\n",
    "        # input_df[col] = input_df[col].apply(lambda x: \"_\".join(x))\n",
    "        first_element_type = input_df[col].apply(type).iloc[0]\n",
    "        if first_element_type == frozenset:\n",
    "            input_df[col] = input_df[col].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    input_df.to_csv(\n",
    "        filepath + \".csv\", sep=config.csv_separator, decimal=config.csv_decimal\n",
    "    )\n",
    "    show(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize co-occurrences\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def visualize_matrix(\n",
    "    config: Config,\n",
    "    matrix: np.ndarray,\n",
    "    rows: list[str],\n",
    "    columns: list[str] = None,\n",
    "    name: str = \"some_matrix\",\n",
    "    format=\".png\",\n",
    "    path=None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualizes a matrix as a heatmap.\n",
    "    matrix: The matrix to visualize\n",
    "    rows: The labels for the rows\n",
    "    columns: The labels for the columns\n",
    "    name: The name of the file to save\n",
    "    format: The format of the file to save (default: '.png', also accepts '.svg' and '.pdf', also accepts a list of formats)\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = rows\n",
    "\n",
    "    if path is None:\n",
    "        path = config.get_output_path(visualization=True)\n",
    "\n",
    "    ## Calculate the maximum size of the plot\n",
    "    dpi = 300\n",
    "    max_dpi = 600\n",
    "    if config.for_git:\n",
    "        dpi = 96\n",
    "        max_dpi = 200\n",
    "    max_pixel = 2**16  # Maximum size in any direction\n",
    "    max_size = max_pixel / dpi  # Maximum size in any direction\n",
    "    max_size_total = max_size * max_size  # Maximum size in total\n",
    "    max_size_total *= 0.05  # produce smaller files\n",
    "\n",
    "    # Experience value of space required per cell\n",
    "    factor = 0.18\n",
    "    size_x: float = 2 + len(columns) * factor\n",
    "    size_y: float = 3 + len(rows) * 0.8 * factor\n",
    "\n",
    "    while size_x * size_y < max_size_total and dpi < max_dpi:\n",
    "        dpi /= 0.95\n",
    "        max_size_total *= 0.95\n",
    "\n",
    "    if dpi > max_dpi:\n",
    "        dpi = max_dpi\n",
    "\n",
    "    while size_x * size_y > max_size_total:\n",
    "        dpi *= 0.95\n",
    "        max_size_total /= 0.95\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(size_x, size_y), dpi=dpi)\n",
    "\n",
    "    cax = ax.matshow(matrix, cmap=\"viridis\")\n",
    "\n",
    "    # use labels from instance_occurrences\n",
    "    ax.set_xticks(range(len(columns)))\n",
    "    ax.set_xticklabels(list(columns), fontsize=10, rotation=90)\n",
    "    ax.set_yticks(range(len(rows)))\n",
    "    ax.set_yticklabels(list(rows), fontsize=10)\n",
    "\n",
    "    # # adjust the spacing between the labels\n",
    "    # plt.gca().tick_params(axis='x', which='major', pad=15)\n",
    "    # plt.gca().tick_params(axis='y', which='major', pad=15)\n",
    "\n",
    "    # show the number of co-occurrences in each cell, if greater than 0\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[i])):\n",
    "            if matrix[i, j] == 0:\n",
    "                continue\n",
    "            # if co_occurrences[i, j] > 100:\n",
    "            #     continue\n",
    "\n",
    "            # make sure the text is at most 3 digits and a dot\n",
    "            decimals = 2\n",
    "            if matrix[i, j] > 99:\n",
    "                decimals = 0\n",
    "            elif matrix[i, j] > 9:\n",
    "                decimals = 1\n",
    "            cell_text = round(matrix[i, j], decimals)\n",
    "            if decimals == 0:\n",
    "                cell_text = int(cell_text)\n",
    "            plt.text(\n",
    "                j, i, cell_text, ha=\"center\", va=\"center\", color=\"white\", fontsize=4\n",
    "            )\n",
    "\n",
    "    # plt.show()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # title\n",
    "    plt.title(name)\n",
    "\n",
    "    if isinstance(format, list):\n",
    "        for f in format:\n",
    "            if f[0] != \".\":\n",
    "                f = \".\" + f\n",
    "            filepath = os.path.join(path, name + f)\n",
    "            fig.savefig(filepath)\n",
    "    else:\n",
    "        if format[0] != \".\":\n",
    "            format = \".\" + format\n",
    "        filepath = os.path.join(path, name + format)\n",
    "        fig.savefig(filepath)\n",
    "\n",
    "\n",
    "def visualize_matrix_graph(\n",
    "    config: Config,\n",
    "    matrix,\n",
    "    instances,\n",
    "    instance_types_dicts,\n",
    "    name=\"some_matrix_graph\",\n",
    "    path=None,\n",
    "    node_size_mode=\"sqrt\",\n",
    "    raise_mode=\"prune\",\n",
    "):\n",
    "    if path is None:\n",
    "        path = config.get_output_path(path, visualization=True)\n",
    "\n",
    "    SEED = config.proximity_seed or 17\n",
    "    K_SPRRING = config.proximity_k_spring or 18\n",
    "    MIN_VALUE = config.proximity_min_value or 0.01\n",
    "\n",
    "    scale = len(instances) * 0.12\n",
    "    # Create a new figure\n",
    "    x = scale / 10 * 16\n",
    "    y = scale / 10 * 9\n",
    "    fig = plt.figure(figsize=(x, y))\n",
    "\n",
    "    # normalize the proximity matrix\n",
    "    matrix = matrix / matrix.max()\n",
    "\n",
    "    # Make sure the matrix is not completely stretched out\n",
    "    if matrix.min() < MIN_VALUE:\n",
    "        if raise_mode == \"prune\":\n",
    "            # remove every value that is below MIN_VALUE\n",
    "            matrix = np.where(matrix < MIN_VALUE, 0, matrix)\n",
    "        elif raise_mode == \"sqrt\":\n",
    "            while np.min(matrix[np.nonzero(matrix)]) < MIN_VALUE:\n",
    "                matrix = np.sqrt(matrix)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown raise mode\")\n",
    "\n",
    "    # alternatives are:\n",
    "    # \"linear\" - take proximity as is\n",
    "    # \"sqrt\" - sqrt(proximity)\n",
    "    # \"log\" - log(proximity)\n",
    "    if node_size_mode == \"log\":\n",
    "        # TODO: see how this works with log(1)\n",
    "        nodesize_map = [np.log(matrix[:, i].sum() + 1) for i in range(len(instances))]\n",
    "    elif node_size_mode == \"sqrt\":\n",
    "        nodesize_map = [np.sqrt(matrix[:, i].sum()) for i in range(len(instances))]\n",
    "    elif node_size_mode == \"linear\":\n",
    "        nodesize_map = [matrix[:, i].sum() for i in range(len(instances))]\n",
    "    else:\n",
    "        nodesize_map = [matrix[:, i].sum() for i in range(len(instances))]\n",
    "\n",
    "    # print(max(nodesize_map))\n",
    "    # print(min(nodesize_map))\n",
    "\n",
    "    nodesize_map = np.array(nodesize_map) / max(nodesize_map) * 1000\n",
    "\n",
    "    # print(max(nodesize_map))\n",
    "    # print(min(nodesize_map))\n",
    "\n",
    "    # Create a graph from the proximity matrix\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "\n",
    "    # Specify the layout\n",
    "    pos = nx.spring_layout(\n",
    "        G, seed=SEED, k=K_SPRRING / math.sqrt(G.order())\n",
    "    )  # Seed for reproducibility\n",
    "\n",
    "    color_map = []\n",
    "\n",
    "    color = {\n",
    "        \"process\": \"#1f77b4\",  # muted blue\n",
    "        \"software\": \"#ff7f0e\",  # safety orange\n",
    "        \"data item\": \"#2ca02c\",  # cooked asparagus green\n",
    "        \"data model\": \"#d62728\",  # brick red\n",
    "        \"data format specification\": \"#9467bd\",  # muted purple\n",
    "        # \"interchange format\": \"#8c564b\",  # chestnut brown\n",
    "        # \"source\": \"#e377c2\",  # raspberry yogurt pink\n",
    "    }\n",
    "\n",
    "    for instance in instances:\n",
    "        added = False\n",
    "        for instance_type in instance_types_dicts:\n",
    "            if instance in instance_types_dicts[instance_type]:\n",
    "                color_map.append(color[instance_type])\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            color_map.append(\"grey\")\n",
    "\n",
    "    # Draw the graph\n",
    "    options = {\n",
    "        \"edge_color\": \"grey\",\n",
    "        \"linewidths\": 0.5,\n",
    "        \"width\": 0.5,\n",
    "        \"with_labels\": True,  # This will add labels to the nodes\n",
    "        \"labels\": {i: label for i, label in enumerate(instances)},\n",
    "        \"node_color\": color_map,\n",
    "        \"node_size\": nodesize_map,\n",
    "        # \"edge_color\": \"white\",\n",
    "        # \"alpha\": 0.9,\n",
    "    }\n",
    "\n",
    "    # print(nx.is_weighted(G))\n",
    "\n",
    "    # nx.set_edge_attributes(G, values = 1, name = 'weight')\n",
    "\n",
    "    nx.draw(G, pos, **options, ax=fig.add_subplot(111))\n",
    "\n",
    "    # Make the graph more spacious\n",
    "    fig.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.9)\n",
    "\n",
    "    # Create a patch for each color\n",
    "    patches = [mpatches.Patch(color=color[key], label=key) for key in color]\n",
    "\n",
    "    # Add the legend to the graph\n",
    "    plt.legend(handles=patches, loc=\"upper right\", fontsize=\"x-large\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # save plot to file\n",
    "    filepath = os.path.join(path, name)\n",
    "    fig.savefig(filepath + \".png\")\n",
    "    fig.savefig(filepath + \".svg\")\n",
    "\n",
    "    # nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "\n",
    "def sankey(\n",
    "    config: Config,\n",
    "    matrix,\n",
    "    instances,\n",
    "    instance_types_dicts,\n",
    "    name=\"some_sankey\",\n",
    "    path=None,\n",
    "):\n",
    "    # TODO: Implement a method to create one graph per Process\n",
    "    if path is None:\n",
    "        path = config.get_output_path(path, visualization=True)\n",
    "    # Convert the proximity matrix into a list of source nodes, target nodes, and values\n",
    "    sources = []\n",
    "    targets = []\n",
    "    values = []\n",
    "\n",
    "    x_pos = [0] * len(instances)\n",
    "    y_pos = [0] * len(instances)\n",
    "    color_map = [0] * len(instances)\n",
    "\n",
    "    max_types = len(instance_types_dicts)\n",
    "    type_positions = [0.1 + (i / max_types) * 0.8 for i in range(max_types)]\n",
    "\n",
    "    color = {\n",
    "        \"process\": \"#1f77b4\",  # muted blue\n",
    "        \"software\": \"#ff7f0e\",  # safety orange\n",
    "        \"data item\": \"#2ca02c\",  # cooked asparagus green\n",
    "        \"data model\": \"#d62728\",  # brick red\n",
    "        \"data format specification\": \"#9467bd\",  # muted purple\n",
    "        \"interchange format\": \"#8c564b\",  # chestnut brown\n",
    "        # \"source\": \"#e377c2\",  # raspberry yogurt pink\n",
    "    }\n",
    "    color = list(color.values())\n",
    "\n",
    "    space = {}\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        source_type = None\n",
    "\n",
    "        for j in range(matrix.shape[1]):\n",
    "            target_type = None\n",
    "\n",
    "            for type_depth, type in enumerate(instance_types_dicts):\n",
    "                if instances[i] in instance_types_dicts[type]:\n",
    "                    source_type = type_depth\n",
    "                if instances[j] in instance_types_dicts[type]:\n",
    "                    target_type = type_depth\n",
    "\n",
    "            # only keep directly forward moving connections\n",
    "            if target_type - source_type != 1:\n",
    "                continue\n",
    "\n",
    "            # only keep forward moving connections\n",
    "            if target_type - source_type <= 0:\n",
    "                continue\n",
    "\n",
    "            if source_type not in space:\n",
    "                space[source_type] = {}\n",
    "            if i not in space[source_type]:\n",
    "                space[source_type][i] = 0\n",
    "            space[source_type][i] += matrix[i][j]\n",
    "\n",
    "            if target_type not in space:\n",
    "                space[target_type] = {}\n",
    "            if j not in space[target_type]:\n",
    "                space[target_type][j] = 0\n",
    "            space[target_type][j] += matrix[i][j]\n",
    "\n",
    "            x_pos[i] = type_positions[source_type]\n",
    "            x_pos[j] = type_positions[target_type]\n",
    "            color_map[i] = color[source_type]\n",
    "            color_map[j] = color[target_type]\n",
    "            if matrix[i][j] > 0.0:  # Ignore zero values\n",
    "                sources.append(i)\n",
    "                targets.append(j)\n",
    "                values.append(matrix[i][j])\n",
    "\n",
    "    for type in space:\n",
    "        sum_values = sum(space[type].values())\n",
    "        space[type] = {\n",
    "            k: v / sum_values\n",
    "            for k, v in sorted(\n",
    "                space[type].items(), key=lambda item: item[1], reverse=True\n",
    "            )\n",
    "        }\n",
    "\n",
    "    # assign each instance a proper y position\n",
    "    for type in space:\n",
    "        bottom = 0.1\n",
    "        for i, instance in enumerate(space[type]):\n",
    "            y_pos[instance] = bottom\n",
    "            bottom += space[type][instance] * 0.8\n",
    "\n",
    "    nodes = dict(\n",
    "        # pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=instances,\n",
    "        color=color_map,\n",
    "        x=x_pos,\n",
    "        y=y_pos,\n",
    "        align=\"right\",\n",
    "    )\n",
    "\n",
    "    # Create a Sankey diagram\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Sankey(\n",
    "                node=nodes, link=dict(source=sources, target=targets, value=values)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig.update_layout(width=1920, height=1080)\n",
    "\n",
    "    fig.update_layout(title_text=\"Sankey Diagram\", font_size=10)\n",
    "    # fig.show()\n",
    "\n",
    "    filepath = os.path.join(path, name)\n",
    "    fig.write_image(filepath + \".png\")\n",
    "    fig.write_image(filepath + \".svg\")\n",
    "    fig.write_html(filepath + \".html\")\n",
    "\n",
    "\n",
    "# Represent a matrix\n",
    "def process_matrix(\n",
    "    config: Config,\n",
    "    matrix,\n",
    "    rows,\n",
    "    columns=None,\n",
    "    name=\"some_matrix\",\n",
    "    path=None,\n",
    "    instance_types_dicts=None,\n",
    "    mode=\"sqrt\",\n",
    "):\n",
    "    if columns is None:\n",
    "        columns = rows\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    df = pd.DataFrame(matrix, columns=columns, index=rows)\n",
    "    filepath = os.path.join(path, name)\n",
    "    df.to_csv(filepath + \".csv\", sep=config.csv_separator, decimal=config.csv_decimal)\n",
    "    path = config.get_output_path(path, visualization=True)\n",
    "    if config.visualize:\n",
    "        if instance_types_dicts:\n",
    "            sankey(\n",
    "                config, matrix, rows, instance_types_dicts, name + \"_sankey\", path=path\n",
    "            )\n",
    "            if mode:\n",
    "                visualize_matrix_graph(\n",
    "                    config,\n",
    "                    matrix,\n",
    "                    rows,\n",
    "                    instance_types_dicts,\n",
    "                    name + \"_graph\",\n",
    "                    path=path,\n",
    "                    node_size_mode=config.proximity_mode,\n",
    "                )\n",
    "            else:\n",
    "                visualize_matrix_graph(\n",
    "                    config,\n",
    "                    matrix,\n",
    "                    rows,\n",
    "                    instance_types_dicts,\n",
    "                    name + \"_graph\",\n",
    "                    path=path,\n",
    "                )\n",
    "        visualize_matrix(config, matrix, rows, columns, name, path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ontology\n",
    "import json\n",
    "\n",
    "from bnw_tools.extract import util_zotero\n",
    "\n",
    "class KnowledgeGraphEntryFactory:\n",
    "    @staticmethod\n",
    "    def create(label=None, properties=None):\n",
    "        if properties:\n",
    "            if \"instance_of\" in properties:\n",
    "                return Instance(label=label, properties=properties)\n",
    "            elif \"subclass_of\" in properties:\n",
    "                return InstanceType(label=label, properties=properties)\n",
    "        return KnowledgeGraphEntry(label=label, properties=properties)\n",
    "    \n",
    "class KnowledgeGraphEntry:\n",
    "    def __init__(self, label=None, properties={}):\n",
    "        self.label = label\n",
    "\n",
    "        self.set_properties(properties)\n",
    "\n",
    "    def set_properties(self, properties, overwrite=False):\n",
    "        mapping = {\n",
    "            \"name\": \"label\",\n",
    "            \"instance_type\": \"instance_of\",\n",
    "            \"also_known_as\": \"aliases\",\n",
    "            \"uri\": \"wikidata_uri\",\n",
    "        }\n",
    "\n",
    "        for prop, value in properties.items():\n",
    "            if prop in mapping:\n",
    "                prop = mapping[prop]\n",
    "            if prop == \"wikidata_uri\" and isinstance(value, list):\n",
    "                prop = \"wikidata_candidates\"\n",
    "            # if not overwrite and hasattr(self, prop) and getattr(self, prop):\n",
    "            if hasattr(self, prop):\n",
    "                if not value:\n",
    "                    continue\n",
    "                current = getattr(self, prop)\n",
    "                if current == value:\n",
    "                    continue\n",
    "                if isinstance(current, list):\n",
    "                    if not isinstance(value, list):\n",
    "                        value = [value]\n",
    "                    for v in value:\n",
    "                        if v not in current:\n",
    "                            current.append(v)\n",
    "                    continue\n",
    "                elif current and not overwrite:\n",
    "                    continue\n",
    "            setattr(self, prop, value)\n",
    "\n",
    "    def get(self, label, default=None):\n",
    "        if hasattr(self, label):\n",
    "            return getattr(self, label)\n",
    "        return default\n",
    "\n",
    "    def update(self, other: KnowledgeGraphEntry, force=False):\n",
    "        was_updated = False\n",
    "        if self.label and other.label and self.label != other.label:\n",
    "            return False\n",
    "        for prop, value in other.__dict__.items():\n",
    "            if not value:\n",
    "                continue\n",
    "            if not hasattr(self, prop) or not getattr(self, prop) or force:\n",
    "                setattr(self, prop, value)\n",
    "                was_updated = True\n",
    "        return was_updated\n",
    "\n",
    "\n",
    "# Class,URI,Label,Laymans Term,Status\n",
    "## e.g. process,software,data item,data model,data format specification,\n",
    "class InstanceType(KnowledgeGraphEntry):  # Class\n",
    "\n",
    "    ## Will be shifted to the individual class instances\n",
    "    # properties = {\n",
    "    #     \"default\": [\n",
    "    #         \"name\",\n",
    "    #         \"instance_type\",\n",
    "    #         \"uri\",\n",
    "    #         \"also_known_as\",\n",
    "    #     ],\n",
    "    #     \"software\": [\n",
    "    #         \"data visualization\",\n",
    "    #         \"data validation\",\n",
    "    #         \"data reasoning\",\n",
    "    #     ],\n",
    "    #     \"format\": [\"neutral\"],\n",
    "    # }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        label=None,\n",
    "        properties={},\n",
    "        instance_properties={},\n",
    "    ):\n",
    "        self.label: str = label\n",
    "        self.source: str = None\n",
    "        self.subclass_of: list[str] = []\n",
    "        self.aliases: list[str] = []\n",
    "        self.tags: list[str] = []\n",
    "        self.wikidata_uri: str = None\n",
    "        self.orkg_uri: str = None\n",
    "        self.wikidata_candidates: list[str] = []\n",
    "        self.orkg_candidates: list[str] = []\n",
    "        self.instance_properties: dict = instance_properties\n",
    "\n",
    "        self.set_properties(properties)\n",
    "\n",
    "\n",
    "class Instance(KnowledgeGraphEntry):\n",
    "    def __init__(\n",
    "        self, label=None, instance_type=None, properties={}, properties_from_class={}\n",
    "    ):\n",
    "        self.label: str = label\n",
    "        self.source: str = None\n",
    "        self.instance_of: list[str] = (\n",
    "            []\n",
    "            if not instance_type\n",
    "            else [instance_type] if isinstance(instance_type, str) else instance_type\n",
    "        )\n",
    "        self.aliases: list[str] = []\n",
    "        self.tags: list[str] = []\n",
    "        self.wikidata_uri: str = None\n",
    "        self.orkg_uri: str = None\n",
    "        self.wikidata_candidates: list[str] = []\n",
    "        self.orkg_candidates: list[str] = []\n",
    "\n",
    "        self.set_properties(properties, properties_from_class)\n",
    "\n",
    "    def set_properties(self, properties, properties_from_class={}, overwrite=False):\n",
    "        candidates = properties_from_class\n",
    "        for candidate in candidates:\n",
    "            if candidate not in properties:\n",
    "                properties[candidate] = None\n",
    "\n",
    "        super().set_properties(properties, overwrite)\n",
    "\n",
    "\n",
    "class Ontology:\n",
    "    def __init__(self):\n",
    "        self.classes: dict[str, InstanceType] = {}\n",
    "        self.instances: dict[str, Instance] = {}\n",
    "        self.relations: dict[str, dict[str, str]] = {}\n",
    "        ## TODO: Consider making papers a subclass of instances\n",
    "\n",
    "        self.instances_by_class = {}\n",
    "\n",
    "        # if instance_types_dicts:\n",
    "        #     self.add_instance_types(instance_types_dicts)\n",
    "\n",
    "    def reorder_classes(self, order):\n",
    "        reordered_classes = {k: self.classes[k] for k in order}\n",
    "        for k in self.classes:\n",
    "            if k not in order:\n",
    "                reordered_classes[k] = self.classes[k]\n",
    "        self.classes = reordered_classes\n",
    "        self.instances_by_class = {k: self.instances_by_class[k] for k in self.classes}\n",
    "\n",
    "    def add_instances(self, instances):\n",
    "        if isinstance(instances, list) or isinstance(instances, set):\n",
    "            # simple list of instances\n",
    "            for instance in instances:\n",
    "                self.add_instance(instance=instance)\n",
    "\n",
    "        elif isinstance(instances, dict):\n",
    "            for key, value in instances.items():\n",
    "                if isinstance(value, Instance):\n",
    "                    # Dict is label: instance\n",
    "                    self.add_instance(instance=value, instance_label=key)\n",
    "\n",
    "                elif isinstance(value, list) or isinstance(value, set):\n",
    "                    # Dict is class : [instance]\n",
    "                    for instance in value:\n",
    "                        self.add_instance(instance=instance, instance_type=key)\n",
    "\n",
    "                elif isinstance(value, dict):\n",
    "                    # Dict is class : {label: instance}\n",
    "                    for label, instance in value.items():\n",
    "                        # Dict is class : {label: instance}\n",
    "                        self.add_instance(\n",
    "                            instance=instance, instance_label=label, instance_type=key\n",
    "                        )\n",
    "\n",
    "    def add_class(\n",
    "        self, instance_type: InstanceType, label=None, properties=None, instance_properties=None, force=False\n",
    "    ):\n",
    "        if isinstance(instance_type, str):\n",
    "            # instance is just a label\n",
    "            label = instance_type\n",
    "            instance = InstanceType(label, properties=properties, instance_properties=instance_properties)\n",
    "\n",
    "        elif isinstance(instance_type, dict):\n",
    "            # instance is still a dict\n",
    "            properties = instance_type\n",
    "            label = properties.get(\"label\", label)\n",
    "            instance = InstanceType(label, properties=properties, instance_properties=instance_properties)\n",
    "\n",
    "        if not label:\n",
    "            label = instance_type.label\n",
    "        # if not subclasses:\n",
    "        #     subclasses = instance_type.subclass_of\n",
    "\n",
    "        if label in self.classes:\n",
    "            was_added = self.classes[label].update(instance_type, force=force)\n",
    "            if not was_added:\n",
    "                return False\n",
    "        else:\n",
    "            self.classes[label] = instance_type\n",
    "        return True\n",
    "\n",
    "    def add_instance(\n",
    "        self, instance: Instance, instance_label=None, instance_type=None, force=False\n",
    "    ):\n",
    "        if isinstance(instance, str):\n",
    "            # instance is just a label\n",
    "            instance_label = instance\n",
    "            instance = Instance(instance_label, instance_type)\n",
    "\n",
    "        elif isinstance(instance, dict):\n",
    "            # instance is still a dict\n",
    "            properties = instance\n",
    "            instance_label = properties.get(\"label\", instance_label)\n",
    "            instance_type = properties.get(\"instance_of\", instance_type)\n",
    "            instance_properties = self.get_properties_from_class(instance_type)\n",
    "            instance = Instance(\n",
    "                instance_label,\n",
    "                instance_type,\n",
    "                properties=properties,\n",
    "                properties_from_class=instance_properties,\n",
    "            )\n",
    "\n",
    "        if not instance_label:\n",
    "            instance_label = instance.label\n",
    "        if not instance_type:\n",
    "            instance_type = instance.instance_of\n",
    "\n",
    "        instance_label = instance_label.strip()\n",
    "\n",
    "        if instance_label in self.instances:\n",
    "            was_added = self.instances[instance_label].update(instance, force=force)\n",
    "            if not was_added:\n",
    "                return False\n",
    "        else:\n",
    "            self.instances[instance_label] = instance\n",
    "        self.update_maps(instance_label, instance_type)\n",
    "        return True\n",
    "\n",
    "    def update_maps(self, instance_label, instance_type):\n",
    "        # self.instances: dict[str, Instance] = {}\n",
    "        if isinstance(instance_type, list):\n",
    "            for it in instance_type:\n",
    "                self.update_maps(instance_label, it)\n",
    "            return\n",
    "        if instance_type not in self.classes:\n",
    "            self.classes[instance_type] = InstanceType(instance_type)\n",
    "\n",
    "        # self.instances_by_class = {}\n",
    "        if instance_type not in self.instances_by_class:\n",
    "            self.instances_by_class[instance_type] = {}\n",
    "        self.instances_by_class[instance_type][instance_label] = self.instances[\n",
    "            instance_label\n",
    "        ]\n",
    "\n",
    "    def get_properties_from_class(self, instance_type):\n",
    "        if instance_type in self.classes:\n",
    "            return self.classes[instance_type].instance_properties\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    # def add_instance_types(self, instance_types_dicts):\n",
    "    #     for instance_type, instances in instance_types_dicts.items():\n",
    "    #         if instance_type not in self.classes:\n",
    "    #             self.classes[instance_type] = InstanceType(instance_type)\n",
    "    #         for instance_label in instances:\n",
    "    #             if instance_label not in self.instances:\n",
    "    #                 instance_properties = self.get_properties_from_class(instance_type)\n",
    "    #                 instance = Instance(\n",
    "    #                     instance_label,\n",
    "    #                     instance_type,\n",
    "    #                     properties_from_class=instance_properties,\n",
    "    #                 )\n",
    "    #                 self.add_instance(instance, instance_label, instance_type)\n",
    "\n",
    "    def get(self, label):\n",
    "        if hasattr(self, label):\n",
    "            if label == \"instances_by_class\":\n",
    "                self.update_instance_by_class()\n",
    "            return getattr(self, label)\n",
    "        elif label in self.instances:\n",
    "            return self.instances[label]\n",
    "        elif label in self.classes:\n",
    "            return self.classes[label]\n",
    "        return None\n",
    "\n",
    "    def update_instance_by_class(self):\n",
    "        UNKNOWNTYPE = \"unknown_instance_type\"\n",
    "        for label, instance in self.instances.items():\n",
    "            instance_types = instance.instance_of\n",
    "            if not isinstance(instance_types, list):\n",
    "                instance_types = [instance_types]\n",
    "            for instance_type in instance_types:\n",
    "                if instance_type not in self.instances_by_class:\n",
    "                    self.instances_by_class[instance_type] = {}\n",
    "                self.instances_by_class[instance_type][label] = instance\n",
    "            if not instance_types:\n",
    "                if UNKNOWNTYPE not in self.instances_by_class:\n",
    "                    self.instances_by_class[UNKNOWNTYPE] = {}\n",
    "                self.instances_by_class[UNKNOWNTYPE][label] = instance\n",
    "        if UNKNOWNTYPE in self.instances_by_class:\n",
    "            Warning(\n",
    "                f\"Instances without instance_of: {len(self.instances_by_class[UNKNOWNTYPE])}\"\n",
    "            )\n",
    "\n",
    "    def get_instances_of_type(self, instance_type):\n",
    "        if self.instances_by_class:\n",
    "            if instance_type in self.instances_by_class:\n",
    "                return self.instances_by_class[instance_type]\n",
    "        return {}\n",
    "        # updated = False\n",
    "        # while True:\n",
    "        #     if self.instances_by_class:\n",
    "        #         if instance_type in self.instances_by_class:\n",
    "        #             return self.instances_by_class[instance_type]\n",
    "        #     if not updated:\n",
    "        #         self.update_instance_by_types()\n",
    "        #         updated = True\n",
    "        #     else:\n",
    "        #         return {}\n",
    "\n",
    "    def sort(self):\n",
    "        # FIXME: This is not working\n",
    "        Warning(\"This function is not working\")\n",
    "        pass\n",
    "        # sort according to the following order:\n",
    "        # first: all instances that have wikidata_uri and orkg_uri\n",
    "        # then: all instances that have wikidata_uri\n",
    "        # then: all instances that have orkg_uri\n",
    "        # then: all instances that have wikidata_candidates\n",
    "        # then: all instances that have orkg_candidates\n",
    "        # self.instances = {\n",
    "        #     k: self.instances[k]\n",
    "        #     for k in sorted(\n",
    "        #         self.instances,\n",
    "        #         key=lambda x: (\n",
    "        #             0 if x[1].wikidata_uri and x[1].orkg_uri else 1,\n",
    "        #             0 if x[1].wikidata_uri else 1,\n",
    "        #             0 if x[1].orkg_uri else 1,\n",
    "        #             0 if x[1].wikidata_candidates else 1,\n",
    "        #             0 if x[1].orkg_candidates else 1,\n",
    "        #         ),\n",
    "        #     )\n",
    "        # }\n",
    "\n",
    "        # self.instances = {\n",
    "        #     k: self.instances[k]\n",
    "        #     for k in sorted(\n",
    "        #         self.instances,\n",
    "        #         key=lambda x: (\n",
    "        #             0\n",
    "        #             if isinstance(self.instances[x].wikidata_uri, str)\n",
    "        #             else (\n",
    "        #                 len(self.instances[x].wikidata_uri)\n",
    "        #                 if isinstance(self.instances[x].wikidata_uri, list)\n",
    "        #                 else 99999\n",
    "        #             )\n",
    "        #             # len(self.instances[x].uri) if isinstance(self.instances[x].uri, list) else 0\n",
    "        #         ),\n",
    "        #     )\n",
    "        # }\n",
    "\n",
    "    def save(self, config: Config=None, path=None, name=\"instances.csv\", sort=True):\n",
    "        # FIXME: Needs to be completely reworked\n",
    "        Warning(\"This function is not working\")\n",
    "        pass\n",
    "        # if not path:\n",
    "        #     path = config.ontology_path\n",
    "        # if not name.endswith(\".csv\"):\n",
    "        #     name += \".csv\"\n",
    "        # if sort:\n",
    "        #     self.sort()\n",
    "        # filepath = os.path.join(path, name)\n",
    "        # # TODO: make a way that this is generated dynamically\n",
    "        # cols = [name for sublist in Instance.properties.values() for name in sublist]\n",
    "        # header = config.csv_separator.join(cols)\n",
    "\n",
    "        # # backup\n",
    "        # if os.path.exists(filepath):\n",
    "        #     try:\n",
    "        #         temp = Ontology()\n",
    "        #         temp.load(config, path, name)\n",
    "        #         # current file is valid, so backup\n",
    "        #         backup_path = filepath.replace(\".csv\", \"_backup.csv\")\n",
    "        #         if os.path.exists(backup_path):\n",
    "        #             os.remove(backup_path)\n",
    "        #         os.rename(filepath, backup_path)\n",
    "        #     except:\n",
    "        #         pass\n",
    "\n",
    "        # with open(filepath, \"w\", encoding=\"utf8\") as f:\n",
    "        #     f.write(header + \"\\n\")\n",
    "        #     for instance in self.instances.values():\n",
    "        #         line = []\n",
    "        #         for col in cols:\n",
    "        #             if hasattr(instance, col):\n",
    "        #                 entry = getattr(instance, col) or \"\"\n",
    "        #                 while isinstance(entry, dict) and list(entry.keys()) == [col]:\n",
    "        #                     entry = entry[col]\n",
    "        #                 if isinstance(entry, dict):\n",
    "        #                     entry = json.dumps(entry)\n",
    "        #                 elif isinstance(entry, list):\n",
    "        #                     entry = json.dumps(entry)\n",
    "        #                 elif isinstance(entry, int):\n",
    "        #                     entry = str(entry)\n",
    "        #                 if config.csv_separator in entry:\n",
    "        #                     entry = f'\"{entry}\"'\n",
    "        #                 line.append(entry)\n",
    "        #             else:\n",
    "        #                 line.append(\"\")\n",
    "        #         f.write(config.csv_separator.join(line) + \"\\n\")\n",
    "\n",
    "    def load(self, config, path=None, name=\"instances.csv\", try_backup=True):\n",
    "        if not path:\n",
    "            path = config.ontology_path\n",
    "        if not name.endswith(\".csv\"):\n",
    "            name += \".csv\"\n",
    "        filepath = os.path.join(path, name)\n",
    "\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "                lines = f.read().splitlines()\n",
    "            cols = lines[0].split(config.csv_separator)\n",
    "            for line in lines[1:]:\n",
    "                data = line.split(config.csv_separator)\n",
    "                for i in range(len(data)):\n",
    "                    if i == len(data):\n",
    "                        break\n",
    "                    while (\n",
    "                        data[i].count('\"') % 2 == 1\n",
    "                        or data[i].count(\"[\") != data[i].count(\"]\")\n",
    "                        or data[i].count(\"{\") != data[i].count(\"}\")\n",
    "                    ):\n",
    "                        data[i] += \",\" + data.pop(i + 1)\n",
    "                    # has separator\n",
    "                    if data[i].startswith('\"') and data[i].endswith('\"'):\n",
    "                        data[i] = data[i][1:-1]\n",
    "                    # json\n",
    "                    if data[i].startswith(\"{\") and data[i].endswith(\"}\"):\n",
    "                        data[i] = json.loads(data[i])\n",
    "                    # list\n",
    "                    if data[i].startswith(\"[\") and data[i].endswith(\"]\"):\n",
    "                        data[i] = json.loads(data[i])\n",
    "                    # int\n",
    "                    if isinstance(data[i], str) and data[i] == \"-1\":\n",
    "                        # TODO: Make this more general\n",
    "                        data[i] = int(data[i])\n",
    "                if len(data) != len(cols):\n",
    "                    raise Exception(f\"Error: Line {line} has too few columns.\")\n",
    "                instance = Instance(\n",
    "                    # data[0],\n",
    "                    # data[1],\n",
    "                    properties={cols[i]: data[i] for i in range(len(data)) if data[i]},\n",
    "                    properties_from_class=self.get_properties_from_class(data[1]),\n",
    "                )\n",
    "                self.add_instance(instance)\n",
    "        except Exception as e:\n",
    "            backup_name = name.replace(\".csv\", \"_backup.csv\")\n",
    "            if try_backup and os.path.exists(os.path.join(path, backup_name)):\n",
    "                self.load(config, path, backup_name, try_backup=False)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    @time_function\n",
    "    def get_metadata(self, label, path=None, return_results=False):\n",
    "        # papers_metadata = {}\n",
    "        if label in self.classes:\n",
    "            candidates = self.get_instances_of_type(label)\n",
    "        elif label in self.instances:\n",
    "            candidates = {label: self.instances[label]}\n",
    "        else:\n",
    "            print(f\"{label} not found in instances or classes.\")\n",
    "            return {}\n",
    "\n",
    "        bib_resources = util_zotero.BibResources(path) if path else None\n",
    "\n",
    "        counter = 0\n",
    "        for instance_label, instance in candidates.items():\n",
    "            for entry in bib_resources.entries:\n",
    "                if (\n",
    "                    hasattr(bib_resources.entries[entry], \"file\")\n",
    "                    and instance_label in bib_resources.entries[entry].file\n",
    "                ):\n",
    "                    instance.set_properties(bib_resources.entries[entry].get_dict())\n",
    "                    counter += 1\n",
    "                    del bib_resources.entries[entry]\n",
    "                    break\n",
    "        if len(candidates) > 1:\n",
    "            print(\n",
    "                f\"{counter} out of {len(candidates)} {label} instances have metadata.\"\n",
    "            )\n",
    "        if return_results:\n",
    "            return candidates\n",
    "\n",
    "\n",
    "def curate_instances(instances, path=None):\n",
    "    if not path:\n",
    "        path = \"whitelist.csv\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "            whitelist = f.read().splitlines()\n",
    "\n",
    "    for instance in instances:\n",
    "        if instance not in whitelist:\n",
    "            instances.remove(instance)\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Director:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        # self.ontology = Ontology()\n",
    "        # self.ontology.load(config)\n",
    "\n",
    "        self.builder:dict[str, Builder] = {}\n",
    "        self.products = {}\n",
    "\n",
    "    def set(self, key, value):\n",
    "        setattr(self, key, value)\n",
    "\n",
    "    def get(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def run(self):\n",
    "        # self.extract_papers()\n",
    "        pass\n",
    "\n",
    "\n",
    "class PaperInstanceDirector(Director):\n",
    "    def __init__(self, config: Config, ontology: Ontology = None):\n",
    "        super().__init__(config)\n",
    "        self.ontology: Ontology = None\n",
    "\n",
    "        self.classes: dict[str, InstanceType] = {}\n",
    "        self.papers: dict[str, Instance] = {}\n",
    "        self.instances: dict[str, Instance] = {}  # any non-paper instances\n",
    "\n",
    "        self.included_papers: dict[str, Instance] = {}\n",
    "        self.excluded_papers: dict[str, Instance] = {}\n",
    "\n",
    "        # products\n",
    "        # self.paper_instance_occurrence_matrix: np.ndarray = None\n",
    "\n",
    "        if ontology:\n",
    "            self.link_ontology(ontology)\n",
    "        else:\n",
    "            self.ontology = Ontology()\n",
    "            self.ontology.load(config)\n",
    "            self.link_ontology(self.ontology)\n",
    "\n",
    "    def link_ontology(self, ontology: Ontology):\n",
    "        self.ontology = ontology\n",
    "\n",
    "        # self.classes = {k: v for k, v in ontology.classes.items() if k not in [\"paper\"]}\n",
    "        self.classes = {k: v for k, v in ontology.classes.items()}\n",
    "\n",
    "        for type, dict_ in ontology.get(\"instances_by_class\").items():\n",
    "            if type == \"paper\":\n",
    "                self.papers.update(dict_)\n",
    "            else:\n",
    "                self.instances.update(dict_)\n",
    "        \n",
    "        self.reduce_to_reviewed_papers()\n",
    "        \n",
    "\n",
    "    def get_papers(self):\n",
    "        papers = {}\n",
    "        for file in os.listdir(self.config.papers_path):\n",
    "            if file.endswith(\".json\"):\n",
    "                papers[file[:-5]] = {\n",
    "                    \"nlp_path\": path_cleaning(os.path.join(self.config.papers_path, file))\n",
    "                }\n",
    "\n",
    "        self.ontology.add_instances({\"paper\": papers})\n",
    "        self.ontology.get_metadata(\"paper\", config.folder_path)\n",
    "\n",
    "    def reduce_to_reviewed_papers(self):\n",
    "        review_path = self.config.review_path\n",
    "        # todo: sort by review score + average rank\n",
    "        ## TODO: Make this a function that imports more data from the reivew files\n",
    "        included_identifier = {\n",
    "            3: \"review_score:: 3\",\n",
    "            4: \"review_score:: 4\",\n",
    "            5: \"review_score:: 5\",\n",
    "        }\n",
    "        excluded_identifier = {\n",
    "            2: \"review_score:: 2\",\n",
    "            1: \"review_score:: 1\",\n",
    "            0: \"review_score:: 0\",\n",
    "        }\n",
    "        for file in os.listdir(review_path):\n",
    "            if file.endswith(\".md\"):\n",
    "                paper_name = file[:-3]\n",
    "                if paper_name in self.papers:\n",
    "                    paper = self.papers[paper_name]\n",
    "                    if paper_name in self.included_papers or paper_name in self.excluded_papers:\n",
    "                        continue\n",
    "                    # check if file contains \"reviewed\"ArithmeticError\n",
    "                    with open(\n",
    "                        os.path.join(review_path, file), \"r\", encoding=\"utf8\"\n",
    "                    ) as f:\n",
    "                        content = f.read()\n",
    "                        for score, text in included_identifier.items():\n",
    "                            if text in content:\n",
    "                                paper.review_score = score\n",
    "                                self.included_papers[paper_name] = paper\n",
    "                                break\n",
    "                        for score, text in excluded_identifier.items():\n",
    "                            if text in content:\n",
    "                                self.excluded_papers[paper_name] = paper\n",
    "                                break\n",
    "        if self.config.only_included_papers:\n",
    "            self.papers = {k:v for k, v in self.included_papers.items()}\n",
    "        self.sort_papers()\n",
    "        if self.included_papers:\n",
    "            self.included_papers = self.sort_papers(self.included_papers)\n",
    "        if self.excluded_papers:\n",
    "            self.excluded_papers = self.sort_papers(self.excluded_papers)\n",
    "\n",
    "    def sort_papers(self, papers=None):\n",
    "        saveback = False\n",
    "        if not papers:\n",
    "            saveback = True\n",
    "        papers = papers or self.papers\n",
    "        # TODO: see if \"if\" and \"else\" can be deleted\n",
    "        papers = {x: papers[x] for x in sorted(\n",
    "            papers,\n",
    "            key=lambda x: (\n",
    "                getattr(papers[x], \"year\", \"9999\")\n",
    "                if hasattr(papers[x], \"year\") else \"9999\"\n",
    "            )\n",
    "        )}\n",
    "        if saveback:\n",
    "            self.papers = papers\n",
    "        else:\n",
    "            return papers\n",
    "        # papers_metadata = self.ontology.get_metadata(\"paper\")\n",
    "        # papers = sorted(\n",
    "        #     papers,\n",
    "        #     key=lambda x: (\n",
    "        #         getattr(self.papers[x], \"year\", \"9999\")\n",
    "        #         if x in papers_metadata and \"year\" in papers_metadata[x]\n",
    "        #         else \"9999\"\n",
    "        #     ),\n",
    "        # )\n",
    "\n",
    "    def get_instances(self):\n",
    "        self.builder[\"InstanceBuilder\"] = InstanceBuilder(self)\n",
    "        instances_by_type = self.builder[\"InstanceBuilder\"].build()\n",
    "        self.ontology.add_instances(instances_by_type)\n",
    "        self.ontology.reorder_classes(instances_by_type.keys())\n",
    "        self.ontology.save(self.config)\n",
    "        self.link_ontology(self.ontology)\n",
    "\n",
    "    def update_instance(self, instance, label):\n",
    "        if \"paper\" in getattr(instance, \"instance_of\"):\n",
    "            return False\n",
    "        \n",
    "        if label not in self.instances:\n",
    "            print(f\"Instance {label} will be newly added to instances.\")\n",
    "\n",
    "        was_added = self.ontology.add_instance(instance, label)\n",
    "        if was_added:\n",
    "            if instance.instance_of == \"paper\":\n",
    "                self.papers[label] = instance\n",
    "            else:\n",
    "                self.instances[label] = instance\n",
    "        return was_added\n",
    "    \n",
    "    \n",
    "    def update_class(self, instance_type, label):\n",
    "        was_added = self.ontology.add_class(instance_type, label)\n",
    "        if was_added:\n",
    "            self.classes[label] = instance_type\n",
    "        return was_added\n",
    "            \n",
    "\n",
    "    def sync(self, other):\n",
    "        updates_done = False\n",
    "        attributes = [\"papers\", \"instances\", \"classes\"]\n",
    "        for attribute in attributes:\n",
    "            if hasattr(other, attribute):\n",
    "                for label, instance in getattr(other, attribute).items():\n",
    "                    if isinstance(instance, Instance):\n",
    "                        res = self.update_instance(instance, label)\n",
    "                    elif isinstance(instance, InstanceType):\n",
    "                        res = self.update_class(instance, label)\n",
    "                    if res:\n",
    "                        updates_done = True\n",
    "                setattr(other, attribute, getattr(self, attribute))\n",
    "        if updates_done:\n",
    "            self.ontology.save(self.config)\n",
    "\n",
    "    def build_obsidian_folder(self):\n",
    "        self.builder['ObsidianFolder'] = ObsidianFolderBuilder(self)\n",
    "        self.sync(self.builder['ObsidianFolder'])\n",
    "        self.builder['ObsidianFolder'].build()\n",
    "        \n",
    "    ## Sorting of instances\n",
    "\n",
    "    # def remove_zeros(\n",
    "    #     self, matrix=None, columns=True, rows=True, row_lists=None, column_lists=None\n",
    "    # ):\n",
    "    #     matrix = matrix or self.paper_instance_occurrence_matrix\n",
    "\n",
    "    #     # remove all columns that are all zeros\n",
    "    #     if columns:\n",
    "    #         deleted_columns = np.all(matrix == 0, axis=0)\n",
    "    #         matrix = matrix[:, ~np.all(matrix == 0, axis=0)]\n",
    "\n",
    "    #     # remove all rows that are all zeros\n",
    "    #     if rows:\n",
    "    #         deleted_rows = np.all(matrix == 0, axis=1)\n",
    "    #         matrix = matrix[~np.all(matrix == 0, axis=1)]\n",
    "\n",
    "    #     return matrix, [deleted_columns, deleted_rows]\n",
    "\n",
    "    # def handle_deletions(self, input, deletions, rows=True):\n",
    "    #     \"\"\"\n",
    "    #     input: list, dict or np.ndarray\n",
    "    #     deletions: list of bools\n",
    "    #     rows: if True, deletions[1] is used, else deletions[0]\n",
    "    #     \"\"\"\n",
    "    #     delID = 1 if rows else 0\n",
    "\n",
    "    #     if deletions[delID].any():\n",
    "    #         # rows were deleted, in this case: papers\n",
    "    #         if isinstance(input, list):\n",
    "    #             input = [\n",
    "    #                 item for i, item in enumerate(input) if not deletions[delID][i]\n",
    "    #             ]\n",
    "    #         elif isinstance(input, dict):\n",
    "    #             input = {\n",
    "    #                 key: item\n",
    "    #                 for i, (key, item) in enumerate(input.items())\n",
    "    #                 if not deletions[delID][i]\n",
    "    #             }\n",
    "    #         elif isinstance(input, np.ndarray):\n",
    "    #             input = input[~deletions[delID]]\n",
    "    #     return input\n",
    "\n",
    "    # def reorder_matrix(self, new_order):\n",
    "    #     self.paper_instance_occurrence_matrix = self.paper_instance_occurrence_matrix[\n",
    "    #         :, new_order\n",
    "    #     ]\n",
    "    #     self.paper_instance_occurrence_matrix, deletions = self.remove_zeros()\n",
    "    #     self.papers = self.handle_deletions(self.papers, deletions)\n",
    "\n",
    "    def sort_instances(self):\n",
    "        if self.builder[\"occurrence_matrix\"].matrix is None:\n",
    "            self.setup_occurence_matrix()\n",
    "\n",
    "        indexed_instances = {\n",
    "            instance: i for i, instance in enumerate(self.instances.keys())\n",
    "        }\n",
    "\n",
    "        instance_occurrences = {}\n",
    "\n",
    "        for i, instance_label in enumerate(self.instances.keys()):\n",
    "            instance_occurrences[instance_label] = (\n",
    "                self.builder[\"occurrence_matrix\"].matrix[:, i].sum()\n",
    "            )\n",
    "\n",
    "        sorted_instances = {\n",
    "            k: float(v)\n",
    "            for k, v in sorted(\n",
    "                instance_occurrences.items(), key=lambda item: item[1], reverse=True\n",
    "            )\n",
    "            if v > 0\n",
    "        }\n",
    "\n",
    "        filepath = os.path.join(self.config.get_output_path(), \"instance_occurrences\")\n",
    "        with open(filepath + \".json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(sorted_instances, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        sorted_instance_list = list(sorted_instances.keys())\n",
    "\n",
    "        instance_types_dict = self.ontology.get(\"instances_by_class\")\n",
    "\n",
    "        type_lists = [[] for _ in range(len(instance_types_dict))]\n",
    "        for instance in sorted_instance_list:\n",
    "            for type_ID, instance_type in enumerate(instance_types_dict):\n",
    "                if instance in instance_types_dict[instance_type]:\n",
    "                    type_lists[type_ID].append(instance)\n",
    "        type_sorted_instances = [item for sublist in type_lists for item in sublist]\n",
    "\n",
    "        new_order = [0] * len(sorted_instance_list)\n",
    "        sorted_instances = {}\n",
    "        for i, instance in enumerate(type_sorted_instances):\n",
    "            new_order[i] = indexed_instances[instance]\n",
    "            sorted_instances[instance] = self.instances[instance]\n",
    "\n",
    "        self.instances = sorted_instances\n",
    "\n",
    "        # sort all matrixes accordingly\n",
    "        new_order = np.array(new_order)\n",
    "        \n",
    "        self.builder[\"occurrence_matrix\"].reorder_matrix(new_order)\n",
    "        self.papers = self.builder[\"occurrence_matrix\"].handle_deletions(self.papers)\n",
    "\n",
    "    def setup_occurence_matrix(self):\n",
    "        self.builder[\"occurrence_matrix\"] = OccurrenceMatrixBuilder(self)\n",
    "        self.builder[\"occurrence_matrix\"].build()\n",
    "        # self.paper_instance_occurrence_matrix = self.builder[\"occurrence_matrix\"].matrix\n",
    "        self.sort_instances()\n",
    "\n",
    "    # def run(self):\n",
    "    #     self.extract_papers()\n",
    "    #     self.extract_reviews()\n",
    "    #     self.extract_ontology()\n",
    "    #     self.extract_orkg()\n",
    "    #     self.extract_obsidian()\n",
    "\n",
    "    # def extract_papers(self):\n",
    "    #     # Extract papers from the papers folder\n",
    "    #     pass\n",
    "\n",
    "    # def extract_reviews(self):\n",
    "    #     # Extract reviews from the reviews folder\n",
    "    #     pass\n",
    "\n",
    "    # def extract_ontology(self):\n",
    "    #     # Extract ontology from the ontology folder\n",
    "    #     pass\n",
    "\n",
    "    # def extract_orkg(self):\n",
    "    #     # Extract orkg from the orkg folder\n",
    "    #     pass\n",
    "\n",
    "    # def extract_obsidian(self):\n",
    "    #     # Extract obsidian from the obsidian folder\n",
    "    #     pass\n",
    "\n",
    "\n",
    "director = PaperInstanceDirector(config)\n",
    "# director.set(\"ontology\", ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no new Zotero export at G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR:\n",
      "There should be a folder called 'files'\n",
      "We now have 1035 PDFs stored at G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR\\00_PDFs\n",
      "1024 out of 1028 paper instances have metadata.\n",
      "get_metadata executed in 23.981743574142456 seconds\n"
     ]
    }
   ],
   "source": [
    "# Extract Paper Metadata\n",
    "director.get_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Likely no longer needed\n",
    "# def exclude_papers(paper_nlp_paths, papers, included_papers, only_included_papers):\n",
    "#     if only_included_papers:\n",
    "#         deletions = [False] * len(papers)\n",
    "#         for p_ID, paper in enumerate(paper_nlp_paths):\n",
    "#             if paper not in included_papers:\n",
    "#                 deletions[p_ID] = True\n",
    "#         for p_ID, deletion in enumerate(deletions):\n",
    "#             if deletion:\n",
    "#                 paper_nlp_paths.pop(papers[p_ID])\n",
    "#         papers = included_papers\n",
    "#         print(\n",
    "#             \"Only included papers are considered. Excluded or not reviewed papers are removed.\"\n",
    "#         )\n",
    "\n",
    "#     return paper_nlp_paths, papers\n",
    "\n",
    "\n",
    "director.reduce_to_reviewed_papers()\n",
    "\n",
    "# paper_nlp_paths, papers = exclude_papers(\n",
    "#     paper_nlp_paths,\n",
    "#     papers,\n",
    "#     included_papers,\n",
    "#     only_included_papers=config.only_included_papers,\n",
    "# )\n",
    "\n",
    "\n",
    "# Free memory\n",
    "\n",
    "\n",
    "# del included_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort papers by year\n",
    "# papers = sorted(\n",
    "#     papers,\n",
    "#     key=lambda x: (\n",
    "#         papers_metadata[x][\"year\"]\n",
    "#         if x in papers_metadata and \"year\" in papers_metadata[x]\n",
    "#         else \"9999\"\n",
    "#     ),\n",
    "# )\n",
    "# sort paper_nlp_paths and papers_metadata accordingly\n",
    "# paper_nlp_paths = {k: paper_nlp_paths[k] for k in papers}\n",
    "# papers_metadata = {k: papers_metadata[k] for k in papers if k in papers_metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Builder:\n",
    "    def __init__(self, director: Director, config: Config = None):\n",
    "        self.director: Director = director\n",
    "        self.config = config or director.config\n",
    "\n",
    "    def build(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MatrixBuilder(Builder):\n",
    "    def __init__(self, director: Director):\n",
    "        super().__init__(director)\n",
    "\n",
    "        self.matrix: np.ndarray = None\n",
    "        self.row_labels = [] # papers\n",
    "        self.col_labels = [] # instances\n",
    "\n",
    "        self.deletions = [np.array([]), np.array([])]\n",
    "        # TODO: Likely add more proerties to map from full corpus to reduced corpus\n",
    "\n",
    "    def remove_zeros(self, columns=True, rows=True):\n",
    "        # remove all columns that are all zeros\n",
    "        # setup deleted_columns empty at first:\n",
    "        deleted_columns, deleted_rows = self.deletions\n",
    "        if columns:\n",
    "            deleted_columns = np.all(self.matrix == 0, axis=0)\n",
    "            self.matrix = self.matrix[:, ~np.all(self.matrix == 0, axis=0)]\n",
    "\n",
    "        # remove all rows that are all zeros\n",
    "        if rows:\n",
    "            deleted_rows = np.all(self.matrix == 0, axis=1)\n",
    "            self.matrix = self.matrix[~np.all(self.matrix == 0, axis=1)]\n",
    "        self.deletions = [deleted_columns, deleted_rows]\n",
    "\n",
    "    def handle_deletions(self, input, deletions=None, rows=True):\n",
    "        \"\"\"\n",
    "        input: list, dict or np.ndarray\n",
    "        deletions: list of bools\n",
    "        rows: if True, deletions[1] is used, else deletions[0]\n",
    "        \"\"\"\n",
    "        delID = 1 if rows else 0\n",
    "\n",
    "        if not deletions:\n",
    "            deletions = self.deletions\n",
    "\n",
    "        if deletions[delID].any():\n",
    "            # rows were deleted, in this case: papers\n",
    "            if isinstance(input, list):\n",
    "                input = [\n",
    "                    item for i, item in enumerate(input) if not deletions[delID][i]\n",
    "                ]\n",
    "            elif isinstance(input, dict):\n",
    "                input = {\n",
    "                    key: item\n",
    "                    for i, (key, item) in enumerate(input.items())\n",
    "                    if not deletions[delID][i]\n",
    "                }\n",
    "            elif isinstance(input, np.ndarray):\n",
    "                input = input[~deletions[delID]]\n",
    "        return input\n",
    "\n",
    "    def reorder_matrix(self, new_order, cols=True):\n",
    "        if cols:\n",
    "            self.matrix = self.matrix[:, new_order]\n",
    "        else:\n",
    "            self.matrix = self.matrix[new_order, :]\n",
    "        self.remove_zeros()\n",
    "        # this isn't quite right anymore\n",
    "        # self.papers = self.handle_deletions(self.papers, self.deletions)\n",
    "\n",
    "\n",
    "class OccurrenceMatrixBuilder(MatrixBuilder):\n",
    "    def __init__(self, director: PaperInstanceDirector, papers=None, instances=None):\n",
    "        super().__init__(director)\n",
    "        self.director: PaperInstanceDirector = director\n",
    "\n",
    "        self.papers = papers or self.director.papers  # first dimension\n",
    "        self.instances = instances or self.director.instances  # second dimension\n",
    "\n",
    "    def count_occurrences(self, papers, instances):\n",
    "        papers = papers or self.papers\n",
    "        instances = instances or self.instances\n",
    "\n",
    "        occurrences = np.zeros((len(papers), len(instances)), dtype=int)\n",
    "\n",
    "        for p, paperpath in enumerate(papers.values()):\n",
    "            if isinstance(paperpath, dict) or isinstance(paperpath, Instance):\n",
    "                paperpath = paperpath.get(\"nlp_path\", None)\n",
    "            with open(paperpath, \"r\", encoding=\"utf8\") as f:\n",
    "                paper = json.load(f)\n",
    "                for i, instance in enumerate(instances):\n",
    "                    present = True\n",
    "                    pieces = split_string(instance)\n",
    "                    for piece in pieces:\n",
    "                        if piece.lower() not in paper[\"bag_of_words\"]:\n",
    "                            present = False\n",
    "                            break\n",
    "\n",
    "                    # if instance == \"system integration\":\n",
    "                    #     if \"Liu und Hu - 2013 - A reuse oriented representation model for capturin\" in paperpath:\n",
    "                    #         print(present)\n",
    "                    if present:\n",
    "                        occurrences[p][i] = 1\n",
    "        return occurrences\n",
    "\n",
    "    def build(self, papers=None, instances=None):\n",
    "        self.matrix = self.count_occurrences(papers, instances)\n",
    "        process_matrix(\n",
    "            self.config,\n",
    "            self.matrix,\n",
    "            list(self.papers.keys()),\n",
    "            list(self.instances.keys()),\n",
    "            name=\"paper_instance_occurrence_matrix\",\n",
    "        )\n",
    "        # return self.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file improved, found 0 errors.\n",
      "column data validation is empty\n",
      "column inference is empty\n"
     ]
    }
   ],
   "source": [
    "# Step 1: find occurrences of instances in bag of words of papers\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class InstanceBuilder(Builder):\n",
    "    def __init__(self, director: Director):\n",
    "        super().__init__(director)\n",
    "\n",
    "    def build(self):\n",
    "        instance_types_dicts = {}\n",
    "\n",
    "        # paper_instance_occurrence_matrix = np.zeros((), dtype=int)\n",
    "\n",
    "        # TODO: Delete first 2 lines and see why this throws error then\n",
    "        instance_types_dicts = self.csv_to_dict_of_sets(config.csv_file, config)\n",
    "\n",
    "        # Extract instance types that are actually property types\n",
    "        instance_types_dicts, property_types_dicts = self.prune_properties(\n",
    "            instance_types_dicts, properties_to_prune=self.config.properties\n",
    "        )\n",
    "\n",
    "        return instance_types_dicts\n",
    "\n",
    "    def preprocess_csv(self, csv_file, config: Config, writeback=True):\n",
    "        with open(csv_file, \"r\", encoding=\"utf8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        expected_columns = len(lines[0].split(config.csv_separator))\n",
    "        processed_lines = []\n",
    "        i = 0\n",
    "\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line.count('\"') % 2 == 1:\n",
    "                # if the number of quotes is odd, the line is not complete\n",
    "                error = True\n",
    "                for j in range(i + 1, len(lines)):\n",
    "                    line = line + \" \" + lines[j].strip()\n",
    "                    if line.count('\"') % 2 == 0:\n",
    "                        error = False\n",
    "                        print(f\"Merged rows {i} to {j}\")\n",
    "                        i = j + 1\n",
    "                        break\n",
    "                if error:\n",
    "                    raise Exception(\n",
    "                        f\"Error: Lines {i} to {j-1} could not be processed. Odd number of quotes.\"\n",
    "                    )\n",
    "            else:\n",
    "                i += 1\n",
    "            if '\"\"' in line:\n",
    "                # remove quotes from line\n",
    "                line = line.replace('\"\"', \"\")\n",
    "            if line.count('\"') % 2 == 0:\n",
    "                pos1 = line.find('\"')\n",
    "                while pos1 != -1:\n",
    "                    pos2 = line.find('\"', pos1 + 1)\n",
    "                    if not config.csv_separator in line[pos1:pos2]:\n",
    "                        line = line[pos1:pos2] + line[pos2 + 1 :]\n",
    "                    pos1 = line.find('\"', pos2 + 1)\n",
    "            processed_lines.append(line)\n",
    "        if writeback and len(processed_lines) != len(lines) or lines != processed_lines:\n",
    "            print(\n",
    "                f\"CSV file improved, found {len(lines) - len(processed_lines)} errors.\"\n",
    "            )\n",
    "            with open(csv_file, \"w\", encoding=\"utf8\") as f:\n",
    "                f.write(\"\\n\".join(processed_lines))\n",
    "\n",
    "        return processed_lines\n",
    "\n",
    "    def csv_to_dict_of_sets(self, csv_file, config: Config, prune_nan=True):\n",
    "        dict_of_sets = {}\n",
    "        # try:\n",
    "        #     df = pd.read_csv(csv_file)\n",
    "        # except pd.errors.ParserError:\n",
    "        #     print(\"Error parsing CSV file. Trying again with 'error_bad_lines=False'\")\n",
    "        # TODO: Specify modular separator and decimal here as well\n",
    "\n",
    "        self.preprocess_csv(csv_file, config)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                csv_file,\n",
    "                on_bad_lines=\"warn\",\n",
    "                delimiter=config.csv_separator,\n",
    "                encoding=\"utf-8\",\n",
    "            )\n",
    "        except:\n",
    "            print(\"Error parsing CSV file. Trying again with 'encoding=ISO-8859-1'\")\n",
    "            df = pd.read_csv(\n",
    "                csv_file,\n",
    "                on_bad_lines=\"warn\",\n",
    "                delimiter=config.csv_separator,\n",
    "                encoding=\"ISO-8859-1\",\n",
    "            )\n",
    "        for column in df.columns:\n",
    "            if df[column].isnull().all():\n",
    "                print(f\"column {column} is empty\")\n",
    "                dict_of_sets[column] = set([np.nan])\n",
    "            else:\n",
    "                dict_of_sets[column] = set(df[column].str.lower())\n",
    "            if prune_nan and np.nan in dict_of_sets[column]:\n",
    "                dict_of_sets[column].remove(np.nan)\n",
    "\n",
    "            # Will be added in next version\n",
    "            deletes = []\n",
    "            adds = []\n",
    "            for original_entry in dict_of_sets[column]:\n",
    "                entry = original_entry.strip()\n",
    "                while entry.startswith('\"'):\n",
    "                    entry = entry[1:]\n",
    "                while entry.endswith('\"'):\n",
    "                    entry = entry[:-1]\n",
    "                if original_entry != entry:\n",
    "                    deletes.append(original_entry)\n",
    "                    if entry not in dict_of_sets[column]:\n",
    "                        adds.append(entry)\n",
    "            for entry in deletes:\n",
    "                dict_of_sets[column].remove(entry)\n",
    "            dict_of_sets[column].update(adds)\n",
    "\n",
    "        # saved_column = df['process'] #you can also use df['column_name']\n",
    "        # delete all that exists in two or more columns\n",
    "        for key in dict_of_sets:\n",
    "            for other_key in dict_of_sets:\n",
    "                if key != other_key:\n",
    "                    dict_of_sets[key] = dict_of_sets[key].difference(\n",
    "                        dict_of_sets[other_key]\n",
    "                    )\n",
    "        return dict_of_sets\n",
    "\n",
    "    def prune_properties(\n",
    "        self,\n",
    "        instance_types_dicts,\n",
    "        properties_to_prune=[],\n",
    "        prune_empty=True,\n",
    "        prune_x=True,\n",
    "    ):\n",
    "        properties = {}\n",
    "\n",
    "        # merge \"interchange format\" into \"data format specification\"\n",
    "        if \"interchange format\" in instance_types_dicts:\n",
    "            pruned = []\n",
    "            for key in instance_types_dicts[\"interchange format\"]:\n",
    "                if len(key) > 1:\n",
    "                    instance_types_dicts[\"data format specification\"].add(key)\n",
    "                    pruned.append(key)\n",
    "            for key in pruned:\n",
    "                instance_types_dicts[\"interchange format\"].remove(key)\n",
    "\n",
    "        for instance_type in instance_types_dicts:\n",
    "            prune = False\n",
    "            if instance_type in properties_to_prune:\n",
    "                prune = True\n",
    "            elif prune_empty and len(instance_types_dicts[instance_type]) == 0:\n",
    "                # prune empty sets\n",
    "                prune = True\n",
    "            elif prune_x and len(max(instance_types_dicts[instance_type], key=len)) < 2:\n",
    "                # prune sets with only one character entries\n",
    "                prune = True\n",
    "\n",
    "            if prune:\n",
    "                properties[instance_type] = instance_types_dicts[instance_type]\n",
    "\n",
    "        for instance_type in properties:\n",
    "            instance_types_dicts.pop(instance_type)\n",
    "\n",
    "        return instance_types_dicts, properties\n",
    "\n",
    "    # # ---------------------- Variables ----------------------\n",
    "\n",
    "    # ## instances: A list of all instances, regardless of their type\n",
    "    # # first all type 1, then all type 2, etc.\n",
    "    # # if possible, instance sare ordered by their occurrence\n",
    "\n",
    "    # ## instances_dicts: A dictionary of all different types (columns) of instances\n",
    "    # #\n",
    "    # # types:\n",
    "    # #  - process\n",
    "    # #  - software\n",
    "    # #  - data item\n",
    "    # #  - data model\n",
    "    # #  - data format specification\n",
    "    # #  - interchange format\n",
    "    # #  - source\n",
    "    # #\n",
    "    # # instances_dicts['process']: A set of all instances of the type 'process'\n",
    "    # #\n",
    "\n",
    "    # instance_types_dicts = {}\n",
    "\n",
    "    # ## paper_nlp_dict: A dictionary of all papers and their NLP data (as dict)\n",
    "\n",
    "    # ## occurrences: A matrix of binary occurrences of instances in papers\n",
    "    # #\n",
    "    # # rows: papers\n",
    "    # # columns: instances\n",
    "    # # cells: 1 if instance is present in paper, 0 otherwise\n",
    "    # #\n",
    "    # paper_instance_occurrence_matrix = np.zeros((), dtype=int)\n",
    "\n",
    "    # # ---------------------- Main ----------------------\n",
    "\n",
    "    # # Usage example\n",
    "\n",
    "    # # TODO: Delete first 2 lines and see why this throws error then\n",
    "    # instance_types_dicts = csv_to_dict_of_sets(config.csv_file, config)\n",
    "\n",
    "    # # Extract instance types that are actually property types\n",
    "    # instance_types_dicts, property_types_dicts = prune_properties(\n",
    "    #     instance_types_dicts, properties_to_prune=config.properties\n",
    "    # )\n",
    "\n",
    "    # def get_instances_list(instance_types_dicts):\n",
    "    #     instances = []\n",
    "    #     # merge all sets into one set\n",
    "    #     for instance_type in instance_types_dicts:\n",
    "    #         instances += instance_types_dicts[instance_type]\n",
    "    #     return instances\n",
    "\n",
    "    # # instances = get_instances_list(instance_types_dicts)\n",
    "\n",
    "\n",
    "director.get_instances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status: Instances and Papers setup\n",
    "\n",
    "## Next: Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "class WikiData:\n",
    "    queries_done = 0\n",
    "    new_labels = 0\n",
    "    new_entries = 0\n",
    "\n",
    "    def print_updates():\n",
    "        print(f\"Queries done: {WikiData.queries_done}\")\n",
    "        print(f\"New labels: {WikiData.new_labels}\")\n",
    "        print(f\"New entries: {WikiData.new_entries}\")\n",
    "\n",
    "    def __init__(self, config: Config = None):\n",
    "        self.entries = {}\n",
    "        self.label_entry_map = {}\n",
    "        if config:\n",
    "            self.load(config)\n",
    "\n",
    "    def save(self, config, path=None, name=\"wikidata.json\"):\n",
    "        if not path:\n",
    "            path = config.ontology_path\n",
    "        if not name.endswith(\".json\"):\n",
    "            name += \".json\"\n",
    "        filepath = os.path.join(path, name)\n",
    "        with open(filepath, \"w\", encoding=\"utf8\") as f:\n",
    "            data = self.__dict__\n",
    "            json.dump(data, f)\n",
    "\n",
    "    def load(self, config, path=None, name=\"wikidata.json\"):\n",
    "        if not path:\n",
    "            path = config.ontology_path\n",
    "        if not name.endswith(\".json\"):\n",
    "            name += \".json\"\n",
    "        filepath = os.path.join(path, name)\n",
    "        if not os.path.exists(filepath):\n",
    "            return\n",
    "        with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "            for key, value in data.items():\n",
    "                setattr(self, key, value)\n",
    "\n",
    "    def query_wikidata(\n",
    "        self, config: Config, label: str, select=\"label\", limit=None, nested=False\n",
    "    ):\n",
    "        if select == \"label\" and not nested:\n",
    "            if WikiData.queries_done > config.wikidata_query_limit:\n",
    "                print(\"Wikidata query limit reached.\")\n",
    "                WikiData.print_updates()\n",
    "                return False\n",
    "            else:\n",
    "                WikiData.queries_done += 1\n",
    "\n",
    "        def transform_results(results):\n",
    "            transformed = {}\n",
    "            for result in results:\n",
    "                item_uri = result[\"item\"][\"value\"]\n",
    "                item_label = result[\"itemLabel\"][\"value\"]\n",
    "                # Handle cases where altLabels or description might not be present\n",
    "                alt_labels = result.get(\"altLabels\", {}).get(\"value\", \"\")\n",
    "                description = result.get(\"description\", {}).get(\"value\", \"\")\n",
    "\n",
    "                transformed[item_uri] = {\n",
    "                    \"itemLabel\": item_label,\n",
    "                    \"altLabels\": alt_labels,\n",
    "                    \"description\": description,  # Include this line only if descriptions are desired\n",
    "                }\n",
    "            return transformed\n",
    "\n",
    "        if not limit:\n",
    "            limit = config.wikidata_query_limit\n",
    "        endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "        selection = {\n",
    "            \"label\": f'?item rdfs:label \"{label}\"@en.',\n",
    "            \"altLabel\": f'?item skos:altLabel \"{label}\"@en.',\n",
    "        }\n",
    "\n",
    "        # query = f\"\"\"\n",
    "        # SELECT ?item ?itemLabel (GROUP_CONCAT(DISTINCT ?altLabel; separator = \", \") AS ?altLabels) WHERE {{\n",
    "        # {selection[select]}\n",
    "        # SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        # }}\n",
    "        # GROUP BY ?item ?itemLabel\n",
    "        # LIMIT {limit}\n",
    "        # \"\"\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT ?item ?itemLabel (GROUP_CONCAT(DISTINCT ?altLabel; separator = \", \") AS ?altLabels) \n",
    "        (SAMPLE(?description) AS ?description) WHERE {{\n",
    "        {selection[select]}\n",
    "        OPTIONAL {{ ?item skos:altLabel ?altLabel FILTER(LANG(?altLabel) = \"en\") }}\n",
    "        OPTIONAL {{ ?item schema:description ?description FILTER(LANG(?description) = \"en\") }}\n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        GROUP BY ?item ?itemLabel\n",
    "        LIMIT {limit}\n",
    "        \"\"\"\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": \"WDQS-example Python/%s.%s\"\n",
    "            % (requests.__version__, \"MyScript\"),\n",
    "            \"Accept\": \"application/json\",\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                endpoint_url, headers=headers, params={\"query\": query, \"format\": \"json\"}\n",
    "            )\n",
    "            response.raise_for_status()  # Raises stored HTTPError, if one occurred\n",
    "\n",
    "            data = response.json()\n",
    "            results = data[\"results\"][\"bindings\"]\n",
    "            results = transform_results(results)\n",
    "            if select == \"label\":\n",
    "                if len(results) < limit:\n",
    "                    results_altLabel = self.query_wikidata(\n",
    "                        config,\n",
    "                        label,\n",
    "                        select=\"altLabel\",\n",
    "                        limit=limit - len(results),\n",
    "                        nested=True,\n",
    "                    )\n",
    "                    results.update(results_altLabel)\n",
    "                if not nested:\n",
    "                    if len(results) < limit and label.lower() != label:\n",
    "                        results_lower = self.query_wikidata(\n",
    "                            config,\n",
    "                            label.lower(),\n",
    "                            select=select,\n",
    "                            limit=limit - len(results),\n",
    "                            nested=True,\n",
    "                        )\n",
    "                        results.update(results_lower)\n",
    "                    if len(results) < limit and label.capitalize() != label:\n",
    "                        results_capitalize = self.query_wikidata(\n",
    "                            config,\n",
    "                            label.capitalize(),\n",
    "                            select=select,\n",
    "                            limit=limit - len(results),\n",
    "                            nested=True,\n",
    "                        )\n",
    "                        results.update(results_capitalize)\n",
    "                    if len(results) < limit and label.upper() != label:\n",
    "                        results_upper = self.query_wikidata(\n",
    "                            config,\n",
    "                            label.upper(),\n",
    "                            select=select,\n",
    "                            limit=limit - len(results),\n",
    "                            nested=True,\n",
    "                        )\n",
    "                        results.update(results_upper)\n",
    "\n",
    "                    wikidata.label_entry_map[label] = list(results.keys())\n",
    "                    WikiData.new_labels += 1\n",
    "                    for key, value in results.items():\n",
    "                        if key not in wikidata.entries:\n",
    "                            wikidata.entries[key] = value\n",
    "                            WikiData.new_entries += 1\n",
    "            if results:\n",
    "                return results\n",
    "            else:\n",
    "                # print(\"No matching Wikidata entry found.\")\n",
    "                return []\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Query failed: {e}\")\n",
    "\n",
    "    def get_uri(self, config: Config, label: str, allow_query=True):\n",
    "        if label in self.label_entry_map:\n",
    "            return self.label_entry_map[label]\n",
    "        elif allow_query:\n",
    "            res = self.query_wikidata(config, label)\n",
    "            if res == False:\n",
    "                print(\"Could not get URI. Query limit reached.\")\n",
    "                return False\n",
    "            if res:\n",
    "                self.save(config)\n",
    "            return self.label_entry_map[label]\n",
    "\n",
    "\n",
    "wikidata = WikiData(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time_function\n",
    "def query_wikidata_for_instances(\n",
    "    config: Config, ontology: Ontology, wikidata: WikiData, stop_at=None\n",
    "):\n",
    "    for instance in ontology.instances.values():\n",
    "        if instance.wikidata_uri:\n",
    "            continue\n",
    "        elif instance.wikidata_candidates:\n",
    "            continue\n",
    "\n",
    "        temp_res = []\n",
    "        check = [instance.label] + getattr(instance, \"also_known_as\", [])\n",
    "        i = 0\n",
    "        while i < len(check):\n",
    "            label = check[i]\n",
    "            res = wikidata.get_uri(config, label)\n",
    "            if res == False:\n",
    "                print(\"Could not query properly. Not saving this instance.\")\n",
    "                ontology.save(config)\n",
    "                return\n",
    "            else:\n",
    "                temp_res += res\n",
    "            i += 1\n",
    "        if temp_res:\n",
    "            # We found some results\n",
    "            instance.uri = temp_res\n",
    "        else:\n",
    "            instance.uri = -1\n",
    "    ontology.save(config)\n",
    "    WikiData.print_updates()\n",
    "\n",
    "# TODO: Rework wikidata querrying\n",
    "# query_wikidata_for_instances(config, ontology, wikidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from orkg import ORKG  # import base class from package\n",
    "\n",
    "# # https://orkg.readthedocs.io/en/latest/client/resources.html#getting-resources-by-lookup\n",
    "\n",
    "# orkg = ORKG(host=\"https://sandbox.orkg.org/\")  # create the connector to the ORKG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: implement ORKG interface\n",
    "# matlab_findings = orkg.resources.get(\n",
    "#     q=\"matlab\", exact=False, size=30, sort=\"label\", desc=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Obsidian folder:\n",
      "number of tempaltes: 2 (Class, Instance)\n",
      "number of classes: 6\n",
      "number of instances: 570\n",
      "Instance avionics analytics ontology (aao)  will be newly added to instances.\n",
      "Instance  cfd simulations of the european hypersonic database will be newly added to instances.\n",
      "Instance cost analysis tool for manufacturing components  will be newly added to instances.\n",
      "Instance  engineering java programming will be newly added to instances.\n",
      "Instance hierarchical data format, version 5 (hdf5)  will be newly added to instances.\n",
      "Instance learning of ontologies from textual data  will be newly added to instances.\n",
      "Instance multidisciplinary modeler (mdm)  will be newly added to instances.\n",
      "Instance ontologies for nextgen avionics systems (onas)  will be newly added to instances.\n",
      "Instance open agent architecture (oaa)  will be newly added to instances.\n",
      "Instance process integration and design optimization (pido) platforms  will be newly added to instances.\n",
      "Instance requirements verification framework (rvf)  will be newly added to instances.\n",
      "Instance  scientific workflows (sw) will be newly added to instances.\n",
      "Instance sysml  will be newly added to instances.\n",
      "Populating Obsidian folder\n",
      "Added 584 instances and 6 classes to Obsidian folder\n",
      ".p21: ontology/obsidian/.p21.md\n",
      "vrml: ontology/obsidian/vrml.md\n",
      "cpacs: ontology/obsidian/cpacs.md\n",
      "parasolid: ontology/obsidian/parasolid.md\n",
      "json: ontology/obsidian/json.md\n",
      "flat file: ontology/obsidian/flat file.md\n",
      "dxf: ontology/obsidian/dxf.md\n",
      "mpeg: ontology/obsidian/mpeg.md\n",
      "txt: ontology/obsidian/txt.md\n",
      "wmv: ontology/obsidian/wmv.md\n",
      "xml: ontology/obsidian/xml.md\n",
      "html: ontology/obsidian/html.md\n",
      "dwf: ontology/obsidian/dwf.md\n",
      "wav: ontology/obsidian/wav.md\n",
      "dwg: ontology/obsidian/dwg.md\n",
      "wrp: ontology/obsidian/wrp.md\n",
      "iges: ontology/obsidian/iges.md\n",
      "database: ontology/obsidian/database.md\n",
      "csv: ontology/obsidian/csv.md\n",
      "mov: ontology/obsidian/mov.md\n",
      "owl: ontology/obsidian/owl.md\n",
      "jpg: ontology/obsidian/jpg.md\n",
      "mp3: ontology/obsidian/mp3.md\n",
      "rdf: ontology/obsidian/rdf.md\n",
      "acis: ontology/obsidian/acis.md\n",
      "igs: ontology/obsidian/igs.md\n",
      "tiff: ontology/obsidian/tiff.md\n",
      "jpeg: ontology/obsidian/jpeg.md\n",
      "png: ontology/obsidian/png.md\n",
      "postscript: ontology/obsidian/postscript.md\n",
      "stl: ontology/obsidian/stl.md\n",
      "soap: ontology/obsidian/soap.md\n",
      "f1: ontology/obsidian/f1.md\n",
      "gif: ontology/obsidian/gif.md\n",
      "pdf: ontology/obsidian/pdf.md\n",
      "step: ontology/obsidian/step.md\n",
      "flash: ontology/obsidian/flash.md\n",
      "add: ontology/obsidian/add.md\n",
      "direct api: ontology/obsidian/direct api.md\n",
      "ms excel: ontology/obsidian/ms excel.md\n",
      "common logic interchange format (clif): ontology/obsidian/common logic interchange format (clif).md\n",
      "ad-drw: ontology/obsidian/ad-drw.md\n",
      "ad_prt: ontology/obsidian/ad_prt.md\n",
      "3d pdf: ontology/obsidian/3d pdf.md\n",
      "drawing exchange format (dxf): ontology/obsidian/drawing exchange format (dxf).md\n",
      "ad_asm: ontology/obsidian/ad_asm.md\n",
      "lotus 123: ontology/obsidian/lotus 123.md\n",
      "jpgs: ontology/obsidian/jpgs.md\n",
      "oobc: ontology/obsidian/oobc.md\n",
      "ms word: ontology/obsidian/ms word.md\n",
      "knowledge interchange format (kif): ontology/obsidian/knowledge interchange format (kif).md\n",
      "configuration management: ontology/obsidian/configuration management.md\n",
      "semantic lexicon: ontology/obsidian/semantic lexicon.md\n",
      "solving: ontology/obsidian/solving.md\n",
      "workflow management: ontology/obsidian/workflow management.md\n",
      "product data management: ontology/obsidian/product data management.md\n",
      "system identification: ontology/obsidian/system identification.md\n",
      "link analysis: ontology/obsidian/link analysis.md\n",
      "knowledge classification: ontology/obsidian/knowledge classification.md\n",
      "analyzing: ontology/obsidian/analyzing.md\n",
      "system integration: ontology/obsidian/system integration.md\n",
      "automated reasoning: ontology/obsidian/automated reasoning.md\n",
      "product support: ontology/obsidian/product support.md\n",
      "ontological engineering: ontology/obsidian/ontological engineering.md\n",
      "knowledge exploration: ontology/obsidian/knowledge exploration.md\n",
      "knowledge representation and reasoning: ontology/obsidian/knowledge representation and reasoning.md\n",
      "project planning: ontology/obsidian/project planning.md\n",
      "flight test: ontology/obsidian/flight test.md\n",
      "conceptual modelling: ontology/obsidian/conceptual modelling.md\n",
      "digital mockup: ontology/obsidian/digital mockup.md\n",
      "concept design: ontology/obsidian/concept design.md\n",
      "information search: ontology/obsidian/information search.md\n",
      "product design: ontology/obsidian/product design.md\n",
      "customer service: ontology/obsidian/customer service.md\n",
      "authoring: ontology/obsidian/authoring.md\n",
      "data validation: ontology/obsidian/data validation.md\n",
      "information sharing: ontology/obsidian/information sharing.md\n",
      "detailed: ontology/obsidian/detailed.md\n",
      "enterprise resource planning: ontology/obsidian/enterprise resource planning.md\n",
      "information classification: ontology/obsidian/information classification.md\n",
      "knowledge discovery: ontology/obsidian/knowledge discovery.md\n",
      "knowledge engineering: ontology/obsidian/knowledge engineering.md\n",
      "decision support: ontology/obsidian/decision support.md\n",
      "data visualization: ontology/obsidian/data visualization.md\n",
      "reasoning: ontology/obsidian/reasoning.md\n",
      "calculating: ontology/obsidian/calculating.md\n",
      "data storage: ontology/obsidian/data storage.md\n",
      "general assembly: ontology/obsidian/general assembly.md\n",
      "problem solving: ontology/obsidian/problem solving.md\n",
      "dissemination: ontology/obsidian/dissemination.md\n",
      "inference: ontology/obsidian/inference.md\n",
      "crawling: ontology/obsidian/crawling.md\n",
      "manufacturing: ontology/obsidian/manufacturing.md\n",
      "manufacture: ontology/obsidian/manufacture.md\n",
      "knowledge management: ontology/obsidian/knowledge management.md\n",
      "navigation: ontology/obsidian/navigation.md\n",
      "check: ontology/obsidian/check.md\n",
      "creation: ontology/obsidian/creation.md\n",
      "scheduling: ontology/obsidian/scheduling.md\n",
      "evaluation: ontology/obsidian/evaluation.md\n",
      "optimization: ontology/obsidian/optimization.md\n",
      "processing: ontology/obsidian/processing.md\n",
      "recycling: ontology/obsidian/recycling.md\n",
      "acquisition: ontology/obsidian/acquisition.md\n",
      "analysis: ontology/obsidian/analysis.md\n",
      "plm: ontology/obsidian/plm.md\n",
      "archive: ontology/obsidian/archive.md\n",
      "failure: ontology/obsidian/failure.md\n",
      "training: ontology/obsidian/training.md\n",
      "certification: ontology/obsidian/certification.md\n",
      "development: ontology/obsidian/development.md\n",
      "p1: ontology/obsidian/p1.md\n",
      "design: ontology/obsidian/design.md\n",
      "alm: ontology/obsidian/alm.md\n",
      "review: ontology/obsidian/review.md\n",
      "interest profiling: ontology/obsidian/interest profiling.md\n",
      "knowlege graph construction:knowledge extraction: ontology/obsidian/knowlege graph constructionknowledge extraction.md\n",
      "test report: ontology/obsidian/test report.md\n",
      "mdo process formulation, visualizations: ontology/obsidian/mdo process formulation, visualizations.md\n",
      "express ontology: ontology/obsidian/express ontology.md\n",
      "preliminary design: ontology/obsidian/preliminary design.md\n",
      "knowlege graph building:text classification: ontology/obsidian/knowlege graph buildingtext classification.md\n",
      "knowledge application:q&a: ontology/obsidian/knowledge applicationq&a.md\n",
      "system architecting: ontology/obsidian/system architecting.md\n",
      "knowlege graph building:document classification: ontology/obsidian/knowlege graph buildingdocument classification.md\n",
      "design and development: ontology/obsidian/design and development.md\n",
      "on-board system: ontology/obsidian/on-board system.md\n",
      "calculation report: ontology/obsidian/calculation report.md\n",
      "multimedia document processing: ontology/obsidian/multimedia document processing.md\n",
      "mission analysis: ontology/obsidian/mission analysis.md\n",
      "knowledge application:knowledge extraction: ontology/obsidian/knowledge applicationknowledge extraction.md\n",
      "rss (really simple syndication): ontology/obsidian/rss (really simple syndication).md\n",
      "generate fem model: ontology/obsidian/generate fem model.md\n",
      "knowledge based engineering: ontology/obsidian/knowledge based engineering.md\n",
      "text analyisis: ontology/obsidian/text analyisis.md\n",
      "integrate the mdo architecture in an executable workflow: ontology/obsidian/integrate the mdo architecture in an executable workflow.md\n",
      "managing project files: ontology/obsidian/managing project files.md\n",
      "engine accessory purchase: ontology/obsidian/engine accessory purchase.md\n",
      "tooling development: ontology/obsidian/tooling development.md\n",
      "data processing:recall: ontology/obsidian/data processingrecall.md\n",
      "knowledge interfacing:classification of question intent: ontology/obsidian/knowledge interfacingclassification of question intent.md\n",
      "mdao: ontology/obsidian/mdao.md\n",
      "knowledge interfacing:data review: ontology/obsidian/knowledge interfacingdata review.md\n",
      "retrieve product term object and task requirement: ontology/obsidian/retrieve product term object and task requirement.md\n",
      "knowledge interfacing:data retrieval: ontology/obsidian/knowledge interfacingdata retrieval.md\n",
      "business process integration: ontology/obsidian/business process integration.md\n",
      "design concepts formalization: ontology/obsidian/design concepts formalization.md\n",
      "data integration/fusion: ontology/obsidian/data integration or fusion.md\n",
      "activity profiling: ontology/obsidian/activity profiling.md\n",
      "data processing:analysis: ontology/obsidian/data processinganalysis.md\n",
      "nation decision: ontology/obsidian/nation decision.md\n",
      "integration of single-media and cross-media information: ontology/obsidian/integration of single-media and cross-media information.md\n",
      "wide body transports: ontology/obsidian/wide body transports.md\n",
      "wf execution: ontology/obsidian/wf execution.md\n",
      "knowlege graph building:data labeling: ontology/obsidian/knowlege graph buildingdata labeling.md\n",
      "translations among distinct vocabularies: ontology/obsidian/translations among distinct vocabularies.md\n",
      "support & service: ontology/obsidian/support & service.md\n",
      "eligibility of model examiniation: ontology/obsidian/eligibility of model examiniation.md\n",
      "ground test: ontology/obsidian/ground test.md\n",
      "failure management: ontology/obsidian/failure management.md\n",
      "physical sensing: ontology/obsidian/physical sensing.md\n",
      "searching for a solution to problems that may have been solved: ontology/obsidian/searching for a solution to problems that may have been solved.md\n",
      "capture and reuse engineering experience: ontology/obsidian/capture and reuse engineering experience.md\n",
      "description of object oriented software: ontology/obsidian/description of object oriented software.md\n",
      "engine analysis: ontology/obsidian/engine analysis.md\n",
      "knowlege graph construction:text classification: ontology/obsidian/knowlege graph constructiontext classification.md\n",
      "sharing of knowledge and expertise: ontology/obsidian/sharing of knowledge and expertise.md\n",
      "annotating the semantic contents of a document: ontology/obsidian/annotating the semantic contents of a document.md\n",
      "knowledge and data sharing: ontology/obsidian/knowledge and data sharing.md\n",
      "knowledge interfacing:problematic entity extraction: ontology/obsidian/knowledge interfacingproblematic entity extraction.md\n",
      "knowledge interfacing:q&a: ontology/obsidian/knowledge interfacingq&a.md\n",
      "data search, access, and retrieval: ontology/obsidian/data search, access, and retrieval.md\n",
      "context inference: ontology/obsidian/context inference.md\n",
      "multidisciplinary modeling: ontology/obsidian/multidisciplinary modeling.md\n",
      "knowledge interfacing:knowledge extraction: ontology/obsidian/knowledge interfacingknowledge extraction.md\n",
      "processe data: ontology/obsidian/processe data.md\n",
      "knowlege graph construction: ontology/obsidian/knowlege graph construction.md\n",
      "mapping between disparate data sets: ontology/obsidian/mapping between disparate data sets.md\n",
      "accessory manufacture: ontology/obsidian/accessory manufacture.md\n",
      "access to knowledge and expertise: ontology/obsidian/access to knowledge and expertise.md\n",
      "detailed design: ontology/obsidian/detailed design.md\n",
      "concept, design & development: ontology/obsidian/concept, design & development.md\n",
      "knowlege graph construction:data management: ontology/obsidian/knowlege graph constructiondata management.md\n",
      "system specification: ontology/obsidian/system specification.md\n",
      "sales and marketing: ontology/obsidian/sales and marketing.md\n",
      "development safeguard actualization: ontology/obsidian/development safeguard actualization.md\n",
      "Model-based systems engineering (MBSE): ontology/obsidian/Model-based systems engineering (MBSE).md\n",
      "life cycle simulation: ontology/obsidian/life cycle simulation.md\n",
      "system arch: ontology/obsidian/system arch.md\n",
      "learning of ontologies from textual data : ontology/obsidian/learning of ontologies from textual data.md\n",
      "data processing:collection: ontology/obsidian/data processingcollection.md\n",
      "manufacturing preparation: ontology/obsidian/manufacturing preparation.md\n",
      "feature processing: ontology/obsidian/feature processing.md\n",
      "systems engineering (SE): ontology/obsidian/systems engineering (SE).md\n",
      "knowledge application:data labeling: ontology/obsidian/knowledge applicationdata labeling.md\n",
      "convert file formats: ontology/obsidian/convert file formats.md\n",
      "data processing:storage: ontology/obsidian/data processingstorage.md\n",
      "knowledge interaction:knowledge visualisation: ontology/obsidian/knowledge interactionknowledge visualisation.md\n",
      "terminology recognition: ontology/obsidian/terminology recognition.md\n",
      "flight test report: ontology/obsidian/flight test report.md\n",
      "rehost' each release of a flight dynamics model from one simulation environment to another one: ontology/obsidian/rehost' each release of a flight dynamics model from one simulation environment to another one.md\n",
      "aceess tooling cad models: ontology/obsidian/aceess tooling cad models.md\n",
      "system synthesis: ontology/obsidian/system synthesis.md\n",
      "knowledge capture: ontology/obsidian/knowledge capture.md\n",
      "resource description: ontology/obsidian/resource description.md\n",
      "cost analysis tool for manufacturing components : ontology/obsidian/cost analysis tool for manufacturing components.md\n",
      "semantic information transferring: ontology/obsidian/semantic information transferring.md\n",
      "first flight decision: ontology/obsidian/first flight decision.md\n",
      "semantic knowledge management within design engineering: ontology/obsidian/semantic knowledge management within design engineering.md\n",
      "sharing and usage of surrogate models: ontology/obsidian/sharing and usage of surrogate models.md\n",
      "design space visualization: ontology/obsidian/design space visualization.md\n",
      "knowledge systematisation: ontology/obsidian/knowledge systematisation.md\n",
      "initiate action according to tooling types: ontology/obsidian/initiate action according to tooling types.md\n",
      "knowledge application:quality assessment: ontology/obsidian/knowledge applicationquality assessment.md\n",
      "knowledge interfacing:model training: ontology/obsidian/knowledge interfacingmodel training.md\n",
      "knowlege graph construction:data labeling: ontology/obsidian/knowlege graph constructiondata labeling.md\n",
      "km:knowledge creation: ontology/obsidian/kmknowledge creation.md\n",
      "knowlege graph building:file importing: ontology/obsidian/knowlege graph buildingfile importing.md\n",
      "data processing:visualization: ontology/obsidian/data processingvisualization.md\n",
      "definition of the optimization problem: ontology/obsidian/definition of the optimization problem.md\n",
      "summarizing: ontology/obsidian/summarizing.md\n",
      "caturing contextual information form information users (profiling techniques): ontology/obsidian/caturing contextual information form information users (profiling techniques).md\n",
      "system design: ontology/obsidian/system design.md\n",
      "flight-test preparation: ontology/obsidian/flight-test preparation.md\n",
      "issue tc pc ac certification: ontology/obsidian/issue tc pc ac certification.md\n",
      "identify different tooling types: ontology/obsidian/identify different tooling types.md\n",
      "knowlege graph building:data management: ontology/obsidian/knowlege graph buildingdata management.md\n",
      "visualization (xdsm): ontology/obsidian/visualization (xdsm).md\n",
      "visualization of large, complex ad processes: ontology/obsidian/visualization of large, complex ad processes.md\n",
      "data processing:organization: ontology/obsidian/data processingorganization.md\n",
      "initiate the product term class: ontology/obsidian/initiate the product term class.md\n",
      "knowledge management (km): ontology/obsidian/knowledge management (km).md\n",
      "km:knowledge storage / retrieval: ontology/obsidian/kmknowledge storage  or  retrieval.md\n",
      "graph-based support in the design problem formulation: ontology/obsidian/graph-based support in the design problem formulation.md\n",
      "collection and sharing of design information through a single source of truth (ssot): ontology/obsidian/collection and sharing of design information through a single source of truth (ssot).md\n",
      "knowlege graph building:knowledge audit: ontology/obsidian/knowlege graph buildingknowledge audit.md\n",
      "structural solver: ontology/obsidian/structural solver.md\n",
      "production & product launch: ontology/obsidian/production & product launch.md\n",
      "write data back to the model: ontology/obsidian/write data back to the model.md\n",
      "disciplinary analysis: ontology/obsidian/disciplinary analysis.md\n",
      "airfraem manufacturing: ontology/obsidian/airfraem manufacturing.md\n",
      "knowledge interaction:q&a: ontology/obsidian/knowledge interactionq&a.md\n",
      "knowledge, skill and background profiling: ontology/obsidian/knowledge, skill and background profiling.md\n",
      "design blueprint: ontology/obsidian/design blueprint.md\n",
      "incorporating visualization into multiobjective optimization tools for mdo problems: ontology/obsidian/incorporating visualization into multiobjective optimization tools for mdo problems.md\n",
      "exchange of models: ontology/obsidian/exchange of models.md\n",
      "pattern analyis: ontology/obsidian/pattern analyis.md\n",
      "handling concepts and relationships between concepts (semantic techniques): ontology/obsidian/handling concepts and relationships between concepts (semantic techniques).md\n",
      "information clustering: ontology/obsidian/information clustering.md\n",
      "deliver first aircraft: ontology/obsidian/deliver first aircraft.md\n",
      "control & stability: ontology/obsidian/control & stability.md\n",
      "km:application: ontology/obsidian/kmapplication.md\n",
      "populate the objects of product term class with semantic information and enclose the objects in message being sent: ontology/obsidian/populate the objects of product term class with semantic information and enclose the objects in message being sent.md\n",
      "classification and terminology development: ontology/obsidian/classification and terminology development.md\n",
      "software interlinking: ontology/obsidian/software interlinking.md\n",
      "knowledge application:knowledge visualisation: ontology/obsidian/knowledge applicationknowledge visualisation.md\n",
      "multi-disciplinary analysis and optimization (mdao): ontology/obsidian/multi-disciplinary analysis and optimization (mdao).md\n",
      "selection of a suitable mdo architecture for the problem at hand: ontology/obsidian/selection of a suitable mdo architecture for the problem at hand.md\n",
      "knowledge interaction: ontology/obsidian/knowledge interaction.md\n",
      "knowledge interfacing:question answer search: ontology/obsidian/knowledge interfacingquestion answer search.md\n",
      "knowlege graph building:knowledge extraction: ontology/obsidian/knowlege graph buildingknowledge extraction.md\n",
      "read data from the model: ontology/obsidian/read data from the model.md\n",
      "perliminary design: ontology/obsidian/perliminary design.md\n",
      "exchange of design intents: ontology/obsidian/exchange of design intents.md\n",
      "descriptive information generation: ontology/obsidian/descriptive information generation.md\n",
      "access product design data: ontology/obsidian/access product design data.md\n",
      "export data, translate to one neutral format once, then exchange that: ontology/obsidian/export data, translate to one neutral format once, then exchange that.md\n",
      "airworthiness test: ontology/obsidian/airworthiness test.md\n",
      "share documents: ontology/obsidian/share documents.md\n",
      "ontology modeling: ontology/obsidian/ontology modeling.md\n",
      "km:transfer: ontology/obsidian/kmtransfer.md\n",
      "knowledge base building: ontology/obsidian/knowledge base building.md\n",
      "machine-to-machine communication: ontology/obsidian/machine-to-machine communication.md\n",
      "information push: ontology/obsidian/information push.md\n",
      "establish links from architectural elements to nodes in the data schema: ontology/obsidian/establish links from architectural elements to nodes in the data schema.md\n",
      "product verification: ontology/obsidian/product verification.md\n",
      "analysing information content (data mining techniques): ontology/obsidian/analysing information content (data mining techniques).md\n",
      "knowledge storage: ontology/obsidian/knowledge storage.md\n",
      "format transforming of semantic information: ontology/obsidian/format transforming of semantic information.md\n",
      "mdo process formulation: ontology/obsidian/mdo process formulation.md\n",
      "semantic representation: ontology/obsidian/semantic representation.md\n",
      "knowlege graph construction:data review: ontology/obsidian/knowlege graph constructiondata review.md\n",
      "model based systems engineering (mbse): ontology/obsidian/model based systems engineering (mbse).md\n",
      "on-board systems: ontology/obsidian/on-board systems.md\n",
      "systems engineering (se): ontology/obsidian/systems engineering (se).md\n",
      "file server: ontology/obsidian/file server.md\n",
      "inverted index: ontology/obsidian/inverted index.md\n",
      "tixi: ontology/obsidian/tixi.md\n",
      "vivisimo: ontology/obsidian/vivisimo.md\n",
      "expert system: ontology/obsidian/expert system.md\n",
      "web server: ontology/obsidian/web server.md\n",
      "triple store: ontology/obsidian/triple store.md\n",
      "networkx: ontology/obsidian/networkx.md\n",
      "neo4j: ontology/obsidian/neo4j.md\n",
      "spss: ontology/obsidian/spss.md\n",
      "reasoner: ontology/obsidian/reasoner.md\n",
      "t-rex: ontology/obsidian/t-rex.md\n",
      "flops: ontology/obsidian/flops.md\n",
      "brics: ontology/obsidian/brics.md\n",
      "matlab: ontology/obsidian/matlab.md\n",
      "protégé: ontology/obsidian/protégé.md\n",
      "data mining: ontology/obsidian/data mining.md\n",
      "catia: ontology/obsidian/catia.md\n",
      "kaon: ontology/obsidian/kaon.md\n",
      "google: ontology/obsidian/google.md\n",
      "optimus: ontology/obsidian/optimus.md\n",
      "rce: ontology/obsidian/rce.md\n",
      "ug: ontology/obsidian/ug.md\n",
      "pest: ontology/obsidian/pest.md\n",
      "excel: ontology/obsidian/excel.md\n",
      "wikipedia: ontology/obsidian/wikipedia.md\n",
      "ideas: ontology/obsidian/ideas.md\n",
      "git: ontology/obsidian/git.md\n",
      "word: ontology/obsidian/word.md\n",
      "saxon: ontology/obsidian/saxon.md\n",
      "daedalus: ontology/obsidian/daedalus.md\n",
      "api: ontology/obsidian/api.md\n",
      "proteus: ontology/obsidian/proteus.md\n",
      "prado: ontology/obsidian/prado.md\n",
      "sumo: ontology/obsidian/sumo.md\n",
      "access: ontology/obsidian/access.md\n",
      "oracle: ontology/obsidian/oracle.md\n",
      "aaa: ontology/obsidian/aaa.md\n",
      "tr: ontology/obsidian/tr.md\n",
      "janus: ontology/obsidian/janus.md\n",
      "s2: ontology/obsidian/s2.md\n",
      "outlook: ontology/obsidian/outlook.md\n",
      "s1: ontology/obsidian/s1.md\n",
      "tornado: ontology/obsidian/tornado.md\n",
      "sas: ontology/obsidian/sas.md\n",
      "jade: ontology/obsidian/jade.md\n",
      "eclipse: ontology/obsidian/eclipse.md\n",
      "r: ontology/obsidian/r.md\n",
      "oracle server: ontology/obsidian/oracle server.md\n",
      "text2rdf: ontology/obsidian/text2rdf.md\n",
      "cloud visualization: ontology/obsidian/cloud visualization.md\n",
      "java agent development environment (jade): ontology/obsidian/java agent development environment (jade).md\n",
      "multilinq: ontology/obsidian/multilinq.md\n",
      "modelcenter: ontology/obsidian/modelcenter.md\n",
      "fortran application: ontology/obsidian/fortran application.md\n",
      "texttoonto: ontology/obsidian/texttoonto.md\n",
      "express application: ontology/obsidian/express application.md\n",
      "share point: ontology/obsidian/share point.md\n",
      "k-search: ontology/obsidian/k-search.md\n",
      "nx4: ontology/obsidian/nx4.md\n",
      "express compiler in jsdai: ontology/obsidian/express compiler in jsdai.md\n",
      "hyper-pareto frontier visualization: ontology/obsidian/hyper-pareto frontier visualization.md\n",
      "discussion forums, knowledge directories: ontology/obsidian/discussion forums, knowledge directories.md\n",
      "ceasiom: ontology/obsidian/ceasiom.md\n",
      "computational analysis programming interface (capri): ontology/obsidian/computational analysis programming interface (capri).md\n",
      "requirements verification framework (rvf) : ontology/obsidian/requirements verification framework (rvf).md\n",
      "cad programming interface: ontology/obsidian/cad programming interface.md\n",
      "simulation workflow management (swfm): ontology/obsidian/simulation workflow management (swfm).md\n",
      "cad representation vdsm: ontology/obsidian/cad representation vdsm.md\n",
      "mopcssoviz: ontology/obsidian/mopcssoviz.md\n",
      "pykechain: ontology/obsidian/pykechain.md\n",
      "i-deas(sdrc): ontology/obsidian/i-deas(sdrc).md\n",
      "nextgen avionics systems: ontology/obsidian/nextgen avionics systems.md\n",
      "knowledege repositories, databases: ontology/obsidian/knowledege repositories, databases.md\n",
      "geographic independent virtual environment (give): ontology/obsidian/geographic independent virtual environment (give).md\n",
      "cad2fem: ontology/obsidian/cad2fem.md\n",
      "database management system servers (dbms): ontology/obsidian/database management system servers (dbms).md\n",
      "knowledge optimized manufacture and design (knomad): ontology/obsidian/knowledge optimized manufacture and design (knomad).md\n",
      "workflow systems: ontology/obsidian/workflow systems.md\n",
      "natural language information analysis method (niam) application: ontology/obsidian/natural language information analysis method (niam) application.md\n",
      "remote component environment (rce): ontology/obsidian/remote component environment (rce).md\n",
      "text2onto: ontology/obsidian/text2onto.md\n",
      "fault knowledge graph construction and platform for aircraft phm: ontology/obsidian/fault knowledge graph construction and platform for aircraft phm.md\n",
      "surrogate model generator (smg): ontology/obsidian/surrogate model generator (smg).md\n",
      "openmdao: ontology/obsidian/openmdao.md\n",
      "davetools: ontology/obsidian/davetools.md\n",
      "optimus workflow management tool: ontology/obsidian/optimus workflow management tool.md\n",
      "java application: ontology/obsidian/java application.md\n",
      "electronic bulletin boards: ontology/obsidian/electronic bulletin boards.md\n",
      "syndeia: ontology/obsidian/syndeia.md\n",
      " engineering java programming: ontology/obsidian/engineering java programming.md\n",
      "python application: ontology/obsidian/python application.md\n",
      "solidedge: ontology/obsidian/solidedge.md\n",
      "ontology-driven interactive search environment for earth sciences (odisees): ontology/obsidian/ontology-driven interactive search environment for earth sciences (odisees).md\n",
      "whatsopt: ontology/obsidian/whatsopt.md\n",
      "cpacspy library: ontology/obsidian/cpacspy library.md\n",
      "open agent architecture (oaa) : ontology/obsidian/open agent architecture (oaa).md\n",
      "software interlinking tools: ontology/obsidian/software interlinking tools.md\n",
      "common mdo workflow schema (cmdows): ontology/obsidian/common mdo workflow schema (cmdows).md\n",
      "visualization in support of the shopping paradigm: ontology/obsidian/visualization in support of the shopping paradigm.md\n",
      "gkn fokker: ontology/obsidian/gkn fokker.md\n",
      "multidisciplinary modeler (mdm) : ontology/obsidian/multidisciplinary modeler (mdm).md\n",
      "jsdai4: ontology/obsidian/jsdai4.md\n",
      "web-based decision tool: ontology/obsidian/web-based decision tool.md\n",
      "solidworks: ontology/obsidian/solidworks.md\n",
      "nasa ames function table processor scripts: ontology/obsidian/nasa ames function table processor scripts.md\n",
      "swf software package rce: ontology/obsidian/swf software package rce.md\n",
      "fsms: ontology/obsidian/fsms.md\n",
      "expresso9: ontology/obsidian/expresso9.md\n",
      "vampzero element: ontology/obsidian/vampzero element.md\n",
      "j2ee application servers: ontology/obsidian/j2ee application servers.md\n",
      "cfview: ontology/obsidian/cfview.md\n",
      "isight: ontology/obsidian/isight.md\n",
      "ke-chain: ontology/obsidian/ke-chain.md\n",
      "social interaction framework (sif): ontology/obsidian/social interaction framework (sif).md\n",
      "libxml240: ontology/obsidian/libxml240.md\n",
      " scientific workflows (sw): ontology/obsidian/scientific workflows (sw).md\n",
      "qfview web interface: ontology/obsidian/qfview web interface.md\n",
      "agile development framework (adf): ontology/obsidian/agile development framework (adf).md\n",
      "visualization interface for physical programming: ontology/obsidian/visualization interface for physical programming.md\n",
      "vdk/hc: ontology/obsidian/vdk or hc.md\n",
      "altova’s umodel: ontology/obsidian/altova´s umodel.md\n",
      "eclipse plugin: ontology/obsidian/eclipse plugin.md\n",
      "c application: ontology/obsidian/c application.md\n",
      "jupyter notebook: ontology/obsidian/jupyter notebook.md\n",
      "multi-agent collaborative system: ontology/obsidian/multi-agent collaborative system.md\n",
      "visualization package: ontology/obsidian/visualization package.md\n",
      "process integration and design optimization (pido) platforms : ontology/obsidian/process integration and design optimization (pido) platforms.md\n",
      "lpg was a cypher-neo4j: ontology/obsidian/lpg was a cypher-neo4j.md\n",
      "process integration and design optimization (pido): ontology/obsidian/process integration and design optimization (pido).md\n",
      "pro/e(ptc): ontology/obsidian/pro or e(ptc).md\n",
      "idef1x application: ontology/obsidian/idef1x application.md\n",
      "brics technology: ontology/obsidian/brics technology.md\n",
      "product data management system: ontology/obsidian/product data management system.md\n",
      "surrogate model repository: ontology/obsidian/surrogate model repository.md\n",
      "cost analysis tool for manufacturing of aircraft components (catmac): ontology/obsidian/cost analysis tool for manufacturing of aircraft components (catmac).md\n",
      "labeled property graph database (lpg): ontology/obsidian/labeled property graph database (lpg).md\n",
      "cadds5(cv): ontology/obsidian/cadds5(cv).md\n",
      "semsearch: ontology/obsidian/semsearch.md\n",
      "tigl: ontology/obsidian/tigl.md\n",
      "operational collaborative environment (oce): ontology/obsidian/operational collaborative environment (oce).md\n",
      "learning tools: ontology/obsidian/learning tools.md\n",
      "imagenet: ontology/obsidian/imagenet.md\n",
      "knowledge mangement tool: ontology/obsidian/knowledge mangement tool.md\n",
      "cameo system modeler (csm): ontology/obsidian/cameo system modeler (csm).md\n",
      "coom10: ontology/obsidian/coom10.md\n",
      "workflow system: ontology/obsidian/workflow system.md\n",
      "point clouds: ontology/obsidian/point clouds.md\n",
      "mind map: ontology/obsidian/mind map.md\n",
      "live video: ontology/obsidian/live video.md\n",
      "business logic: ontology/obsidian/business logic.md\n",
      "knowledge base: ontology/obsidian/knowledge base.md\n",
      "manufacturing method: ontology/obsidian/manufacturing method.md\n",
      "knowledge graph: ontology/obsidian/knowledge graph.md\n",
      "how it works: ontology/obsidian/how it works.md\n",
      "assemblies: ontology/obsidian/assemblies.md\n",
      "ontology: ontology/obsidian/ontology.md\n",
      "freebase: ontology/obsidian/freebase.md\n",
      "description: ontology/obsidian/description.md\n",
      "quantity: ontology/obsidian/quantity.md\n",
      "capp: ontology/obsidian/capp.md\n",
      "difficulty: ontology/obsidian/difficulty.md\n",
      "taxonomy: ontology/obsidian/taxonomy.md\n",
      "audio: ontology/obsidian/audio.md\n",
      "forms: ontology/obsidian/forms.md\n",
      "animation: ontology/obsidian/animation.md\n",
      "video: ontology/obsidian/video.md\n",
      "metadata: ontology/obsidian/metadata.md\n",
      "documents: ontology/obsidian/documents.md\n",
      "name: ontology/obsidian/name.md\n",
      "material: ontology/obsidian/material.md\n",
      "bom: ontology/obsidian/bom.md\n",
      "images: ontology/obsidian/images.md\n",
      "image: ontology/obsidian/image.md\n",
      "cad: ontology/obsidian/cad.md\n",
      "d1: ontology/obsidian/d1.md\n",
      "knowledge graph (kg): ontology/obsidian/knowledge graph (kg).md\n",
      "2d engineering drawings: ontology/obsidian/2d engineering drawings.md\n",
      "design parts: ontology/obsidian/design parts.md\n",
      "formatted documents: ontology/obsidian/formatted documents.md\n",
      "measurement data: ontology/obsidian/measurement data.md\n",
      "implicit geometry: ontology/obsidian/implicit geometry.md\n",
      "cad representation: ontology/obsidian/cad representation.md\n",
      "babelnet: ontology/obsidian/babelnet.md\n",
      "2d drawings: ontology/obsidian/2d drawings.md\n",
      "ikewiki: ontology/obsidian/ikewiki.md\n",
      "sweetwiki: ontology/obsidian/sweetwiki.md\n",
      "cfd simulation data: ontology/obsidian/cfd simulation data.md\n",
      "tools needed: ontology/obsidian/tools needed.md\n",
      "ontologies for nextgen avionics systems (onas) : ontology/obsidian/ontologies for nextgen avionics systems (onas).md\n",
      "lexical data: ontology/obsidian/lexical data.md\n",
      "knowledge architecture (ka): ontology/obsidian/knowledge architecture (ka).md\n",
      " cfd simulations of the european hypersonic database: ontology/obsidian/cfd simulations of the european hypersonic database.md\n",
      "external web page: ontology/obsidian/external web page.md\n",
      "cfd code: ontology/obsidian/cfd code.md\n",
      "sectors of database: ontology/obsidian/sectors of database.md\n",
      "national airspace system (nas): ontology/obsidian/national airspace system (nas).md\n",
      "extensible stylesheet (xsl) conversion: ontology/obsidian/extensible stylesheet (xsl) conversion.md\n",
      "architecture design space graph (adsg): ontology/obsidian/architecture design space graph (adsg).md\n",
      "process definitions: ontology/obsidian/process definitions.md\n",
      "pdf files: ontology/obsidian/pdf files.md\n",
      "avionics analytics ontology (aao) : ontology/obsidian/avionics analytics ontology (aao).md\n",
      "extended design structure matrix (xdsm): ontology/obsidian/extended design structure matrix (xdsm).md\n",
      "business objects & processes: ontology/obsidian/business objects & processes.md\n",
      "3d models: ontology/obsidian/3d models.md\n",
      "word doc: ontology/obsidian/word doc.md\n",
      "object definitions: ontology/obsidian/object definitions.md\n",
      "tooling parts: ontology/obsidian/tooling parts.md\n",
      "animations: ontology/obsidian/animations.md\n",
      "section description: ontology/obsidian/section description.md\n",
      "unformatted documents: ontology/obsidian/unformatted documents.md\n",
      "system definition: ontology/obsidian/system definition.md\n",
      "tables, spreadsheets: ontology/obsidian/tables, spreadsheets.md\n",
      "3d solid model: ontology/obsidian/3d solid model.md\n",
      "explicit geometry: ontology/obsidian/explicit geometry.md\n",
      "sysml model: ontology/obsidian/sysml model.md\n",
      "word processor documents: ontology/obsidian/word processor documents.md\n",
      "solr: ontology/obsidian/solr.md\n",
      "mdax: ontology/obsidian/mdax.md\n",
      "idef1x: ontology/obsidian/idef1x.md\n",
      "owl dl: ontology/obsidian/owl dl.md\n",
      "wordnet: ontology/obsidian/wordnet.md\n",
      "sparql: ontology/obsidian/sparql.md\n",
      "fortan: ontology/obsidian/fortan.md\n",
      "addam: ontology/obsidian/addam.md\n",
      "sql: ontology/obsidian/sql.md\n",
      "yawl: ontology/obsidian/yawl.md\n",
      "rvf: ontology/obsidian/rvf.md\n",
      "kadmos: ontology/obsidian/kadmos.md\n",
      "uml: ontology/obsidian/uml.md\n",
      "vba: ontology/obsidian/vba.md\n",
      "bfo: ontology/obsidian/bfo.md\n",
      "codex: ontology/obsidian/codex.md\n",
      "d11: ontology/obsidian/d11.md\n",
      "cco: ontology/obsidian/cco.md\n",
      "rds: ontology/obsidian/rds.md\n",
      "cypher: ontology/obsidian/cypher.md\n",
      "python: ontology/obsidian/python.md\n",
      "express: ontology/obsidian/express.md\n",
      "c: ontology/obsidian/c.md\n",
      "extensible graphics language (xgl): ontology/obsidian/extensible graphics language (xgl).md\n",
      "web ontology language (owl): ontology/obsidian/web ontology language (owl).md\n",
      "xml schema definition (xsd): ontology/obsidian/xml schema definition (xsd).md\n",
      "resources description framework (rdf): ontology/obsidian/resources description framework (rdf).md\n",
      "dtad: ontology/obsidian/dtad.md\n",
      "postgresql: ontology/obsidian/postgresql.md\n",
      "space situational awareness ontology (ssao): ontology/obsidian/space situational awareness ontology (ssao).md\n",
      "xcalibr (xml capability analysis library): ontology/obsidian/xcalibr (xml capability analysis library).md\n",
      "extensible markup language (xml): ontology/obsidian/extensible markup language (xml).md\n",
      "nosql: ontology/obsidian/nosql.md\n",
      "serql: ontology/obsidian/serql.md\n",
      "xquery: ontology/obsidian/xquery.md\n",
      "sysml : ontology/obsidian/sysml.md\n",
      "step part 42:edm: ontology/obsidian/step part 42edm.md\n",
      "cmdows: ontology/obsidian/cmdows.md\n",
      "adaptive neuro-fuzzy inference system (anfis): ontology/obsidian/adaptive neuro-fuzzy inference system (anfis).md\n",
      "common parametric aircraft schema (cpacs): ontology/obsidian/common parametric aircraft schema (cpacs).md\n",
      "nasa taxonomy: ontology/obsidian/nasa taxonomy.md\n",
      "multi-agent modeling language (maml): ontology/obsidian/multi-agent modeling language (maml).md\n",
      "mdo fad (framework for analysis and design): ontology/obsidian/mdo fad (framework for analysis and design).md\n",
      "multiwordnet: ontology/obsidian/multiwordnet.md\n",
      "radex (rationale-based design explanation): ontology/obsidian/radex (rationale-based design explanation).md\n",
      "topic maps: ontology/obsidian/topic maps.md\n",
      "document type definition (dtd: ontology/obsidian/document type definition (dtd.md\n",
      "darpa agent markup language (daml): ontology/obsidian/darpa agent markup language (daml).md\n",
      "aerospace ontology: ontology/obsidian/aerospace ontology.md\n",
      "hierarchical data format, version 5 (hdf5) : ontology/obsidian/hierarchical data format, version 5 (hdf5).md\n",
      "javascript object notation (json): ontology/obsidian/javascript object notation (json).md\n",
      "common parametric aircraft configuration schema (cpacs): ontology/obsidian/common parametric aircraft configuration schema (cpacs).md\n",
      "object role model (orm): ontology/obsidian/object role model (orm).md\n",
      "rom (recursive object model): ontology/obsidian/rom (recursive object model).md\n",
      "x-forms: ontology/obsidian/x-forms.md\n",
      "orbital debris ontology (odo): ontology/obsidian/orbital debris ontology (odo).md\n",
      "bond graphs: ontology/obsidian/bond graphs.md\n",
      "vampzero: ontology/obsidian/vampzero.md\n",
      "openvsp: ontology/obsidian/openvsp.md\n",
      "k-store: ontology/obsidian/k-store.md\n",
      "orbital space environment domain ontology: ontology/obsidian/orbital space environment domain ontology.md\n",
      "xml-schema-definition: ontology/obsidian/xml-schema-definition.md\n",
      "numerical control markup language (ncml): ontology/obsidian/numerical control markup language (ncml).md\n",
      "k-forms: ontology/obsidian/k-forms.md\n",
      "nasa semantic web for earth and environmental terminology (sweet): ontology/obsidian/nasa semantic web for earth and environmental terminology (sweet).md\n",
      "common mdao workflow schema (cmdows): ontology/obsidian/common mdao workflow schema (cmdows).md\n",
      "rest (http) api: ontology/obsidian/rest (http) api.md\n",
      "airspace system ontology: ontology/obsidian/airspace system ontology.md\n",
      "owl-ql: ontology/obsidian/owl-ql.md\n",
      "avionics analytics ontology (aao) : ontology/obsidian/avionics analytics ontology (aao).md\n",
      " cfd simulations of the european hypersonic database: ontology/obsidian/cfd simulations of the european hypersonic database.md\n",
      "cost analysis tool for manufacturing components : ontology/obsidian/cost analysis tool for manufacturing components.md\n",
      " engineering java programming: ontology/obsidian/engineering java programming.md\n",
      "hierarchical data format, version 5 (hdf5) : ontology/obsidian/hierarchical data format, version 5 (hdf5).md\n",
      "learning of ontologies from textual data : ontology/obsidian/learning of ontologies from textual data.md\n",
      "multidisciplinary modeler (mdm) : ontology/obsidian/multidisciplinary modeler (mdm).md\n",
      "ontologies for nextgen avionics systems (onas) : ontology/obsidian/ontologies for nextgen avionics systems (onas).md\n",
      "open agent architecture (oaa) : ontology/obsidian/open agent architecture (oaa).md\n",
      "process integration and design optimization (pido) platforms : ontology/obsidian/process integration and design optimization (pido) platforms.md\n",
      "requirements verification framework (rvf) : ontology/obsidian/requirements verification framework (rvf).md\n",
      " scientific workflows (sw): ontology/obsidian/scientific workflows (sw).md\n",
      "sysml : ontology/obsidian/sysml.md\n",
      "Struck et al. - 1984 - EVALUATION OF OPERATIONAL LOACS TO VERIFY STRUCTUR: ontology/obsidian/Struck et al. - 1984 - EVALUATION OF OPERATIONAL LOACS TO VERIFY STRUCTUR.md\n",
      "Argialas und Mintzer - 1993 - The potential of hypermedia to photointerpretation: ontology/obsidian/Argialas und Mintzer - 1993 - The potential of hypermedia to photointerpretation.md\n",
      "Johnston - 2000 - Issues for Using Computing and Data Grids for Larg: ontology/obsidian/Johnston - 2000 - Issues for Using Computing and Data Grids for Larg.md\n",
      "Ding - 2001 - A review of ontologies with the Semantic Web in vi: ontology/obsidian/Ding - 2001 - A review of ontologies with the Semantic Web in vi.md\n",
      "Modi - 2002 - Real-time visualization of aerospace simulations u: ontology/obsidian/Modi - 2002 - Real-time visualization of aerospace simulations u.md\n",
      "Knight Jr et al. - 2002 - Rapid Modeling and Analysis Tools Evolution, Stat: ontology/obsidian/Knight Jr et al. - 2002 - Rapid Modeling and Analysis Tools Evolution, Stat.md\n",
      "Nambiar et al. - 2002 - Current approaches to XML management: ontology/obsidian/Nambiar et al. - 2002 - Current approaches to XML management.md\n",
      "Cañas et al. - 2003 - A summary of literature pertaining to the use of c: ontology/obsidian/Cañas et al. - 2003 - A summary of literature pertaining to the use of c.md\n",
      "Chang - 2003 - A knowledge base approach to assisting collaborati: ontology/obsidian/Chang - 2003 - A knowledge base approach to assisting collaborati.md\n",
      "Hawker et al. - 2003 - A web-based process and process models to find and: ontology/obsidian/Hawker et al. - 2003 - A web-based process and process models to find and.md\n",
      "Vargas-Hernandez et al. - 2003 - Development of a computer aided conceptual design : ontology/obsidian/Vargas-Hernandez et al. - 2003 - Development of a computer aided conceptual design.md\n",
      "Vargas-Hernandez und Shah - 2004 - 2nd-CAD a tool for conceptual systems design in e: ontology/obsidian/Vargas-Hernandez und Shah - 2004 - 2nd-CAD a tool for conceptual systems design in e.md\n",
      "Agrawal et al. - 2004 - Web-based visualization framework for decision-mak: ontology/obsidian/Agrawal et al. - 2004 - Web-based visualization framework for decision-mak.md\n",
      "Heyward und Gott - 2004 - Lewis' Educational and Research Collaborative Inte: ontology/obsidian/Heyward und Gott - 2004 - Lewis' Educational and Research Collaborative Inte.md\n",
      "Some et al. - 2004 - XML hierarchical database for missions and technol: ontology/obsidian/Some et al. - 2004 - XML hierarchical database for missions and technol.md\n",
      "Helander und Zhang - 2005 - Web-based configure-to-order platform for collabor: ontology/obsidian/Helander und Zhang - 2005 - Web-based configure-to-order platform for collabor.md\n",
      "Vargas-Hernandez und Shah - 2005 - PRINCIPLES FOR THE DEVELOPMENT OF A COMPUTER AIDED: ontology/obsidian/Vargas-Hernandez und Shah - 2005 - PRINCIPLES FOR THE DEVELOPMENT OF A COMPUTER AIDED.md\n",
      "Charles et al. - 2005 - Standardization of the finite element analysis dat: ontology/obsidian/Charles et al. - 2005 - Standardization of the finite element analysis dat.md\n",
      "Pohl und Vempati - 2005 - A translational web services bridge for meaningful: ontology/obsidian/Pohl und Vempati - 2005 - A translational web services bridge for meaningful.md\n",
      "Karwowski et al. - 2006 - A Review and Reappraisal of Adaptive Human-Compute: ontology/obsidian/Karwowski et al. - 2006 - A Review and Reappraisal of Adaptive Human-Compute.md\n",
      "Jackson und Hildreth - 2006 - Progress Toward a Format Standard for Flight Dynam: ontology/obsidian/Jackson und Hildreth - 2006 - Progress Toward a Format Standard for Flight Dynam.md\n",
      "Murphy - 2006 - A multi-dimensional approach to fault protection i: ontology/obsidian/Murphy - 2006 - A multi-dimensional approach to fault protection i.md\n",
      "Yuan Fu et al. - 2006 - Knowledge identification and management in product: ontology/obsidian/Yuan Fu et al. - 2006 - Knowledge identification and management in product.md\n",
      "Van der Velden et al. - 2007 - An intelligent system for automatic layout routing: ontology/obsidian/Van der Velden et al. - 2007 - An intelligent system for automatic layout routing.md\n",
      "Gurnani und Lewis - 2007 - The Use of Really Simple Syndication (RSS) Feeds f: ontology/obsidian/Gurnani und Lewis - 2007 - The Use of Really Simple Syndication (RSS) Feeds f.md\n",
      "Shelton - 2008 - The use of reciprocal interdependencies management: ontology/obsidian/Shelton - 2008 - The use of reciprocal interdependencies management.md\n",
      "Van der Velden - 2008 - Application of knowledge based engineering princip: ontology/obsidian/Van der Velden - 2008 - Application of knowledge based engineering princip.md\n",
      "Kemény et al. - 2008 - Representation and Navigation Techniques for Semi-: ontology/obsidian/Kemény et al. - 2008 - Representation and Navigation Techniques for Semi-.md\n",
      "Wong et al. - 2008 - Knowledge transfer from maintenance to engine des: ontology/obsidian/Wong et al. - 2008 - Knowledge transfer from maintenance to engine des.md\n",
      "Bochner et al. - 2008 - A Python Library for Provenance Recording and Quer: ontology/obsidian/Bochner et al. - 2008 - A Python Library for Provenance Recording and Quer.md\n",
      "Yan et al. - 2008 - A multi-agent-based semantic collaboration framewo: ontology/obsidian/Yan et al. - 2008 - A multi-agent-based semantic collaboration framewo.md\n",
      "Fu Qiu et al. - 2008 - Cognitive understanding of knowledge processing an: ontology/obsidian/Fu Qiu et al. - 2008 - Cognitive understanding of knowledge processing an.md\n",
      "Böhnke - 2009 - Data integration in preliminary airplane design: ontology/obsidian/Böhnke - 2009 - Data integration in preliminary airplane design.md\n",
      "Dadzie et al. - 2009 - Applying semantic web technologies to knowledge sh: ontology/obsidian/Dadzie et al. - 2009 - Applying semantic web technologies to knowledge sh.md\n",
      "Eickhoff - 2009 - Simulation Tools in a System Engineering Infrastru: ontology/obsidian/Eickhoff - 2009 - Simulation Tools in a System Engineering Infrastru.md\n",
      "Feiler et al. - 2009 - System architecture virtual integration An indust: ontology/obsidian/Feiler et al. - 2009 - System architecture virtual integration An indust.md\n",
      "Swindells - 2009 - The representation and exchange of material and ot: ontology/obsidian/Swindells - 2009 - The representation and exchange of material and ot.md\n",
      "Leutenmayr et al. - 2009 - Passive User Integration in Social Networking Serv: ontology/obsidian/Leutenmayr et al. - 2009 - Passive User Integration in Social Networking Serv.md\n",
      "Koelfgen und Faber - 2010 - Using the Integrated Vehicle Health Management Res: ontology/obsidian/Koelfgen und Faber - 2010 - Using the Integrated Vehicle Health Management Res.md\n",
      "Curran et al. - 2010 - The KNOMAD Methodology for Integration of Multidis: ontology/obsidian/Curran et al. - 2010 - The KNOMAD Methodology for Integration of Multidis.md\n",
      "Vucinic - 2010 - Multidisciplinary visualization aspects in Europea: ontology/obsidian/Vucinic - 2010 - Multidisciplinary visualization aspects in Europea.md\n",
      "Boydston und Lewis - 2010 - Certification challenges of mixed critical cyber-p: ontology/obsidian/Boydston und Lewis - 2010 - Certification challenges of mixed critical cyber-p.md\n",
      "Fairfax - 2010 - SEMANTIC TECHNOLOGY FOR IN℡LIGENCE, DEFENSE AND SE: ontology/obsidian/Fairfax - 2010 - SEMANTIC TECHNOLOGY FOR IN℡LIGENCE, DEFENSE AND SE.md\n",
      "Mastella - 2010 - Semantic exploitation of engineering models appli: ontology/obsidian/Mastella - 2010 - Semantic exploitation of engineering models appli.md\n",
      "Regli et al. - 2010 - Semantics for digital engineering archives support: ontology/obsidian/Regli et al. - 2010 - Semantics for digital engineering archives support.md\n",
      "Kuofie - 2010 - Radex A rationale-based ontology for aerospace de: ontology/obsidian/Kuofie - 2010 - Radex A rationale-based ontology for aerospace de.md\n",
      "Gil et al. - 2010 - Modern approaches in the design of complex Aerospa: ontology/obsidian/Gil et al. - 2010 - Modern approaches in the design of complex Aerospa.md\n",
      "Adala et al. - 2011 - A framework for automatic web service discovery ba: ontology/obsidian/Adala et al. - 2011 - A framework for automatic web service discovery ba.md\n",
      "Sanya et al. - 2011 - Challenges in semantic knowledge management for ae: ontology/obsidian/Sanya et al. - 2011 - Challenges in semantic knowledge management for ae.md\n",
      "Reichwein - 2011 - Application-specific UML profiles for multidiscipl: ontology/obsidian/Reichwein - 2011 - Application-specific UML profiles for multidiscipl.md\n",
      "Jian - 2011 - Knowledge search for new product development a mu: ontology/obsidian/Jian - 2011 - Knowledge search for new product development a mu.md\n",
      "Petrelli et al. - 2011 - Highly focused document retrieval in aerospace eng: ontology/obsidian/Petrelli et al. - 2011 - Highly focused document retrieval in aerospace eng.md\n",
      "Iribarne et al. - 2011 - Open-environmental ontology modeling: ontology/obsidian/Iribarne et al. - 2011 - Open-environmental ontology modeling.md\n",
      "Yi und Hua - 2011 - A method of CADCAE data integration based on XML: ontology/obsidian/Yi und Hua - 2011 - A method of CADCAE data integration based on XML.md\n",
      "Nagel et al. - 2012 - Communication in aircraft design Can we establish: ontology/obsidian/Nagel et al. - 2012 - Communication in aircraft design Can we establish.md\n",
      "Verhagen et al. - 2012 - Knowledge-based cost modelling of composite wing s: ontology/obsidian/Verhagen et al. - 2012 - Knowledge-based cost modelling of composite wing s.md\n",
      "Sajadfar et al. - 2013 - A Review of Data Representation of Product and Pro: ontology/obsidian/Sajadfar et al. - 2013 - A Review of Data Representation of Product and Pro.md\n",
      "Xie - 2013 - Application of context aware systems to support kn: ontology/obsidian/Xie - 2013 - Application of context aware systems to support kn.md\n",
      "Liu et al. - 2013 - A design method with knowledge modeling for comple: ontology/obsidian/Liu et al. - 2013 - A design method with knowledge modeling for comple.md\n",
      "Munjulury - 2014 - Knowledge based integrated multidisciplinary aircr: ontology/obsidian/Munjulury - 2014 - Knowledge based integrated multidisciplinary aircr.md\n",
      "Walker - 2014 - Development Of A Conceptual Design Weight Estimati: ontology/obsidian/Walker - 2014 - Development Of A Conceptual Design Weight Estimati.md\n",
      "Jeeson Daniel - 2014 - An investigation into information reuse for cost p: ontology/obsidian/Jeeson Daniel - 2014 - An investigation into information reuse for cost p.md\n",
      "Li et al. - 2014 - An approach for design rationale retrieval using o: ontology/obsidian/Li et al. - 2014 - An approach for design rationale retrieval using o.md\n",
      "Cristofaro - 2014 - Elements of computational flight dynamics for comp: ontology/obsidian/Cristofaro - 2014 - Elements of computational flight dynamics for comp.md\n",
      "Englehorn - 2014 - Consortium for Robotics and Unmanned Systems Educa: ontology/obsidian/Englehorn - 2014 - Consortium for Robotics and Unmanned Systems Educa.md\n",
      "Sanya und Shehab - 2014 - An ontology framework for developing platform-inde: ontology/obsidian/Sanya und Shehab - 2014 - An ontology framework for developing platform-inde.md\n",
      "Hoogreef et al. - 2015 - A multidisciplinary design optimization advisory s: ontology/obsidian/Hoogreef et al. - 2015 - A multidisciplinary design optimization advisory s.md\n",
      "Wemembu et al. - 2015 - Knowledge-Based Management System and Dearth of Fl: ontology/obsidian/Wemembu et al. - 2015 - Knowledge-Based Management System and Dearth of Fl.md\n",
      "Guo - 2015 - A Distributed Test Facility for Cyber-Physical Sys: ontology/obsidian/Guo - 2015 - A Distributed Test Facility for Cyber-Physical Sys.md\n",
      "Tan - 2015 - Contribution to t he Onboard Context Sensitive Inf: ontology/obsidian/Tan - 2015 - Contribution to t he Onboard Context Sensitive Inf.md\n",
      "Deokattey et al. - 2015 - Knowledge Organization Research An overview: ontology/obsidian/Deokattey et al. - 2015 - Knowledge Organization Research An overview.md\n",
      "Ramakers - 2015 - Accelerating aircraft de-sign using automated proc: ontology/obsidian/Ramakers - 2015 - Accelerating aircraft de-sign using automated proc.md\n",
      "Reddy et al. - 2015 - Knowledge based engineering notion, approaches an: ontology/obsidian/Reddy et al. - 2015 - Knowledge based engineering notion, approaches an.md\n",
      "Li et al. - 2015 - A smart component data model in PLM: ontology/obsidian/Li et al. - 2015 - A smart component data model in PLM.md\n",
      "Gonzalez - 2016 - Complex multidisciplinary system composition for a: ontology/obsidian/Gonzalez - 2016 - Complex multidisciplinary system composition for a.md\n",
      "Cornford und Feather - 2016 - Model Based Mission Assurance in a Model Based Sys: ontology/obsidian/Cornford und Feather - 2016 - Model Based Mission Assurance in a Model Based Sys.md\n",
      "Haney - 2016 - Data Engineering in Aerospace Systems Design & For: ontology/obsidian/Haney - 2016 - Data Engineering in Aerospace Systems Design & For.md\n",
      "Solanki et al. - 2017 - Ontology-Driven Unified Governance in Software Eng: ontology/obsidian/Solanki et al. - 2017 - Ontology-Driven Unified Governance in Software Eng.md\n",
      "Pandey und Pandey - 2017 - JSON and its use in Semantic Web: ontology/obsidian/Pandey und Pandey - 2017 - JSON and its use in Semantic Web.md\n",
      "Ren et al. - 2017 - Small Unmanned Aircraft System (sUAS) Trajectory M: ontology/obsidian/Ren et al. - 2017 - Small Unmanned Aircraft System (sUAS) Trajectory M.md\n",
      "Virós Martin - 2017 - Design and development of a cognitive assistant fo: ontology/obsidian/Virós Martin - 2017 - Design and development of a cognitive assistant fo.md\n",
      "Van Gent et al. - 2017 - Knowledge architecture supporting collaborative MD: ontology/obsidian/Van Gent et al. - 2017 - Knowledge architecture supporting collaborative MD.md\n",
      "Van Gent et al. - 2017 - Composing MDAO symphonies graph-based generation : ontology/obsidian/Van Gent et al. - 2017 - Composing MDAO symphonies graph-based generation.md\n",
      "Rovetto - 2017 - Ontology-based knowledge management for space data: ontology/obsidian/Rovetto - 2017 - Ontology-based knowledge management for space data.md\n",
      "Zhai et al. - 2017 - Research on Knowledge Network Modelling for Aero-c: ontology/obsidian/Zhai et al. - 2017 - Research on Knowledge Network Modelling for Aero-c.md\n",
      "Qin et al. - 2017 - Status, Comparison, and Issues of Computer-Aided D: ontology/obsidian/Qin et al. - 2017 - Status, Comparison, and Issues of Computer-Aided D.md\n",
      "Zhang - 2018 - A knowledge enriched computational model to suppor: ontology/obsidian/Zhang - 2018 - A knowledge enriched computational model to suppor.md\n",
      "Slezak et al. - 2018 - Virtual reality application for enhancing risk ass: ontology/obsidian/Slezak et al. - 2018 - Virtual reality application for enhancing risk ass.md\n",
      "Ali und Mohamed - 2018 - A framework for visualizing heterogeneous construc: ontology/obsidian/Ali und Mohamed - 2018 - A framework for visualizing heterogeneous construc.md\n",
      "Jeong - 2018 - A study on the BIM evaluation, analytics, and pred: ontology/obsidian/Jeong - 2018 - A study on the BIM evaluation, analytics, and pred.md\n",
      "Baalbergen et al. - 2018 - Integrated collaboration capabilities for competit: ontology/obsidian/Baalbergen et al. - 2018 - Integrated collaboration capabilities for competit.md\n",
      "Van Gent et al. - 2018 - A Critical Look at Design Automation Solutions for: ontology/obsidian/Van Gent et al. - 2018 - A Critical Look at Design Automation Solutions for.md\n",
      "Asare - 2018 - An integrated product, process and resource modeli: ontology/obsidian/Asare - 2018 - An integrated product, process and resource modeli.md\n",
      "Gutierrez - 2018 - A requirement ontology to guide the analysis of sy: ontology/obsidian/Gutierrez - 2018 - A requirement ontology to guide the analysis of sy.md\n",
      "Hedberg Jr - 2018 - Enabling connections in the product lifecycle usin: ontology/obsidian/Hedberg Jr - 2018 - Enabling connections in the product lifecycle usin.md\n",
      "Li - 2018 - A smart products lifecycle management (sPLM) frame: ontology/obsidian/Li - 2018 - A smart products lifecycle management (sPLM) frame.md\n",
      "Herbst - 2018 - Development of an aircraft design environment usin: ontology/obsidian/Herbst - 2018 - Development of an aircraft design environment usin.md\n",
      "Aigner et al. - 2018 - Graph-based algorithms and data-driven documents f: ontology/obsidian/Aigner et al. - 2018 - Graph-based algorithms and data-driven documents f.md\n",
      "Wang et al. - 2018 - Knowledge representation and reasoning methods in : ontology/obsidian/Wang et al. - 2018 - Knowledge representation and reasoning methods in.md\n",
      "Bebawi - 2019 - FMI Based Spacecraft Simulation Along Life Cycle P: ontology/obsidian/Bebawi - 2019 - FMI Based Spacecraft Simulation Along Life Cycle P.md\n",
      "Smith et al. - 2019 - The GENUS aircraft conceptual design environment: ontology/obsidian/Smith et al. - 2019 - The GENUS aircraft conceptual design environment.md\n",
      "Kibret - 2019 - A Formal Systems Engineering Methodology for Cyber: ontology/obsidian/Kibret - 2019 - A Formal Systems Engineering Methodology for Cyber.md\n",
      "Rossoni et al. - 2019 - Integration of virtual reality in a knowledge-base: ontology/obsidian/Rossoni et al. - 2019 - Integration of virtual reality in a knowledge-base.md\n",
      "van Gent - 2019 - Agile MDAO systems a graph-based methodology to e: ontology/obsidian/van Gent - 2019 - Agile MDAO systems a graph-based methodology to e.md\n",
      "Kang et al. - 2019 - Automated feedback generation for formal manufactu: ontology/obsidian/Kang et al. - 2019 - Automated feedback generation for formal manufactu.md\n",
      "Giovingo - 2019 - RAMS and Maintenance cost assessment in a Multidis: ontology/obsidian/Giovingo - 2019 - RAMS and Maintenance cost assessment in a Multidis.md\n",
      "Berquand et al. - 2019 - Artificial intelligence for the early design phase: ontology/obsidian/Berquand et al. - 2019 - Artificial intelligence for the early design phase.md\n",
      "Kossmann et al. - 2020 - Extending the scope of configuration management fo: ontology/obsidian/Kossmann et al. - 2020 - Extending the scope of configuration management fo.md\n",
      "Zhao et al. - 2020 - Application of knowledge engineering in spacecraft: ontology/obsidian/Zhao et al. - 2020 - Application of knowledge engineering in spacecraft.md\n",
      "Mangortey et al. - 2020 - Prediction and Analysis of Ground Stops with Machi: ontology/obsidian/Mangortey et al. - 2020 - Prediction and Analysis of Ground Stops with Machi.md\n",
      "Mangortey et al. - 2020 - Application of Machine Learning to the Analysis an: ontology/obsidian/Mangortey et al. - 2020 - Application of Machine Learning to the Analysis an.md\n",
      "Vajda - 2020 - An Ontological Approach to Autonomous Navigational: ontology/obsidian/Vajda - 2020 - An Ontological Approach to Autonomous Navigational.md\n",
      "Pauly - 2020 - DELIVERABLE D4. 2: ontology/obsidian/Pauly - 2020 - DELIVERABLE D4. 2.md\n",
      "Rovetto et al. - 2020 - Orbital debris ontology, terminology, and knowledg: ontology/obsidian/Rovetto et al. - 2020 - Orbital debris ontology, terminology, and knowledg.md\n",
      "Shobowale et al. - 2021 - Impact of ontology in aviation incident and accide: ontology/obsidian/Shobowale et al. - 2021 - Impact of ontology in aviation incident and accide.md\n",
      "Hao und Hu - 2021 - Smart Design on the Flexible Gear based on Knowled: ontology/obsidian/Hao und Hu - 2021 - Smart Design on the Flexible Gear based on Knowled.md\n",
      "De Leon - 2021 - Intelligent Data Understanding for Entry, Descent,: ontology/obsidian/De Leon - 2021 - Intelligent Data Understanding for Entry, Descent,.md\n",
      "Berquand - 2021 - Text mining and natural language processing for th: ontology/obsidian/Berquand - 2021 - Text mining and natural language processing for th.md\n",
      "Roelofs und Vos - 2021 - Automatically inferring technology compatibility w: ontology/obsidian/Roelofs und Vos - 2021 - Automatically inferring technology compatibility w.md\n",
      "Hu et al. - 2021 - Digital twin A state-of-the-art review of its ena: ontology/obsidian/Hu et al. - 2021 - Digital twin A state-of-the-art review of its ena.md\n",
      "Mccall - 2021 - Automating Aerospace Synthesis Code Generation A T: ontology/obsidian/Mccall - 2021 - Automating Aerospace Synthesis Code Generation A T.md\n",
      "Torres et al. - 2021 - A systematic literature review of cross-domain mod: ontology/obsidian/Torres et al. - 2021 - A systematic literature review of cross-domain mod.md\n",
      "Crawford - 2021 - A progressive learning framework, leveraging machi: ontology/obsidian/Crawford - 2021 - A progressive learning framework, leveraging machi.md\n",
      "Jeyaraj et al. - 2021 - Connecting Model-based Systems Engineering and Mul: ontology/obsidian/Jeyaraj et al. - 2021 - Connecting Model-based Systems Engineering and Mul.md\n",
      "Chen et al. - 2021 - Cloud-Based Environment for Aircraft Design Collab: ontology/obsidian/Chen et al. - 2021 - Cloud-Based Environment for Aircraft Design Collab.md\n",
      "Demirhan - 2022 - Identification of the abnormal fuel consumption in: ontology/obsidian/Demirhan - 2022 - Identification of the abnormal fuel consumption in.md\n",
      "Procko und Ochoa - 2022 - Leveraging Linked Data for Knowledge Management A: ontology/obsidian/Procko und Ochoa - 2022 - Leveraging Linked Data for Knowledge Management A.md\n",
      "Royal - 2022 - Advancing Cross-Organizational Collaboration in Ai: ontology/obsidian/Royal - 2022 - Advancing Cross-Organizational Collaboration in Ai.md\n",
      "Procko und Ochoa - 2022 - An Ontology Based Approach to the Software Enginee: ontology/obsidian/Procko und Ochoa - 2022 - An Ontology Based Approach to the Software Enginee.md\n",
      "Markusheska et al. - 2022 - Implementing a system architecture model for autom: ontology/obsidian/Markusheska et al. - 2022 - Implementing a system architecture model for autom.md\n",
      "Flink et al. - 2022 - Orchestrating tool chains for model-based systems : ontology/obsidian/Flink et al. - 2022 - Orchestrating tool chains for model-based systems.md\n",
      "Halvorson et al. - 2022 - An Ontology for Prognostic Health Management in Sp: ontology/obsidian/Halvorson et al. - 2022 - An Ontology for Prognostic Health Management in Sp.md\n",
      "Herrero - 2022 - A System Architecture for the Digital Thread in Co: ontology/obsidian/Herrero - 2022 - A System Architecture for the Digital Thread in Co.md\n",
      "Jeyaraj und Liscouët-Hanke - 2022 - A Safety-Focused System Architecting Framework for: ontology/obsidian/Jeyaraj und Liscouët-Hanke - 2022 - A Safety-Focused System Architecting Framework for.md\n",
      "Meng et al. - 2023 - Fault Knowledge Graph Construction and Platform De: ontology/obsidian/Meng et al. - 2023 - Fault Knowledge Graph Construction and Platform De.md\n",
      "Niu et al. - 2023 - A hybrid methodology for knowledge organization an: ontology/obsidian/Niu et al. - 2023 - A hybrid methodology for knowledge organization an.md\n",
      "Thangavel - 2023 - Trusted autonomous operations of distributed satel: ontology/obsidian/Thangavel - 2023 - Trusted autonomous operations of distributed satel.md\n",
      "Zhang et al. - 2023 - Aic an industrial knowledge graph with Abstractio: ontology/obsidian/Zhang et al. - 2023 - Aic an industrial knowledge graph with Abstractio.md\n",
      "Bussemaker et al. - 2023 - The AGILE 4.0 Project MBSE to Support Cyber‐Physi: ontology/obsidian/Bussemaker et al. - 2023 - The AGILE 4.0 Project MBSE to Support Cyber‐Physi.md\n",
      "Zhu et al. - 2023 - A Survey of Advanced Information Fusion System fr: ontology/obsidian/Zhu et al. - 2023 - A Survey of Advanced Information Fusion System fr.md\n",
      "Padilha - 2023 - Enabling fuel system component placement on the ba: ontology/obsidian/Padilha - 2023 - Enabling fuel system component placement on the ba.md\n",
      "Kim et al. - 2023 - Design for additive manufacturing knowledgebase de: ontology/obsidian/Kim et al. - 2023 - Design for additive manufacturing knowledgebase de.md\n",
      "Adam et al. - 2023 - An Open and Customizable Software Suite for System: ontology/obsidian/Adam et al. - 2023 - An Open and Customizable Software Suite for System.md\n",
      "Kwakye et al. - 2024 - Platform health management for aircraft maintenanc: ontology/obsidian/Kwakye et al. - 2024 - Platform health management for aircraft maintenanc.md\n",
      "Mihaylov et al. - A Space Conversational Agent for Retrieving Lesson: ontology/obsidian/Mihaylov et al. - A Space Conversational Agent for Retrieving Lesson.md\n",
      "Faris et al. - A SEMANTIC KNOWLEDGE SEARCH PLATFORM FOR AEROSPACE: ontology/obsidian/Faris et al. - A SEMANTIC KNOWLEDGE SEARCH PLATFORM FOR AEROSPACE.md\n",
      "Saluz und Geyer - Semio A meta-model for formalizing reusable param: ontology/obsidian/Saluz und Geyer - Semio A meta-model for formalizing reusable param.md\n",
      "Zhao - The Role of Information, Knowledge and Decision in: ontology/obsidian/Zhao - The Role of Information, Knowledge and Decision in.md\n",
      "Franzén et al. - ONTOLOGY-ASSISTED AIRCRAFT CONCEPT GENERATION: ontology/obsidian/Franzén et al. - ONTOLOGY-ASSISTED AIRCRAFT CONCEPT GENERATION.md\n",
      "Ezhilarasu - A framework for aerospace vehicle reasoning (FAVER: ontology/obsidian/Ezhilarasu - A framework for aerospace vehicle reasoning (FAVER.md\n",
      "Bruni et al. - The Assisted Experimental Designer A Decision Sup: ontology/obsidian/Bruni et al. - The Assisted Experimental Designer A Decision Sup.md\n",
      "process: ontology/obsidian/process.md\n",
      "software: ontology/obsidian/software.md\n",
      "data item: ontology/obsidian/data item.md\n",
      "data model: ontology/obsidian/data model.md\n",
      "data format specification: ontology/obsidian/data format specification.md\n",
      "paper: ontology/obsidian/paper.md\n"
     ]
    }
   ],
   "source": [
    "# Obsidian\n",
    "\n",
    "\n",
    "def fill_digits(var_s, cap=2):\n",
    "    \"\"\"Add digits until cap is reached\"\"\"\n",
    "    var_s = str(var_s)\n",
    "    while len(var_s) < cap:\n",
    "        var_s = \"0\" + var_s\n",
    "    return var_s\n",
    "\n",
    "\n",
    "def get_clean_title(title, eID=None, obsidian=False):\n",
    "    \"\"\"\n",
    "    Cleans a given title to make it suitable for filenames.\n",
    "\n",
    "    Parameters:\n",
    "    title (string): The title. \"episode1\"\n",
    "    eID (int): The position of that episode in the playlist.\n",
    "\n",
    "    Returns:\n",
    "    string: cleaned title for use as filename\n",
    "    \"\"\"\n",
    "    noFileChars = r'\":\\<>*?/'\n",
    "\n",
    "    replace_dict = {\n",
    "        \"–\": \"-\",\n",
    "        \"’\": \"´\",\n",
    "        \" \": \" \",\n",
    "    }\n",
    "    clean_title = title\n",
    "    for each in noFileChars:\n",
    "        clean_title = clean_title.replace(each, \"\")\n",
    "    for key, value in replace_dict.items():\n",
    "        clean_title = clean_title.replace(key, value)\n",
    "    if obsidian:\n",
    "        clean_title = clean_title.replace(\"#\", \"\")\n",
    "    if eID:\n",
    "        clean_title = fill_digits(eID, 3) + \"_\" + clean_title\n",
    "    return clean_title.strip()\n",
    "\n",
    "\n",
    "def clear_name(name, can_be_folder=True):\n",
    "    if can_be_folder:\n",
    "        clean_title = name.replace(\"/\", \"[SLASH]\")\n",
    "    else:\n",
    "        clean_title = name.replace(\"/\", \" or \")\n",
    "    clean_title = get_clean_title(clean_title, obsidian=True)\n",
    "    if can_be_folder:\n",
    "        clean_title = clean_title.replace(\"[SLASH]\", \"/\")\n",
    "    if clean_title.startswith(\"https\"):\n",
    "        clean_title = clean_title[5:]\n",
    "    while clean_title.startswith(\"/\"):\n",
    "        clean_title = clean_title[1:]\n",
    "    if clean_title.startswith(\"www.\"):\n",
    "        clean_title = clean_title[4:]\n",
    "    while clean_title.endswith(\"/\"):\n",
    "        clean_title = clean_title[:-1]\n",
    "    return clean_title\n",
    "\n",
    "\n",
    "def parse_link(text):\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1]\n",
    "    if text.startswith(\"[[\") and text.endswith(\"]]\"):\n",
    "        text = text[2:-2]\n",
    "    return text\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self, instance: KnowledgeGraphEntry = None, path: str = None, content: str = None\n",
    "    ):\n",
    "        self.instance: KnowledgeGraphEntry = instance\n",
    "        self.path = path if path else self.instance.get(\"path\", None) if self.instance else None\n",
    "        self.content = content if content else self.instance.get(\"content\", None) if self.instance else None\n",
    "\n",
    "        if path and not instance:\n",
    "            self.load()\n",
    "\n",
    "    def get(self, key, default=None):\n",
    "        if hasattr(self, key):\n",
    "            return getattr(self, key)\n",
    "        elif hasattr(self.instance, key):\n",
    "            return self.instance.get(key)\n",
    "        elif key == \"properties\":\n",
    "            return self.instance.__dict__\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "    def load(self, path=None):\n",
    "        if not path:\n",
    "            path = self.path\n",
    "        with open(self.path, \"r\", encoding=\"utf8\") as f:\n",
    "            content = f.read()\n",
    "            # read properties from yaml frontmatter\n",
    "            yaml_find = 0\n",
    "            lines = content.splitlines()\n",
    "            properties = {}\n",
    "            key = None\n",
    "            value = None\n",
    "            while lines:\n",
    "                line = lines.pop(0)\n",
    "                if line.startswith(\"---\"):\n",
    "                    # yaml frontmatter either begins or ends\n",
    "                    yaml_find += 1\n",
    "\n",
    "                elif yaml_find == 1:\n",
    "                    # inside yaml frontmatter\n",
    "                    # if \":\" in line:\n",
    "                    if not line.strip().startswith(\"-\") and \":\" in line:\n",
    "                        if key:\n",
    "                            properties[key] = value\n",
    "                        pieces = line.split(\":\")\n",
    "                        key = pieces[0].strip()\n",
    "                        value = \":\".join(pieces[1:]).strip()\n",
    "                        value = parse_link(value)\n",
    "                    else:\n",
    "                        # multiline value\n",
    "                        if not isinstance(value, list):\n",
    "                            if value:\n",
    "                                value = [value]\n",
    "                            else:\n",
    "                                value = []\n",
    "\n",
    "                        prefix = \" - \"\n",
    "                        line = line.replace(\n",
    "                            prefix, \"\"\n",
    "                        ).strip()  # TODO: remove [[ ]] from links\n",
    "                        line = parse_link(line)\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        else:\n",
    "                            value.append(line)\n",
    "\n",
    "                elif yaml_find == 2:\n",
    "                    # end of yaml frontmatter\n",
    "                    if key:\n",
    "                        properties[key] = value\n",
    "                    self.content = \"\\n\".join(lines)\n",
    "                    break\n",
    "            if properties:\n",
    "                self.instance = KnowledgeGraphEntryFactory.create(properties = properties)\n",
    "\n",
    "    def prep_for_yaml(self, text):\n",
    "        # this needs to be improved\n",
    "        mapping = {\n",
    "            # \"\\&\": \"&\",\n",
    "            \"\\\\&\": \"&\",\n",
    "            '\"': \"'quotationmark'\",\n",
    "            \"\\n\": \" 'newline' \",\n",
    "        }\n",
    "        if \"\\\\\" in text and \"/\" in text:\n",
    "            text = path_cleaning(text)\n",
    "        for key, value in mapping.items():\n",
    "            text = text.replace(key, value)\n",
    "        \n",
    "        if not text.startswith('\"') and not text.endswith('\"'):\n",
    "            text = '\"{}\"'.format(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def save(self, path=None):\n",
    "        # lists = [\n",
    "        #     \"layman term\",\n",
    "        #     \"subclass of\",\n",
    "        #     \"instance of\",\n",
    "        #     \"aliases\",\n",
    "        #     \"tags\",\n",
    "        #     \"wikidata candidates\",\n",
    "        #     \"orkg candidates\",\n",
    "        # ]\n",
    "        # data = self.convert_python_to_obsidian()\n",
    "        # if data:\n",
    "        if not path:\n",
    "            path = self.path\n",
    "        data = {}\n",
    "        for key, value in self.instance.__dict__.items():\n",
    "            # if key in [\"path\", \"content\"]:\n",
    "            #     continue\n",
    "            if value == None:\n",
    "                value = \"\"\n",
    "            # if key == \"wikidata ID\" and isinstance(value, list):\n",
    "            #     data[\"wikidata candidates\"] = value\n",
    "            #     value = \"\"\n",
    "            # elif key in lists:\n",
    "            #     if not isinstance(value, list):\n",
    "            #         value = [value]\n",
    "            # if key == \"instance of\":\n",
    "            if isinstance(value, str):\n",
    "                # if \"{\" in value or \"}\" in value:\n",
    "                value = self.prep_for_yaml(value)\n",
    "\n",
    "            if isinstance(value, list):\n",
    "                # TODO: improve this lookup\n",
    "                if key in [\"instance_of\", \"subclass_of\"]:\n",
    "                    value = ['\"[[{}]]\"'.format(val) for val in value]\n",
    "                value = \"\\n - \" + \"\\n - \".join(value)\n",
    "            elif isinstance(value, dict):\n",
    "                # value = \"\\n\".join([f\" - {k}: {v}\" for k, v in value.items()])\n",
    "                value = json.dumps(value)\n",
    "            data[key] = value\n",
    "\n",
    "        with open(path, \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(\"---\\n\")\n",
    "            for key, value in data.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "            f.write(\"---\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            if self.content:\n",
    "                self.content.strip()\n",
    "                while self.content.startswith(\"\\n\"):\n",
    "                    self.content = self.content[1:]\n",
    "                while self.content.endswith(\"\\n\"):\n",
    "                    self.content = self.content[:-1]\n",
    "                f.write(self.content)\n",
    "                f.write(\"\\n\")\n",
    "        return True\n",
    "\n",
    "\n",
    "class ObsidianFolderBuilder(Builder):\n",
    "    \"\"\"\n",
    "    Represents the Obsidian readable representation of our ontology.\n",
    "    Structure:\n",
    "    /templates # contains templates for the different node types\n",
    "        - Instance.md\n",
    "        - Class.md\n",
    "    And then the actual nodes, each accoring to their type specified in templates\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, director: Director, classes={}, instances={}):\n",
    "        super().__init__(director)\n",
    "\n",
    "        self.path = (\n",
    "            self.config.obsidian_path\n",
    "            if self.config and hasattr(self.config, \"obsidian_path\")\n",
    "            else None\n",
    "        )\n",
    "        self.templates: dict[str, Node] = {}\n",
    "        self.nodes: dict[str, Node] = {}\n",
    "\n",
    "        self.classes: dict[str, Instance] = {}\n",
    "        self.instances: dict[str, Instance] = {}\n",
    "        self.papers: dict[str, Instance] = {}\n",
    "        # self.other_nodes: dict[str, Node] = {}\n",
    "\n",
    "        if self.path:\n",
    "            self.load(self.path)\n",
    "\n",
    "    def build(self):\n",
    "        self.populate(force=True)\n",
    "        self.save()\n",
    "\n",
    "    def load(self, path):\n",
    "        self.path = path\n",
    "        templates_path = os.path.join(path, \"templates\")\n",
    "        for file in os.listdir(templates_path):\n",
    "            if file.endswith(\".md\"):\n",
    "                filepath = os.path.join(templates_path, file)\n",
    "                self.templates[file[:-3]] = Node(path=filepath)\n",
    "\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\".md\"):\n",
    "                filepath = os.path.join(path, file)\n",
    "                node = Node(path=filepath)\n",
    "                try:\n",
    "                    if \"class\" in node.instance.tags or hasattr(node.instance, \"subclass of\"):\n",
    "                        self.classes[node.instance.label] = node.instance\n",
    "                    elif \"instance\" in node.instance.tags or hasattr(\n",
    "                        node.instance, \"instance of\"\n",
    "                    ):\n",
    "                        if \"paper\" in node.instance.instance_of:\n",
    "                            self.papers[node.instance.label] = node.instance\n",
    "                        else:\n",
    "                            self.instances[node.instance.label] = node.instance\n",
    "                    else:\n",
    "                        self.other_nodes[node.instance.label] = node.instance\n",
    "                except:\n",
    "                    # self.other_nodes[node.name] = node\n",
    "                    print(f\"Error: Could not determine type of node {filepath}\")\n",
    "\n",
    "        print(\"Loaded Obsidian folder:\")\n",
    "        print(\n",
    "            \"number of tempaltes: \"\n",
    "            + str(len(self.templates))\n",
    "            + \" (\"\n",
    "            + \", \".join(self.templates.keys())\n",
    "            + \")\"\n",
    "        )\n",
    "        print(\"number of classes: \" + str(len(self.classes)))\n",
    "        print(\"number of instances: \" + str(len(self.instances)))\n",
    "        # print(\"number of other nodes: \" + str(len(self.other_nodes)))\n",
    "\n",
    "    def save(self, path=None, overwrite_templates=False):\n",
    "        if not path:\n",
    "            path = self.path\n",
    "        templates_path = os.path.join(path, \"templates\")\n",
    "        if not os.path.exists(templates_path):\n",
    "            os.makedirs(templates_path)\n",
    "\n",
    "        paths = []\n",
    "\n",
    "        if overwrite_templates:\n",
    "            for template in self.templates.values():\n",
    "                template.save()\n",
    "\n",
    "        for node in self.nodes.values():\n",
    "            node.save()\n",
    "            print(node.instance.label + \": \" + node.path)\n",
    "            if node.path not in paths:\n",
    "                paths.append(node.path)\n",
    "            else:\n",
    "                Warning(f\"Error: Duplicate path {node.path}\")\n",
    "\n",
    "    # def obsidify_property(self, text:str):\n",
    "    #     mapping = {\n",
    "    #         \"_\": \" \",\n",
    "    #     }\n",
    "    #     for key, value in mapping.items():\n",
    "    #         text = text.replace(key, value)\n",
    "    #     return text\n",
    "\n",
    "    def properties_from_template(self, template_name, node: Node):\n",
    "        # print(node)\n",
    "        template = self.templates.get(template_name, {})\n",
    "        if not template:\n",
    "            return\n",
    "\n",
    "        for key, value in template.__dict__.items():\n",
    "            # node properties\n",
    "            if not hasattr(node, key) or not getattr(node, key):\n",
    "                setattr(node, key, value)\n",
    "\n",
    "        # properties.update(template.instance.get(\"properties\", {}))\n",
    "\n",
    "        properties = {}\n",
    "        properties.update(template.instance.__dict__)\n",
    "        for key, value in node.instance.__dict__.items():\n",
    "            # key = self.obsidify_property(key)\n",
    "            if key not in properties or not properties[key]:\n",
    "                properties[key] = value\n",
    "            elif not value:\n",
    "                continue\n",
    "            else:\n",
    "                # both exist, try to merge\n",
    "                # print(f\"Merging {key} ({value}, {properties[key]}) for {name}\")\n",
    "                if isinstance(value, list):\n",
    "                    for entry in value:\n",
    "                        entry = entry.strip()\n",
    "                        if entry not in properties[key]:\n",
    "                            properties[key].append(entry)\n",
    "\n",
    "                    if key == \"tags\":\n",
    "                        added = []\n",
    "                        to_delete = []\n",
    "                        for listID, entry in enumerate(properties[key]):\n",
    "                            entry = entry.strip()\n",
    "                            if entry.startswith(\"#\"):\n",
    "                                entry = entry[1:]\n",
    "                            if entry in added:\n",
    "                                to_delete.append(listID)\n",
    "                            properties[key][listID] = entry\n",
    "                            added.append(entry)\n",
    "                        for listID in to_delete:\n",
    "                            properties[key].pop(listID)\n",
    "\n",
    "                elif isinstance(value, dict):\n",
    "                    properties[key].update(value)\n",
    "                else:\n",
    "                    name = getattr(node, \"name\", getattr(node, \"label\", \"unknown name\"))\n",
    "                    Warning(\n",
    "                        f\"Error: Could not merge properties {key} ({value}, {properties[key]}) for {name}\"\n",
    "                    )\n",
    "        node.instance.set_properties(properties)\n",
    "\n",
    "    def add_nodes(\n",
    "        self, instances: dict[str, Instance], template_name=\"Instance\", force=False\n",
    "    ):\n",
    "        for name, instance in instances.items():\n",
    "            if name not in self.nodes or force:\n",
    "                filepath = os.path.join(\n",
    "                    self.path, f\"{clear_name(name, can_be_folder = False)}.md\"\n",
    "                )\n",
    "                node = Node(instance, filepath)\n",
    "                self.properties_from_template(template_name, node)\n",
    "                self.nodes[name] = node\n",
    "\n",
    "    def populate(self, instances={}, classes={}, force=False):\n",
    "        self.instances.update(instances)\n",
    "        self.classes.update(classes)\n",
    "\n",
    "        print(\"Populating Obsidian folder\")\n",
    "        self.add_nodes(self.instances, template_name=\"Instance\", force=force)\n",
    "        self.add_nodes(self.papers, template_name=\"Instance\", force=force)\n",
    "        self.add_nodes(self.classes, template_name=\"Class\", force=force)\n",
    "\n",
    "        # if force:\n",
    "        #     print(\"Forcing overwrite of all nodes\")\n",
    "        #     self.classes = {}\n",
    "        #     self.instances = {}\n",
    "        #     self.other_nodes = {}\n",
    "        # for name, instance in instances.items():\n",
    "        #     if name not in self.instances:\n",
    "        #         filepath = os.path.join(\n",
    "        #             self.path, f\"{clear_name(name, can_be_folder = False)}.md\"\n",
    "        #         )\n",
    "        #         node = Node(instance, filepath)\n",
    "        #         self.properties_from_template(\"Instance\", node)\n",
    "        #         self.instances[name] = instance\n",
    "        #         self.nodes[name] = node\n",
    "        # for name, instance_type in classes.items():\n",
    "        #     if name not in self.classes:\n",
    "        #         filepath = os.path.join(\n",
    "        #             self.path, f\"{clear_name(name, can_be_folder = False)}.md\"\n",
    "        #         )\n",
    "        #         properties = self.properties_from_template(\"Class\", instance_type)\n",
    "        #         self.classes[name] = Node(filepath, properties)\n",
    "        print(\n",
    "            \"Added \"\n",
    "            + str(len(self.instances))\n",
    "            + \" instances and \"\n",
    "            + str(len(self.classes))\n",
    "            + \" classes to Obsidian folder\"\n",
    "        )\n",
    "\n",
    "        # def __init__(self, path, properties={}):\n",
    "\n",
    "    #     self.path = path\n",
    "    #     self.content = properties.get(\"content\", \"\")\n",
    "    #     self.set_properties(properties)\n",
    "\n",
    "    # def get(self, label=\"\", default={}):\n",
    "    #     if not label:\n",
    "    #         return self.__dict__\n",
    "    #     if label == \"properties\":\n",
    "    #         return {\n",
    "    #             key: value\n",
    "    #             for key, value in self.__dict__.items()\n",
    "    #             if key not in [\"path\"]\n",
    "    #         }\n",
    "    #     elif label in self.__dict__:\n",
    "    #         return self.__dict__[label]\n",
    "    #     else:\n",
    "    #         return default\n",
    "\n",
    "    # def set_properties(self, properties={}, force=False):\n",
    "    #     if properties:\n",
    "    #         # properties = self.convert_python_to_obsidian(properties)\n",
    "    #         for key, value in properties.items():\n",
    "    #             if hasattr(self, key):\n",
    "    #                 existing = getattr(self, key)\n",
    "    #                 if existing != value:\n",
    "    #                     if not force:\n",
    "    #                         print(f\"Preserving existing value for {key}: {existing}\")\n",
    "    #                         continue\n",
    "    #                     else:\n",
    "    #                         print(f\"Overwriting existing value for {key}: {existing}\")\n",
    "    #             setattr(self, key, value)\n",
    "    #     else:\n",
    "    #         try:\n",
    "    #             self.load()\n",
    "    #         except:\n",
    "    #             pass\n",
    "\n",
    "    # def convert_python_to_obsidian(self, input={}, back_to_python=False):\n",
    "    #     if not input:\n",
    "    #         input = self.__dict__\n",
    "\n",
    "    #     forbidden = [\"path\", \"content\"]\n",
    "    #     data = {k: v for k, v in input.items() if k not in forbidden}\n",
    "\n",
    "    # data = {}\n",
    "    # for key, value in input.items():\n",
    "    #     if key == \"path\" or key == \"content\":\n",
    "    #         continue\n",
    "\n",
    "    # for key, value in Node.obsidian_python_property_map.items():\n",
    "    #     if key == \"path\" or key == \"content\" or hasattr(Node, key):\n",
    "    #         continue\n",
    "\n",
    "    #     # pos_key = key if not back_to_python else value\n",
    "    #     # if isinstance(value, dict):\n",
    "    #     #     if \"instance of\" in input and input[\"instance of\"] == key:\n",
    "    #     #         for subkey, subvalue in value.items():\n",
    "    #     #             pos_key = subkey if not back_to_python else subvalue\n",
    "    #     #             if subvalue in input:\n",
    "    #     #                 data[pos_key] = input[subvalue]\n",
    "    #     #             elif subkey in input:\n",
    "    #     #                 data[pos_key] = input[subkey]\n",
    "    #     #             else:\n",
    "    #     #                 data[pos_key] = None\n",
    "    #     # elif value in input:\n",
    "    #     #     data[pos_key] = input[value]\n",
    "    #     # elif key in input:\n",
    "    #     #     data[pos_key] = input[key]\n",
    "    #     # else:\n",
    "    #     #     data[pos_key] = None\n",
    "    # if data[\"instance of\"]:\n",
    "    #     # is an instance, delete subclass attribute\n",
    "    #     del data[\"subclass of\"]\n",
    "    # else:\n",
    "    #     # is a class, delete instance attribute\n",
    "    #     del data[\"instance of\"]\n",
    "\n",
    "    # return data\n",
    "\n",
    "\n",
    "director.build_obsidian_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Setup done.\n",
    "Proceeding to:\n",
    "\n",
    "## Matrix calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free unneeded memory\n",
    "# del paper_nlp_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# run_debug_test(config, instances, papers, paper_instance_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "director.setup_occurence_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_debug_test(config, literals, papers, paper_instance_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all text files\n",
    "def get_paper_full_text(directory):\n",
    "    paper_full_text = {}\n",
    "    for folder in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    paper_full_text[file[:-4]] = file_path\n",
    "                    break\n",
    "\n",
    "    return paper_full_text\n",
    "\n",
    "\n",
    "paper_full_text = get_paper_full_text(\n",
    "    \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/00_PDFs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: find occurrences of instances in full text of papers\n",
    "import sys\n",
    "from bisect import bisect_left\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "class PosInPaper:\n",
    "    def __init__(self):\n",
    "        # List of paper identifiers\n",
    "        self.papers = []\n",
    "        # List of literals\n",
    "        self.literals = []\n",
    "        # Dict of unique words across all literals\n",
    "        self.words = {}\n",
    "        self.word_len = []\n",
    "        # List of unique combinations of words across all literals\n",
    "        self.word_combinations = {}\n",
    "        self.word_combination_lists = []\n",
    "        self.word_combination_index_literal = {}\n",
    "        # 2D list mapping pairs of literals to their word combination index\n",
    "        self.word_combination_index_literal_literal = []\n",
    "        # 2D list of SortedSets, each containing the positions of a word in a paper\n",
    "        self.word_occurrences_in_papers = []\n",
    "        # 3D list containing the minimum distances between word combinations in each paper\n",
    "        self.min_distances = []\n",
    "\n",
    "    @time_function\n",
    "    def populate(\n",
    "        self,\n",
    "        config: Config,\n",
    "        papers: list,\n",
    "        literals: list[str],\n",
    "        paper_full_text,\n",
    "        optimize=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Populates the internal data structures with occurrences and distances of literals in papers.\n",
    "\n",
    "        Parameters:\n",
    "        - config (Config): Configuration object containing settings.\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - literals (list[str]): List of literals to process.\n",
    "        - paper_full_text (dict): Mapping from paper identifiers to their full text file paths.\n",
    "        - optimize (bool): Flag to optimize data structures after population.\n",
    "        \"\"\"\n",
    "        self.initialize_variables(papers, literals)\n",
    "        self.process_literals()\n",
    "        self.process_literal_combinations()\n",
    "        self.setup_data_structures()\n",
    "        self.find_occurrences_in_texts(paper_full_text)\n",
    "        if optimize:\n",
    "            self.optimize_data()\n",
    "\n",
    "    def update_list_attribute(self, list, name):\n",
    "        existing = getattr(self, name)\n",
    "        if not existing:\n",
    "            setattr(self, name, list)\n",
    "        else:\n",
    "            if existing != list:\n",
    "                print(f\"Warning: {name} has changed.\")\n",
    "                for item in list:\n",
    "                    if item not in existing:\n",
    "                        print(f\"Item {item} is new.\")\n",
    "                        existing.append(item)\n",
    "                # update\n",
    "                raise NotImplementedError(\n",
    "                    \"create a function to update other relying attributes\"\n",
    "                )\n",
    "\n",
    "    @time_function\n",
    "    def initialize_variables(self, papers, literals):\n",
    "        \"\"\"\n",
    "        Initializes basic variables for the class instance.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        if not self.papers:\n",
    "            self.papers = papers\n",
    "        else:\n",
    "            if self.papers != papers:\n",
    "                print(\"Warning: Papers have changed.\")\n",
    "                for paper in papers:\n",
    "                    if paper not in self.papers:\n",
    "                        print(f\"Paper {paper} is new.\")\n",
    "                        self.papers.append(paper)\n",
    "        # self.papers = papers\n",
    "\n",
    "        if not self.literals:\n",
    "            self.literals = literals\n",
    "        else:\n",
    "            if self.literals != literals:\n",
    "                print(\"Warning: Literals have changed.\")\n",
    "                for literal in literals:\n",
    "                    if literal not in self.literals:\n",
    "                        print(f\"Literal {literal} is new.\")\n",
    "                        self.literals.append(literal)\n",
    "                        self.word_combination_index_literal[literal] = None\n",
    "\n",
    "        if len(self.literals) != len(self.word_combination_index_literal):\n",
    "            for literal in self.literals:\n",
    "                if literal not in self.word_combination_index_literal:\n",
    "                    self.word_combination_index_literal[literal] = None\n",
    "            # sort self.word_combination_index_literal by self.literals\n",
    "            self.word_combination_index_literal = {\n",
    "                k: self.word_combination_index_literal[k] for k in self.literals\n",
    "            }\n",
    "        if (\n",
    "            isinstance(self.word_combination_index_literal_literal, np.ndarray)\n",
    "            and self.word_combination_index_literal_literal.size == 0\n",
    "        ):\n",
    "            self.word_combination_index_literal_literal = np.full(\n",
    "                (len(self.literals), len(self.literals)), None, dtype=object\n",
    "            )\n",
    "        elif (\n",
    "            isinstance(self.word_combination_index_literal_literal, list)\n",
    "            and self.word_combination_index_literal_literal == []\n",
    "        ):\n",
    "            self.word_combination_index_literal_literal = [\n",
    "                [None] * len(self.literals) for _ in range(len(self.literals))\n",
    "            ]\n",
    "        elif len(self.literals) != len(self.word_combination_index_literal_literal):\n",
    "            # pad self.word_combination_index_literal_literal\n",
    "            len_dif = len(self.literals) - len(\n",
    "                self.word_combination_index_literal_literal\n",
    "            )\n",
    "            self.word_combination_index_literal_literal = np.pad(\n",
    "                self.word_combination_index_literal_literal,\n",
    "                ((0, len_dif), (0, len_dif)),\n",
    "                \"constant\",\n",
    "                constant_values=None,\n",
    "            )\n",
    "            # self.word_combination_index_literal_literal = [[None] * len(self.literals) for _ in range(len(self.literals))]\n",
    "\n",
    "        # self.literals = literals\n",
    "\n",
    "    @time_function\n",
    "    def process_literals(self):\n",
    "        \"\"\"\n",
    "        Processes each literal to extract and store unique words and word combinations.\n",
    "\n",
    "        Parameters:\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        for lit in self.literals:\n",
    "            word_list = split_string(lit)\n",
    "            self.add_words(word_list)\n",
    "            self.add_if_word_combination(word_list, lit)\n",
    "\n",
    "    def add_words(self, word_list):\n",
    "        \"\"\"\n",
    "        Adds unique words from a list to the internal list of words.\n",
    "\n",
    "        Parameters:\n",
    "        - word_list (list): List of words to add.\n",
    "        \"\"\"\n",
    "        for word in word_list:\n",
    "            if word not in self.words:\n",
    "                self.words[word] = len(self.words)\n",
    "                self.word_len.append(len(word))\n",
    "\n",
    "    def add_if_word_combination(self, word_list, lit):\n",
    "        \"\"\"\n",
    "        Adds a unique combination of words from a list to the internal list of word combinations.\n",
    "\n",
    "        Parameters:\n",
    "        - word_list (list): List of words forming a combination.\n",
    "        - lit (str): The literal corresponding to the word combination.\n",
    "        \"\"\"\n",
    "        if len(word_list) > 1:\n",
    "            pos = self.word_combination_index_literal.get(lit, -1)\n",
    "            if pos == -1 or pos == None:\n",
    "                froz = frozenset(word_list)\n",
    "                pos = len(self.word_combinations)\n",
    "                self.add_word_combination(froz, pos)\n",
    "                self.word_combination_index_literal[lit] = pos\n",
    "\n",
    "    def add_word_combination(self, froz, pos):\n",
    "        self.word_combinations[froz] = pos\n",
    "        self.word_combination_lists.append(\n",
    "            [self.words[word] for word in sorted(froz, key=len, reverse=True)]\n",
    "        )\n",
    "\n",
    "    @time_function\n",
    "    def process_literal_combinations(self):\n",
    "        \"\"\"\n",
    "        Processes combinations of literals to store their indices in the internal data structure.\n",
    "\n",
    "        Parameters:\n",
    "        - literals (list): List of literals to process.\n",
    "        \"\"\"\n",
    "        # Use a dictionary for quick lookup and storage\n",
    "        combination_index = len(self.word_combinations)\n",
    "\n",
    "        for id1, literal1 in enumerate(self.literals):\n",
    "            for id2 in range(id1 + 1, len(self.literals)):\n",
    "                if self.word_combination_index_literal_literal[id1][id2] is not None:\n",
    "                    continue\n",
    "                literal2 = self.literals[id2]\n",
    "                # Use a sorted tuple for consistent ordering\n",
    "                froz = frozenset(split_string(literal1) + split_string(literal2))\n",
    "                # Check if the combination is already in the dictionary\n",
    "                pos = self.word_combinations.get(froz, -1)\n",
    "                if pos == -1:\n",
    "                    pos = combination_index\n",
    "                    combination_index += 1\n",
    "\n",
    "                    self.add_word_combination(froz, pos)\n",
    "\n",
    "                # Update the matrix with the index of the combination\n",
    "                self.word_combination_index_literal_literal[id1][id2] = pos\n",
    "                self.word_combination_index_literal_literal[id2][id1] = pos\n",
    "\n",
    "    @time_function\n",
    "    def setup_data_structures(self):\n",
    "        \"\"\"\n",
    "        Initializes the data structures for storing word occurrences and minimum distances.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        \"\"\"\n",
    "        if self.word_occurrences_in_papers == []:\n",
    "            self.word_occurrences_in_papers = [\n",
    "                [[] for _ in self.words] for _ in self.papers\n",
    "            ]\n",
    "        new_papers = len(self.papers) - len(self.word_occurrences_in_papers)\n",
    "        new_words = len(self.words) - len(self.word_occurrences_in_papers[0])\n",
    "        if new_words > 0:\n",
    "            for paper in self.word_occurrences_in_papers:\n",
    "                paper += [[] for _ in range(new_words)]\n",
    "        if new_papers > 0:\n",
    "            self.word_occurrences_in_papers += [\n",
    "                [[] for _ in self.words] for _ in range(new_papers)\n",
    "            ]\n",
    "\n",
    "\n",
    "        if (\n",
    "            isinstance(self.min_distances, list)\n",
    "            and self.min_distances == []\n",
    "            or self.min_distances is None\n",
    "        ):\n",
    "            self.min_distances = np.full(\n",
    "                (len(self.papers), len(self.word_combinations)), -2, dtype=int\n",
    "            )\n",
    "        # If new papers or words have been added, update the data structures\n",
    "        if len(self.papers) > len(self.min_distances):\n",
    "            self.min_distances = np.pad(\n",
    "                self.min_distances,\n",
    "                ((0, len(self.papers) - len(self.min_distances)), (0, 0)),\n",
    "                \"constant\",\n",
    "                constant_values=-2,\n",
    "            )\n",
    "        if len(self.word_combinations) > len(self.min_distances[0]):\n",
    "            len_dif = len(self.word_combinations) - len(self.min_distances[0])\n",
    "            self.min_distances = np.pad(\n",
    "                self.min_distances,\n",
    "                ((0, 0), (0, len_dif)),\n",
    "                \"constant\",\n",
    "                constant_values=-2,\n",
    "            )\n",
    "\n",
    "    @time_function\n",
    "    def find_occurrences_in_texts(self, paper_full_text):\n",
    "        \"\"\"\n",
    "        Finds and stores the occurrences of each word in the full text of each paper.\n",
    "\n",
    "        Parameters:\n",
    "        - papers (list): List of paper identifiers.\n",
    "        - paper_full_text (dict): Mapping from paper identifiers to their full text file paths.\n",
    "        \"\"\"\n",
    "        for paperID, paper in enumerate(self.papers):\n",
    "            if paper in paper_full_text:\n",
    "                with open(paper_full_text[paper], \"r\", encoding=\"utf8\") as f:\n",
    "                    text = f.read().lower()\n",
    "                    for word, wordID in self.words.items():\n",
    "                        if not self.word_occurrences_in_papers[paperID][wordID]:\n",
    "                            self.find_and_add_word_occurrences(\n",
    "                                paperID, wordID, word, text\n",
    "                            )\n",
    "            else:\n",
    "                print(f\"Paper {paper} has no full text available.\")\n",
    "\n",
    "    def find_and_add_word_occurrences(self, paperID, wordID, word, text):\n",
    "        \"\"\"\n",
    "        Finds and adds the occurrences of a word in a paper's text to the internal data structure.\n",
    "\n",
    "        Parameters:\n",
    "        - paperID (int): The index of the paper in the internal list.\n",
    "        - wordID (int): The index of the word in the internal list.\n",
    "        - word (str): The word to find occurrences of.\n",
    "        - text (str): The full text of the paper.\n",
    "        \"\"\"\n",
    "        pos = text.find(word)\n",
    "        while pos != -1:\n",
    "            # self.word_occurrences_in_papers[paperID][wordID].add(pos)\n",
    "            self.word_occurrences_in_papers[paperID][wordID].append([pos, wordID])\n",
    "            pos = text.find(word, pos + 1)\n",
    "\n",
    "    @time_function\n",
    "    def optimize_data(self):\n",
    "        \"\"\"\n",
    "        Optimizes the internal data structures for faster access and smaller memory footprint.\n",
    "        \"\"\"\n",
    "        # self.word_combination_index_literal_literal = np.array(self.word_combination_index_literal_literal, dtype=int)\n",
    "        for paperID in range(len(self.papers)):\n",
    "            for wordID in range(len(self.words)):\n",
    "                # self.word_occurrences_in_papers[paperID][wordID] = SortedSet(self.word_occurrences_in_papers[paperID][wordID])\n",
    "                if not self.word_occurrences_in_papers[paperID][wordID]:\n",
    "                    continue\n",
    "                if isinstance(self.word_occurrences_in_papers[paperID][wordID][0], int):\n",
    "                    self.word_occurrences_in_papers[paperID][wordID] = [\n",
    "                        (x, wordID)\n",
    "                        for x in self.word_occurrences_in_papers[paperID][wordID]\n",
    "                    ]\n",
    "                    print(\n",
    "                        f\"Optimizing {list(self.words.keys())[wordID]} in paper {paperID}\"\n",
    "                    )\n",
    "                for occurrence in self.word_occurrences_in_papers[paperID][wordID]:\n",
    "                    if isinstance(occurrence, int):\n",
    "                        self.word_occurrences_in_papers[paperID][wordID] = [\n",
    "                            (occurrence, wordID)\n",
    "                        ]\n",
    "                        # break\n",
    "                        raise Exception(\"This should not happen\")\n",
    "\n",
    "    def save_to_csv(self, config: Config = None, path=None, name=\"pos_in_paper\"):\n",
    "        if path is None:\n",
    "            path = config.get_output_path()\n",
    "\n",
    "        # save min_distances to csv\n",
    "        # dump self.min_distances to csv, with self.papers as row headers and self.word_combinations as column headers\n",
    "        filepath = os.path.join(path, name + \"_min_distances.csv\")\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            word_combinations = [\n",
    "                \"_\".join(sorted(froz, key=len, reverse=True))\n",
    "                for froz in self.word_combinations.keys()\n",
    "            ]\n",
    "            f.write(\n",
    "                \"papers\"\n",
    "                + config.csv_separator\n",
    "                + config.csv_separator.join(word_combinations)\n",
    "                + \"\\n\"\n",
    "            )\n",
    "            for i, paper in enumerate(self.papers):\n",
    "                f.write(\n",
    "                    paper\n",
    "                    + config.csv_separator\n",
    "                    + config.csv_separator.join(map(str, self.min_distances[i]))\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "\n",
    "    @time_function\n",
    "    def save_to_file(\n",
    "        self,\n",
    "        config,\n",
    "        path=None,\n",
    "        name=\"pos_in_paper\",\n",
    "        check_size=False,\n",
    "        min_distance_to_csv=False,\n",
    "        backup=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Saves the internal data structures to files for persistence.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str, optional): The base path for the output files. Defaults to \"pos_in_paper\".\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = config.get_output_path()\n",
    "        filepath = os.path.join(path, name + \".json\")\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        for key, value in self.__dict__.items():\n",
    "            # if key == \"min_distances\" or key == \"word_combination_index_literal_literal\":\n",
    "            if (\n",
    "                value.__class__.__name__ == \"ndarray\"\n",
    "            ):  # min_distances, word_combination_index_literal_literal\n",
    "                data[key] = value.tolist()\n",
    "            # if key == \"min_distances\":\n",
    "            #     data[key] = value.tolist()\n",
    "            elif key == \"word_combinations\":\n",
    "                data[key] = {\n",
    "                    \"_\".join(key): value\n",
    "                    for key, value in self.word_combinations.items()\n",
    "                }\n",
    "            else:\n",
    "                data[key] = value\n",
    "            if check_size:\n",
    "                # Construct the file name for each sub-dictionary\n",
    "                filepath = os.path.join(path, f\"{name}_{key}.json\")\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(data[key], f, ensure_ascii=False)\n",
    "            pass\n",
    "\n",
    "        if min_distance_to_csv:\n",
    "            self.save_to_csv(config, path, name)\n",
    "\n",
    "        if backup:\n",
    "            backup_path = os.path.join(path, name + \"_backup\" + \".json\")\n",
    "            if os.path.exists(filepath):\n",
    "                existing_is_healthy = True\n",
    "                try:\n",
    "                    self.load_from_file(config, path, name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Overwriting existing save\")\n",
    "                    existing_is_healthy = False\n",
    "                if existing_is_healthy:\n",
    "                    if os.path.exists(backup_path):\n",
    "                        os.remove(backup_path)\n",
    "                    os.rename(filepath, backup_path)\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "    @time_function\n",
    "    def load_from_file(self, config, path=None, name=\"pos_in_paper\", backup=True):\n",
    "        \"\"\"\n",
    "        Loads the internal data structures from files.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str, optional): The base path for the input files. Defaults to \"pos_in_paper\".\n",
    "        \"\"\"\n",
    "        if path is None:\n",
    "            path = config.get_output_path()\n",
    "        filepath = os.path.join(path, name + \".json\")\n",
    "\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {filepath}: {e}\")\n",
    "            if backup:\n",
    "                backup_path = os.path.join(path, name + \"_backup\" + \".json\")\n",
    "                if os.path.exists(backup_path):\n",
    "                    print(f\"Trying to load backup file {backup_path}\")\n",
    "                    with open(backup_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)\n",
    "                else:\n",
    "                    raise Exception(f\"No backup file found at {backup_path}\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        for key, value in data.items():\n",
    "            if key == \"word_combinations\":\n",
    "                setattr(\n",
    "                    self,\n",
    "                    key,\n",
    "                    {\n",
    "                        frozenset(split_string(sub_key)): i\n",
    "                        for i, sub_key in enumerate(value)\n",
    "                    },\n",
    "                )\n",
    "            elif (\n",
    "                key == \"min_distances\"\n",
    "                or key == \"word_combination_index_literal_literal\"\n",
    "            ):\n",
    "                setattr(self, key, np.array(value))\n",
    "            else:\n",
    "                try:\n",
    "                    setattr(self, key, value)\n",
    "                except Exception as e:\n",
    "                    raise Exception(f\"Error loading pos_in_paper attribute {key}: {e}\")\n",
    "\n",
    "        self.setup_data_structures()\n",
    "\n",
    "    @time_function\n",
    "    def calculate_all_possible(self):\n",
    "        \"\"\"\n",
    "        Calculates the minimum distances between all possible combinations of literals in all papers.\n",
    "        \"\"\"\n",
    "        save_every = None\n",
    "        if len(self.papers) > 300:\n",
    "            print(\n",
    "                \"Warning: This operation is computationally expensive and may take a long time.\"\n",
    "            )\n",
    "            save_every = 300\n",
    "        for p in range(len(self.papers)):\n",
    "            if save_every and p % save_every == 0:\n",
    "                print(f\"Processing paper {p} of {len(self.papers)}\")\n",
    "                self.save_to_file(config)\n",
    "            for w in range(len(self.word_combinations)):\n",
    "                self.find_min_distance_by_id(p, w)\n",
    "            # for i in range(len(self.literals)):\n",
    "            #     for j in range(i + 1, len(self.literals)):\n",
    "            #         # get word_combination_index_literal_literal\n",
    "            # self.find_min_distance_by_id(p, self.word_combination_index_literal_literal[i][j])\n",
    "\n",
    "    def find_min_distance_by_id(self, paperID, wcID):\n",
    "        \"\"\"\n",
    "        Finds the minimum distance between occurrences of literals in a paper.\n",
    "\n",
    "        Parameters:\n",
    "        - paper (str): The identifier for the paper.\n",
    "        - literals (list): A list of literals for which the distance is to be found.\n",
    "        - allow_call (bool): Flag to allow recursive call to get_min_distance.\n",
    "\n",
    "        Returns:\n",
    "        - int: The minimum distance between occurrences of the literals.\n",
    "        \"\"\"\n",
    "        distance = self.min_distances[paperID][wcID]\n",
    "\n",
    "        if distance == -1:\n",
    "            # word combination not found in paper\n",
    "            return -1\n",
    "        if distance == -2:\n",
    "            # calculate distance\n",
    "            pass\n",
    "        else:\n",
    "            return distance\n",
    "\n",
    "        list_ids = self.word_combination_lists[wcID]\n",
    "        # since we have attached global Word IDs to the occurrences, we need to map to their local position\n",
    "        list_ids_map = {list_ids[i]: i for i in range(len(list_ids))}\n",
    "        # literals = [list(self.words)[i] for i in list_ids]\n",
    "\n",
    "        # TODO: It should be possible to remove smaller words from the list,\n",
    "        # if a larger word contains it:\n",
    "        # e.g. remove \"engine\" if \"engineer\" is in the list\n",
    "        ## The following implementation works, but is not used for now. Reasons:\n",
    "        ## 1. It could be slower than the current implementation\n",
    "        ## 2. It might be beneficial to future use-cases to not remove smaller words\n",
    "        # literals = [list(self.words)[key] for key in list_ids_map]\n",
    "        # # check if any literal is a substring of another\n",
    "        # for i, lit1 in enumerate(literals):\n",
    "        #     for j, lit2 in enumerate(literals):\n",
    "        #         if i != j and lit1 in lit2:\n",
    "        #             # if lit1 is a substring of lit2, remove it from the list\n",
    "        #             list_ids_map.pop(list_ids[i])\n",
    "        #             break\n",
    "\n",
    "        lit_len = [self.word_len[i] for i in list_ids]\n",
    "\n",
    "        for i in list_ids:\n",
    "            if not self.word_occurrences_in_papers[paperID][i]:\n",
    "                self.min_distances[paperID][wcID] = -1\n",
    "                return -1\n",
    "        # Outsourced to optimize\n",
    "        # inputs = [[(x, i) for x in self.word_occurrences_in_papers[paperID][wordID]] for i, wordID in enumerate(list_ids)]\n",
    "        inputs = [\n",
    "            self.word_occurrences_in_papers[paperID][wordID] for wordID in list_ids\n",
    "        ]\n",
    "\n",
    "        indices = [lst[0][0] for lst in inputs]\n",
    "        best = float(\"inf\")\n",
    "\n",
    "        for item in sorted(sum(inputs, [])):\n",
    "            if item[0] not in indices:\n",
    "                continue\n",
    "            indices[list_ids_map[item[1]]] = item[0]\n",
    "            arr_min = min(indices)\n",
    "            best = min(max(indices) - arr_min - lit_len[indices.index(arr_min)], best)\n",
    "            if best <= 0:\n",
    "                best = 0\n",
    "                break\n",
    "        self.min_distances[paperID][wcID] = best\n",
    "\n",
    "        return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup_data_structures executed in 0.0 seconds\n",
      "load_from_file executed in 2.8189027309417725 seconds\n",
      "Warning: Papers have changed.\n",
      "Warning: Literals have changed.\n",
      "initialize_variables executed in 0.0009996891021728516 seconds\n",
      "process_literals executed in 0.0 seconds\n",
      "process_literal_combinations executed in 0.010002374649047852 seconds\n",
      "setup_data_structures executed in 0.0 seconds\n",
      "Paper Vargas-Hernandez et al. - 2003 - Development of a computer aided conceptual design has no full text available.\n",
      "Paper Van Gent et al. - 2017 - Composing MDAO symphonies graph-based generation has no full text available.\n",
      "Paper Wang et al. - 2018 - Knowledge representation and reasoning methods in has no full text available.\n",
      "Paper Flink et al. - 2022 - Orchestrating tool chains for model-based systems has no full text available.\n",
      "find_occurrences_in_texts executed in 4.802048444747925 seconds\n",
      "optimize_data executed in 0.04400992393493652 seconds\n",
      "populate executed in 4.857060432434082 seconds\n"
     ]
    }
   ],
   "source": [
    "pos_in_paper = PosInPaper()\n",
    "\n",
    "# config.recalculate_pos_in_paper = True # May be used for debug purposes\n",
    "\n",
    "if not config.recalculate_pos_in_paper:\n",
    "    try:\n",
    "        pos_in_paper.load_from_file(config)\n",
    "    # print exception\n",
    "    except Exception as e:\n",
    "        # if config.debug:\n",
    "        #     raise e\n",
    "        print(e)\n",
    "        print(\"Starting from scratch.\")\n",
    "\n",
    "        config.recalculate_pos_in_paper = True\n",
    "\n",
    "# TODO:\n",
    "# raise NotImplementedError(\"Implement a check that compares the loaded instances and papers with the current ones\")\n",
    "\n",
    "# if config.recalculate_pos_in_paper:\n",
    "pos_in_paper.populate(config, list(director.papers.keys()), list(director.instances.keys()), paper_full_text)\n",
    "# pos_in_paper.save_to_file(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.recalculate_pos_in_paper or -2 in pos_in_paper.min_distances:\n",
    "    pos_in_paper.calculate_all_possible()\n",
    "    # Info: This method is extremely slow. requires more testing, which is currently done in a side project:\n",
    "    ## scripts\\SLR\\MVP\\test_case.ipynb\n",
    "    pos_in_paper.save_to_file(config)\n",
    "    config.recalculate_pos_in_paper = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: find the gap between the pieces of an instance\n",
    "import sys\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # intermediate stitching:\n",
    "# papers = list(director.papers.keys())\n",
    "# literals = list(director.instances.keys())\n",
    "# paper_instance_occurrence_matrix = director.paper_instance_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_matrix executed in 0.06461954116821289 seconds\n"
     ]
    }
   ],
   "source": [
    "class ErrorMatrixBuilder(MatrixBuilder):\n",
    "    def __init__(self, director:Director, pos_in_paper: PosInPaper):\n",
    "        super().__init__(director)\n",
    "\n",
    "        self.papers = list(director.papers.keys())\n",
    "        self.literals = list(director.instances.keys())\n",
    "        self.pos_in_paper = pos_in_paper\n",
    "        self.paper_full_text = director.paper_full_text\n",
    "        self.paper_instance_occurrence_matrix = director.builder[\"occurrence_matrix\"].matrix\n",
    "\n",
    "    @time_function\n",
    "    def build_matrix(self):\n",
    "        # self.matrix = find_instance_piece_gap(\n",
    "            # self.config,\n",
    "            # self.papers,\n",
    "            # self.paper_full_text,\n",
    "            # self.literals,\n",
    "            # self.paper_instance_occurrence_matrix,\n",
    "            # self.pos_in_paper,\n",
    "        # )\n",
    "    # def find_instance_piece_gap(\n",
    "    #     config: Config,\n",
    "    #     papers,\n",
    "    #     paper_full_text,\n",
    "    #     instances,\n",
    "    #     paper_instance_occurrence_matrix,\n",
    "    #     pos_in_paper: PosInPaper,\n",
    "    # ):\n",
    "        self.matrix = np.zeros(self.paper_instance_occurrence_matrix.shape, dtype=float)\n",
    "        for paperID, paper in enumerate(self.papers):\n",
    "            if paperID % 100 == 0:\n",
    "                # print(f\"Processing paper {paperID} of {len(papers)}\")\n",
    "                pass\n",
    "            if paper in paper_full_text:\n",
    "                for i, instance in enumerate(self.literals):\n",
    "                    if self.paper_instance_occurrence_matrix[paperID][i] == 0:\n",
    "                        continue\n",
    "                    wcID = pos_in_paper.word_combination_index_literal[instance]\n",
    "                    # TODO: handle if that instance has no word combination index entry\n",
    "                    if wcID is None:\n",
    "                        # word has no distance\n",
    "                        continue\n",
    "                    min_distance = pos_in_paper.find_min_distance_by_id(paperID, wcID)\n",
    "                    if min_distance is None:\n",
    "                        pass\n",
    "                    if min_distance > config.gap_too_large_threshold:\n",
    "                        # print(f\"Gap for {instance} in {paper} ({min_distance} > {GAP_TOO_LARGE_THRESHOLD})\")\n",
    "                        self.paper_instance_occurrence_matrix[paperID][i] = 0\n",
    "                        # get log base 10 of min distance\n",
    "                        self.matrix[paperID][i] = round(np.log10(min_distance), 1)\n",
    "\n",
    "                    # Some pieces may not be found in the full text\n",
    "                    if min_distance == -1:\n",
    "                        # print(f\"{instance} not found in {paper} at all\")\n",
    "                        self.paper_instance_occurrence_matrix[paperID][i] = 0\n",
    "                        self.matrix[paperID][i] = min_distance\n",
    "                        # for these, we do not store the gap\n",
    "                        continue\n",
    "\n",
    "        # return error_matrix\n",
    "    \n",
    "    def build(self):\n",
    "        self.build_matrix()\n",
    "\n",
    "        self.remove_zeros()\n",
    "        # error_matrix, has_error = remove_zeros(error_matrix)\n",
    "        \n",
    "        self.papers = self.handle_deletions(self.papers)\n",
    "        self.literals = self.handle_deletions(self.literals, rows=False)\n",
    "\n",
    "        process_matrix(\n",
    "            self.config,\n",
    "            self.matrix,\n",
    "            self.papers,\n",
    "            self.literals,\n",
    "            \"error_matrix\",\n",
    "        )\n",
    "        # paper_instance_occurrence_matrix, instances, deletions = update_instances(\n",
    "        #     paper_instance_occurrence_matrix, literals, instance_types_dicts\n",
    "        # )\n",
    "\n",
    "        # papers = handle_deletions(papers, deletions)\n",
    "\n",
    "\n",
    "\n",
    "        # free unneeded memory\n",
    "        # del deletions, has_error\n",
    "\n",
    "# instance_instance_co_occurrence_matrix = np.dot(\n",
    "#     paper_instance_occurrence_matrix.T, paper_instance_occurrence_matrix\n",
    "# )\n",
    "\n",
    "# error_matrix = find_instance_piece_gap(\n",
    "#     config,\n",
    "#     papers,\n",
    "#     paper_full_text,\n",
    "#     literals,\n",
    "#     paper_instance_occurrence_matrix,\n",
    "#     pos_in_paper,\n",
    "# )\n",
    "\n",
    "director.paper_full_text = paper_full_text\n",
    "\n",
    "director.builder['error_matrix_builder'] = ErrorMatrixBuilder(director, pos_in_paper)\n",
    "director.builder['error_matrix_builder'].build()\n",
    "director.sort_instances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance_instance Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_instance_co_occurrence_matrix = np.dot(\n",
    "    director.builder[\"occurrence_matrix\"].matrix.T, director.builder[\"occurrence_matrix\"].matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_instance_relative_co_occurrence_matrix = (\n",
    "    instance_instance_co_occurrence_matrix\n",
    "    / np.diag(instance_instance_co_occurrence_matrix)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize timeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def visualize_timeline(\n",
    "    config: Config,\n",
    "    year_instance_occurrence_matrix,\n",
    "    year_papers,\n",
    "    instances,\n",
    "    instance_types_dicts,\n",
    "    name=\"some_timeline\",\n",
    "    path=None,\n",
    "    recursion_depth=0,\n",
    "    start_index=0,\n",
    "    error_matrix=None,\n",
    "    error_instances=None,\n",
    "):\n",
    "    if not path:\n",
    "        path = config.get_output_path(path, visualization=True)\n",
    "    years = list(year_papers.keys())\n",
    "    max_papers = max([len(year_papers[year]) for year in years])\n",
    "    yearly_papers = [len(year_papers[year]) for year in years]\n",
    "\n",
    "    ALPHA_ERROR_LINE = 0.3\n",
    "    ALPHA_ERROR_ZONE = 0.2\n",
    "    ALPHA_PAPER_BAR = 0.3\n",
    "\n",
    "    for type in instance_types_dicts:\n",
    "        use = [instance in instance_types_dicts[type] for instance in instances]\n",
    "        type_instances = [\n",
    "            instance for instance, use_flag in zip(instances, use) if use_flag\n",
    "        ]\n",
    "        total_occurrences = [\n",
    "            np.sum(year_instance_occurrence_matrix[:, instances.index(instance)])\n",
    "            for instance in type_instances\n",
    "        ]\n",
    "        type_instances_sorted = [\n",
    "            x\n",
    "            for _, x in sorted(\n",
    "                zip(total_occurrences, type_instances),\n",
    "                key=lambda pair: pair[0],\n",
    "                reverse=True,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        PARTITION_SIZE = 10\n",
    "        # if error_instances is not None:\n",
    "        #     PARTITION_SIZE = int(0.5 * PARTITION_SIZE)\n",
    "\n",
    "        type_matrix = year_instance_occurrence_matrix[\n",
    "            :, [instances.index(instance) for instance in type_instances_sorted]\n",
    "        ]\n",
    "        factor = 1\n",
    "        size_x = (2 + len(years) / 6) * factor\n",
    "        size_y = (2 + max_papers / 15) * factor\n",
    "        size_y_2 = (2 + PARTITION_SIZE / 2) * factor\n",
    "        size_y = max(size_y, size_y_2)\n",
    "        fig, ax = plt.subplots(figsize=(size_x, size_y), dpi=300)\n",
    "\n",
    "        ax.set_xticks(range(len(years)))\n",
    "        years_labels = [year if len(year_papers[year]) > 0 else \"\" for year in years]\n",
    "        ax.set_xticklabels(years_labels, fontsize=10, rotation=90)\n",
    "\n",
    "        step_size = max(1, math.ceil(max_papers / 10))\n",
    "        ax.set_yticks(np.arange(0, max_papers + 1, step=step_size))\n",
    "        ax.set_yticklabels(\n",
    "            [str(int(x)) for x in np.arange(0, max_papers + 1, step=step_size)],\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "        # set y axis label\n",
    "        ax.set_ylabel(\"absolute\", fontsize=10)\n",
    "\n",
    "        plt.bar(\n",
    "            range(len(years)),\n",
    "            yearly_papers,\n",
    "            color=\"black\",\n",
    "            alpha=ALPHA_PAPER_BAR,\n",
    "            label=f\"Total papers ({sum(yearly_papers)})\",\n",
    "            zorder=0,\n",
    "        )\n",
    "\n",
    "        line_count = 0\n",
    "        i = start_index\n",
    "        while line_count < PARTITION_SIZE and i < len(type_instances_sorted):\n",
    "            instance = type_instances_sorted[i]\n",
    "            yearly_occurrences = type_matrix[:, i]\n",
    "            i_total_occurrences = yearly_occurrences.sum()\n",
    "            label = f\"{instance} ({i_total_occurrences})\"\n",
    "            values = yearly_occurrences\n",
    "            line = plt.plot(range(len(years)), values, label=label, zorder=3)[0]\n",
    "            line_count += 1\n",
    "            if error_matrix is not None and instance in error_instances:\n",
    "                color = line.get_color()\n",
    "                errors = error_matrix[:, error_instances.index(instance)]\n",
    "                errors_plus = yearly_occurrences + errors\n",
    "                line.set_label(f\"{instance} ({i_total_occurrences}-{sum(errors_plus)})\")\n",
    "                # Plot the error as a half transparent line on top of the normal line\n",
    "                plt.plot(\n",
    "                    range(len(years)),\n",
    "                    errors_plus,\n",
    "                    color=color,\n",
    "                    alpha=ALPHA_ERROR_LINE,\n",
    "                    label=f\"{instance} (w/o proximity)\",\n",
    "                    zorder=2,\n",
    "                )\n",
    "                line_count += 1\n",
    "                # color in the area between the normal line and the error line\n",
    "                plt.fill_between(\n",
    "                    range(len(years)),\n",
    "                    yearly_occurrences,\n",
    "                    errors_plus,\n",
    "                    color=color,\n",
    "                    alpha=ALPHA_ERROR_ZONE,\n",
    "                    zorder=1,\n",
    "                )\n",
    "            i += 1\n",
    "\n",
    "            # plt.scatter(range(len(years)), errors, color='red', label=f\"{instance} (error)\", zorder=1)\n",
    "        stop_index = i\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        plt.title(\n",
    "            f\"Number of papers covering {type} instances (#{start_index+1} to #{stop_index} of {len(type_instances_sorted)})\"\n",
    "        )\n",
    "\n",
    "        # Inset for relative values\n",
    "        fig.canvas.draw()\n",
    "        x_lim = ax.get_xlim()  # Get the current x-axis limits from the main plot\n",
    "\n",
    "        bbox = ax.get_position()\n",
    "        bb_left, bb_bottom = bbox.x0, bbox.y0\n",
    "        bb_width, bb_height = bbox.width, bbox.height\n",
    "\n",
    "        ax_inset = plt.axes(\n",
    "            [bb_left, 0.05, bb_width, 0.15],\n",
    "            alpha=ALPHA_PAPER_BAR,\n",
    "            facecolor=\"lightgrey\",\n",
    "        )\n",
    "        for i, instance in enumerate(\n",
    "            type_instances_sorted[start_index:stop_index], start=start_index\n",
    "        ):\n",
    "            yearly_occurrences = type_matrix[:, i]\n",
    "            values_relative = [\n",
    "                occurrences / papers if papers > 0 else 0\n",
    "                for occurrences, papers in zip(yearly_occurrences, yearly_papers)\n",
    "            ]\n",
    "            line_relative = ax_inset.plot(\n",
    "                range(len(years)),\n",
    "                values_relative,\n",
    "                label=f\"{instance} (relative)\",\n",
    "                zorder=3,\n",
    "            )[0]\n",
    "\n",
    "            # add the error part\n",
    "            if error_matrix is not None and instance in error_instances:\n",
    "                color = line_relative.get_color()\n",
    "                errors = error_matrix[:, error_instances.index(instance)]\n",
    "                errors_plus = yearly_occurrences + errors\n",
    "                errors_relative = [\n",
    "                    error / papers if papers > 0 else 0\n",
    "                    for error, papers in zip(errors_plus, yearly_papers)\n",
    "                ]\n",
    "                if max(errors_relative) > 1:\n",
    "                    print(f\"Error: {instance} has a relative error > 1\")\n",
    "                    # throw an exception because this should never be the case:\n",
    "                    # raise Exception(f\"Error: relative {instance} occurrence + error > 1\")\n",
    "\n",
    "                ax_inset.plot(\n",
    "                    range(len(years)),\n",
    "                    errors_relative,\n",
    "                    alpha=ALPHA_ERROR_LINE,\n",
    "                    color=color,\n",
    "                    label=f\"{instance} (error, relative)\",\n",
    "                    zorder=2,\n",
    "                )\n",
    "                # color in the area between the normal line and the error line\n",
    "                ax_inset.fill_between(\n",
    "                    range(len(years)),\n",
    "                    values_relative,\n",
    "                    errors_relative,\n",
    "                    alpha=ALPHA_ERROR_ZONE,\n",
    "                    color=color,\n",
    "                    zorder=1,\n",
    "                )\n",
    "\n",
    "        ax_inset.set_xlim(x_lim)\n",
    "\n",
    "        ax_inset.set_xticks([])\n",
    "        ax_inset.set_yticks(np.arange(0, 1.1, step=0.5))\n",
    "        ax_inset.set_yticklabels(\n",
    "            [f\"{int(x*100)}%\" for x in np.arange(0, 1.1, step=0.5)], fontsize=8\n",
    "        )\n",
    "\n",
    "        # set y axis label\n",
    "        ax_inset.set_ylabel(\"relative\", fontsize=10)\n",
    "\n",
    "        plt.subplots_adjust(bottom=0.3)\n",
    "\n",
    "        start_string = f\"{start_index+1}\"\n",
    "        stop_string = f\"{stop_index}\"\n",
    "\n",
    "        # fill up with 0 to have a constant length\n",
    "        start_string = \"0\" * (3 - len(start_string)) + start_string\n",
    "        stop_string = \"0\" * (3 - len(stop_string)) + stop_string\n",
    "\n",
    "        part_appendix = f\"{start_string}_to_{stop_string}\"\n",
    "        filepath = os.path.join(path, name)\n",
    "        plt.savefig(f\"{filepath}_{type.replace(' ', '_')}_{part_appendix}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        start_index = stop_index\n",
    "        if start_index < len(type_instances_sorted):\n",
    "            # if recursion_depth > 0:\n",
    "            #     break\n",
    "            visualize_timeline(\n",
    "                config,\n",
    "                year_instance_occurrence_matrix,\n",
    "                year_papers,\n",
    "                instances,\n",
    "                {type: instance_types_dicts[type]},\n",
    "                name,\n",
    "                path=path,\n",
    "                recursion_depth=recursion_depth + 1,\n",
    "                start_index=start_index,\n",
    "                error_matrix=error_matrix,\n",
    "                error_instances=error_instances,\n",
    "            )\n",
    "        start_index = 0\n",
    "\n",
    "\n",
    "if config.visualize:\n",
    "    yearly_error_matrix, year_error_papers = create_year_paper_occurrence_matrix(\n",
    "        papers_metadata, error_matrix, error_papers, is_error_matrix=True\n",
    "    )\n",
    "    visualize_timeline(\n",
    "        config,\n",
    "        year_instance_occurrence_matrix,\n",
    "        year_papers,\n",
    "        instances,\n",
    "        instance_types_dicts,\n",
    "        name=\"year_instance_occurrence_matrix\",\n",
    "        error_matrix=yearly_error_matrix,\n",
    "        error_instances=error_instances,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year_paper_occurrence_matrix\n",
    "class YearPaperOccurrenceMatrixBuilder(MatrixBuilder):\n",
    "    def __init__(self, director, papers = None, paper_instance_occurrence_matrix = None, is_error_matrix=False, ):\n",
    "        super().__init__(director)\n",
    "\n",
    "        # self.papers_metadata = papers_metadata\n",
    "        self.papers:dict[str,Instance] = papers or director.papers\n",
    "        self.paper_instance_occurrence_matrix = paper_instance_occurrence_matrix or director.builder[\"occurrence_matrix\"].matrix\n",
    "        self.is_error_matrix = is_error_matrix\n",
    "        self.year_papers:dict[int,dict[str,Instance]] = {}\n",
    "\n",
    "\n",
    "\n",
    "    def build_matrix(self, paper_instance_occurrence_matrix = None, papers = None, is_error_matrix=False):\n",
    "        paper_instance_occurrence_matrix = paper_instance_occurrence_matrix or self.paper_instance_occurrence_matrix\n",
    "        papers = papers or self.papers\n",
    "        # self.matrix, self.year_papers = create_year_paper_occurrence_matrix(\n",
    "        #     papers_metadata, paper_instance_occurrence_matrix, papers, is_error_matrix\n",
    "        # )\n",
    "\n",
    "        # def create_year_paper_occurrence_matrix(\n",
    "        #     papers_metadata, paper_instance_occurrence_matrix, papers, is_error_matrix=False\n",
    "        # ):\n",
    "        indexed_papers = {paper: i for i, paper in enumerate(papers)}\n",
    "        for paper, instance in self.papers.items():\n",
    "            if hasattr(instance, \"year\"):\n",
    "                year = int(getattr(instance, \"year\"))\n",
    "                if year not in self.year_papers:\n",
    "                    self.year_papers[year] = {}\n",
    "                self.year_papers[year][paper] = instance\n",
    "\n",
    "        earliest = min(self.year_papers)\n",
    "        latest = max(self.year_papers)\n",
    "        span = latest - earliest + 1\n",
    "\n",
    "        for year in range(earliest, latest):\n",
    "            if year not in self.year_papers:\n",
    "                self.year_papers[year] = []\n",
    "\n",
    "        self.year_papers = {\n",
    "            k: v for k, v in sorted(self.year_papers.items(), key=lambda item: item[0])\n",
    "        }\n",
    "\n",
    "        if is_error_matrix:\n",
    "            # convert any value != 0 to 1\n",
    "            paper_instance_occurrence_matrix = np.where(\n",
    "                paper_instance_occurrence_matrix != 0, 1, 0\n",
    "            )\n",
    "\n",
    "        # create a year_instance_occurrence matrix from the paper_instance_occurrence_matrix\n",
    "        year_instance_occurrence_matrix = np.zeros(\n",
    "            (span, paper_instance_occurrence_matrix.shape[1]), dtype=int\n",
    "        )\n",
    "        for yearID, year in enumerate(self.year_papers):\n",
    "            for paper in self.year_papers[year]:\n",
    "                if paper in papers:\n",
    "                    paperID = indexed_papers[paper]\n",
    "                    year_instance_occurrence_matrix[\n",
    "                        yearID\n",
    "                    ] += paper_instance_occurrence_matrix[paperID]\n",
    "\n",
    "    def build(self):\n",
    "        self.build_matrix()\n",
    "\n",
    "\n",
    "director.builder['year_instance_occurrence_matrix'] = YearPaperOccurrenceMatrixBuilder(director)\n",
    "director.builder['year_instance_occurrence_matrix'].build()\n",
    "\n",
    "# year_instance_occurrence_matrix, year_papers = create_year_paper_occurrence_matrix(\n",
    "#     papers_metadata, paper_instance_occurrence_matrix, papers\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Complete\n",
    "\n",
    "We now have:\n",
    "\n",
    "| Variable                          | Type    | Size         | Comments |\n",
    "|-----------------------------------|---------|--------------|----------|\n",
    "| error_instances                   | list    | 165          | Comments |\n",
    "| error_matrix                      | ndarray | (999, 165)   | Comments |\n",
    "| error_papers                      | list    | 999          | Comments |\n",
    "| gap_too_large_threshold           | int     | n.a.         | Comments |\n",
    "| instance_piece_gap                | dict    | 151          | Comments |\n",
    "| instance_types_dicts              | dict    | 5            | Comments |\n",
    "| instances                         | list    | 315          | Comments |\n",
    "| paper_full_text                   | dict    | 1029         | Comments |\n",
    "| paper_instance_occurrence_matrix  | ndarray | (1003, 315)  | Comments |\n",
    "| papers                            | list    | 1003         | Comments |\n",
    "| pos_in_paper                      | dict    | 1003         | Comments |\n",
    "\n",
    "Consisting of:\n",
    "* The paper_instance_occurrence_matrix, binary listing if a term (instance) is present in a paper\n",
    "  * papers x instances\n",
    "* The error_matrix, of all instances that were dropped from the paper_instance_occurrence_matrix\n",
    "  * error_papers x error_instances\n",
    "\n",
    "And some leftover variables:\n",
    "* instance_types_dicts, listing all instance types (\"process\", \"software\", ...) and their respective instance sets (\"Curation\", \"Knowledge Work\", ...)\n",
    "* paper_full_text, containing each papers full text\n",
    "  * pos_in_paper, listing for each paper: for each instance: each position of that instance in that papers full text.\n",
    "* instance_piece_gap, a dict listing all instances made up from compound words (e.g. \"Knowledge Work\", and their minimum distance in each papers full text)\n",
    "  * gap_too_large_threshold, defining how far appart a finding of \"Knowledge\" and \"Work\" would qualify as \"Knowledge Work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~3 min | {( len(papers) * len(instances) ) / (3 * 1000) }seconds  compare proximity of all instances with one antoher\n",
    "# ~8 min right now.\n",
    "# 3 min 30 sec with 164 papers and 339 instances\n",
    "class ProximityMatrixBuilder(MatrixBuilder):\n",
    "    def __init__(self, director:Director, instances = None, papers = None, pos_in_paper = None, mode = \"sqrt\"):\n",
    "        super().__init__(director)\n",
    "\n",
    "        self.instances:dict[str,Instance] = instances or director.instances\n",
    "        self.papers:dict[str,Instance] = papers or director.papers\n",
    "        self.pos_in_paper:PosInPaper = pos_in_paper or director.pos_in_paper\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "    def build_matrix(self, instances = None, papers = None, pos_in_paper = None):\n",
    "        instances = instances or self.instances\n",
    "        papers = papers or self.papers\n",
    "        pos_in_paper = pos_in_paper or self.pos_in_paper\n",
    "\n",
    "        # self.matrix, self.proximity_instances = calculate_proximity_matrix(\n",
    "        #     self.config, pos_in_paper, instances, mode=\"sqrt\"\n",
    "        # )\n",
    "\n",
    "    def build(self):\n",
    "        self.build_matrix()\n",
    "        self.remove_zeros()\n",
    "        self.instances = self.handle_deletions(self.instances)\n",
    "\n",
    "    @time_function\n",
    "    def build_matrix(self,\n",
    "        # config: Config,\n",
    "        # pos_in_paper: PosInPaper,\n",
    "        # instances,\n",
    "        # mode=\"sqrt\",\n",
    "        try_to_save_time=False,\n",
    "    ):\n",
    "        # TODO: Optimize this function.\n",
    "        # each instance needs to have it's occurrences as pieces clustered together, so that only those below max distance are considered\n",
    "\n",
    "        # create a np zeros matrix of size instances x instances\n",
    "        indexed_instances = {instance: i for i, instance in enumerate(self.instances)}\n",
    "\n",
    "        self.matrix = np.zeros(\n",
    "            (len(self.instances), len(self.instances)), dtype=float\n",
    "        )\n",
    "\n",
    "        # alternatives are:\n",
    "        # \"sqrt\" - 1 / (square root of the distance)\n",
    "        # \"linear\" - 1 / distance\n",
    "        # \"binary\" - 1 if distance < MAX_GAP_THRESHOLD, 0 otherwise\n",
    "        # \"log\" - 1 / log(distance)\n",
    "\n",
    "        # There is a chance that pos_in_paper papers and instances are out of sync with the current papers and instances\n",
    "        paperIDs = [\n",
    "            paperID for paperID, name in enumerate(pos_in_paper.papers) if name in self.papers\n",
    "        ]\n",
    "        lID_map = {\n",
    "            indexed_instances[name]: instanceID\n",
    "            for instanceID, name in enumerate(pos_in_paper.literals)\n",
    "            if name in self.instances\n",
    "        }\n",
    "\n",
    "        for id1 in range(len(self.instances)):\n",
    "            # print (f\"Processing {id1} of {len(instances)}: {instance1}\")\n",
    "            for id2 in range(id1 + 1, len(self.instances)):\n",
    "                # FIXME: this resulted in a matrix which was not symmetric.\n",
    "                # That hints at a problem with the calclulation, [id1][id2] and [id2][id1] should be the same\n",
    "                wcID = pos_in_paper.word_combination_index_literal_literal[lID_map[id1]][\n",
    "                    lID_map[id2]\n",
    "                ]\n",
    "                for paperID in paperIDs:\n",
    "                    distance = pos_in_paper.find_min_distance_by_id(paperID, wcID)\n",
    "\n",
    "                    if distance < 0:\n",
    "                        # print(f\"Error: {instance1} and {instance2} not found in {paper}\")\n",
    "                        continue\n",
    "                    result = 0.0\n",
    "                    if distance == 0:\n",
    "                        result = 1\n",
    "                    elif distance == 1:\n",
    "                        result = 1\n",
    "                    elif self.mode == \"sqrt\":\n",
    "                        result = 1 / np.sqrt(distance)\n",
    "                    elif self.mode == \"linear\":\n",
    "                        result = 1 / distance\n",
    "                    elif self.mode == \"binary\":\n",
    "                        result = 1 if distance < config.gap_too_large_threshold else 0\n",
    "                    elif self.mode == \"log\":\n",
    "                        result = 1 / np.log(distance)\n",
    "                    else:\n",
    "                        print(\"Error: unknown mode\")\n",
    "                        break\n",
    "                    if result > 0.0:\n",
    "                        self.matrix[id1][id2] += result\n",
    "                        self.matrix[id2][id1] += result\n",
    "\n",
    "        # TODO rest doesnt seem to work, short fix implemented:\n",
    "        # create a copy of labels that only contains instances that are in the proximity matrix\n",
    "\n",
    "        # instance_instance_proximity_matrix, deletions = remove_zeros(\n",
    "        #     instance_instance_proximity_matrix\n",
    "        # )\n",
    "        # proximity_instances = handle_deletions(instances, deletions, rows=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_matrix executed in 5.47377872467041 seconds\n"
     ]
    }
   ],
   "source": [
    "director.pos_in_paper = pos_in_paper\n",
    "director.builder['proximity_matrix'] = ProximityMatrixBuilder(director)\n",
    "director.builder['proximity_matrix'].build()\n",
    "# instance_instance_proximity_matrix, proximity_instances = calculate_proximity_matrix(\n",
    "#     config, pos_in_paper, instances\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "\n",
    "def get_rules(matrix, columns):\n",
    "    # AttributeError: 'numpy.ndarray' object has no attribute 'dtypes'\n",
    "    dataframe = pd.DataFrame(matrix, columns=columns).astype(bool)\n",
    "\n",
    "    # for each process:\n",
    "    # create one res\n",
    "\n",
    "    res = apriori(dataframe, min_support=0.4, use_colnames=True, max_len=2)\n",
    "\n",
    "    # visualize res\n",
    "    res = res.sort_values(by=\"support\", ascending=False)\n",
    "    res = res.reset_index(drop=True)\n",
    "    # res\n",
    "\n",
    "    rules = association_rules(res)\n",
    "    # sort rules by confidence\n",
    "    # rules = rules.sort_values(by='confidence', ascending=False)\n",
    "    rules = rules.sort_values(by=\"lift\", ascending=False)  # (propably most important)\n",
    "    # rules = rules.sort_values(by='leverage', ascending=False)\n",
    "    # export rules to csv\n",
    "    return rules\n",
    "\n",
    "\n",
    "rules = get_rules(director.builder[\"occurrence_matrix\"].matrix, list(director.instances.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table id=\"itables_94442716_eed8_483a_970c_d80078aac787\" class=\"display nowrap\" data-quarto-disable-processing=\"true\" style=\"table-layout:auto;width:auto;margin:auto;caption-side:bottom\">\n",
       "<thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead><tbody><tr>\n",
       "<td style=\"vertical-align:middle; text-align:left\">\n",
       "<div style=\"float:left; margin-right: 10px;\">\n",
       "<a href=https://mwouts.github.io/itables/><svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
       "width=\"64\" viewBox=\"0 0 500 400\" style=\"font-family: 'Droid Sans', sans-serif;\">\n",
       "    <g style=\"fill:#d9d7fc\">\n",
       "        <path d=\"M100,400H500V357H100Z\" />\n",
       "        <path d=\"M100,300H400V257H100Z\" />\n",
       "        <path d=\"M0,200H400V157H0Z\" />\n",
       "        <path d=\"M100,100H500V57H100Z\" />\n",
       "        <path d=\"M100,350H500V307H100Z\" />\n",
       "        <path d=\"M100,250H400V207H100Z\" />\n",
       "        <path d=\"M0,150H400V107H0Z\" />\n",
       "        <path d=\"M100,50H500V7H100Z\" />\n",
       "    </g>\n",
       "    <g style=\"fill:#1a1366;stroke:#1a1366;\">\n",
       "   <rect x=\"100\" y=\"7\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "      <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;500\"\n",
       "      dur=\"5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"0\" y=\"107\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"3.5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "    <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"0;0;400\"\n",
       "      dur=\"3.5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"100\" y=\"207\" width=\"300\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;300;0\"\n",
       "      dur=\"3s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "    <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;400\"\n",
       "      dur=\"3s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"100\" y=\"307\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"4s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "      <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;500\"\n",
       "      dur=\"4s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <g style=\"fill:transparent;stroke-width:8; stroke-linejoin:round\" rx=\"5\">\n",
       "            <g transform=\"translate(45 50) rotate(-45)\">\n",
       "                <circle r=\"33\" cx=\"0\" cy=\"0\" />\n",
       "                <rect x=\"-8\" y=\"32\" width=\"16\" height=\"30\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(450 152)\">\n",
       "                <polyline points=\"-15,-20 -35,-20 -35,40 25,40 25,20\" />\n",
       "                <rect x=\"-15\" y=\"-40\" width=\"60\" height=\"60\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(50 352)\">\n",
       "                <polygon points=\"-35,-5 0,-40 35,-5\" />\n",
       "                <polygon points=\"-35,10 0,45 35,10\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(75 250)\">\n",
       "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
       "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(425 250) rotate(180)\">\n",
       "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
       "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
       "            </g>\n",
       "        </g>\n",
       "    </g>\n",
       "</svg>\n",
       "</a>\n",
       "</div>\n",
       "<div>\n",
       "Loading ITables v2.1.1 from the internet...\n",
       "(need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td>\n",
       "</div>\n",
       "</tr></tbody>\n",
       "\n",
       "</table>\n",
       "<link href=\"https://www.unpkg.com/dt_for_itables@2.0.10/dt_bundle.css\" rel=\"stylesheet\">\n",
       "<script type=\"module\">\n",
       "    import {DataTable, jQuery as $} from 'https://www.unpkg.com/dt_for_itables@2.0.10/dt_bundle.js';\n",
       "\n",
       "    document.querySelectorAll(\"#itables_94442716_eed8_483a_970c_d80078aac787:not(.dataTable)\").forEach(table => {\n",
       "        // Define the table data\n",
       "        const data = [[237, \"manufacture\", \"manufacturing\", 0.450331, 0.649007, 0.423841, 0.941176, 1.45018, 0.131573, 5.966887, 0.564759], [223, \"reasoning\", \"ontology\", 0.556291, 0.596026, 0.456954, 0.821429, 1.378175, 0.125389, 2.262252, 0.61843], [257, \"cad\", \"manufacturing\", 0.463576, 0.649007, 0.403974, 0.871429, 1.342711, 0.10311, 2.729948, 0.475815], [177, \"manufacturing\", \"material\", 0.649007, 0.662252, 0.529801, 0.816327, 1.232653, 0.099996, 1.838852, 0.537736], [176, \"material\", \"manufacturing\", 0.662252, 0.649007, 0.529801, 0.8, 1.232653, 0.099996, 1.754967, 0.558824], [259, \"manufacture\", \"review\", 0.450331, 0.768212, 0.403974, 0.897059, 1.167723, 0.058024, 2.251656, 0.261308], [216, \"training\", \"review\", 0.523179, 0.768212, 0.463576, 0.886076, 1.153426, 0.061664, 2.034584, 0.278968], [186, \"check\", \"detailed\", 0.615894, 0.728477, 0.516556, 0.83871, 1.15132, 0.067892, 1.683444, 0.342175], [181, \"word\", \"express\", 0.642384, 0.708609, 0.523179, 0.814433, 1.14934, 0.067979, 1.570272, 0.363338], [219, \"training\", \"evaluation\", 0.523179, 0.761589, 0.456954, 0.873418, 1.146835, 0.058506, 1.883444, 0.268519], [202, \"ontology\", \"express\", 0.596026, 0.708609, 0.483444, 0.811111, 1.144652, 0.061094, 1.542657, 0.312823], [241, \"training\", \"express\", 0.523179, 0.708609, 0.423841, 0.810127, 1.143263, 0.053112, 1.534658, 0.262804], [144, \"material\", \"evaluation\", 0.662252, 0.761589, 0.576159, 0.87, 1.142348, 0.071795, 1.833928, 0.368943], [218, \"training\", \"processing\", 0.523179, 0.768212, 0.456954, 0.873418, 1.136949, 0.055041, 1.831126, 0.252617], [236, \"acquisition\", \"review\", 0.490066, 0.768212, 0.423841, 0.864865, 1.125815, 0.047366, 1.715232, 0.219156], [247, \"difficulty\", \"review\", 0.483444, 0.768212, 0.417219, 0.863014, 1.123406, 0.045831, 1.692053, 0.212658], [235, \"failure\", \"review\", 0.503311, 0.768212, 0.430464, 0.855263, 1.113317, 0.043814, 1.601445, 0.204923], [160, \"manufacturing\", \"evaluation\", 0.649007, 0.761589, 0.549669, 0.846939, 1.112067, 0.055392, 1.557616, 0.287111], [169, \"optimization\", \"detailed\", 0.662252, 0.728477, 0.536424, 0.81, 1.111909, 0.053989, 1.429069, 0.297991], [164, \"check\", \"add\", 0.615894, 0.794702, 0.543046, 0.88172, 1.109498, 0.053594, 1.735701, 0.256939], [245, \"acquisition\", \"processing\", 0.490066, 0.768212, 0.417219, 0.851351, 1.108225, 0.040744, 1.559302, 0.191507], [174, \"material\", \"detailed\", 0.662252, 0.728477, 0.529801, 0.8, 1.098182, 0.047366, 1.357616, 0.264706], [178, \"creation\", \"processing\", 0.629139, 0.768212, 0.529801, 0.842105, 1.096189, 0.046489, 1.467991, 0.236607], [240, \"failure\", \"processing\", 0.503311, 0.768212, 0.423841, 0.842105, 1.096189, 0.037191, 1.467991, 0.176667], [79, \"processing\", \"database\", 0.768212, 0.827815, 0.695364, 0.905172, 1.093448, 0.059427, 1.815774, 0.368707], [78, \"database\", \"processing\", 0.827815, 0.768212, 0.695364, 0.84, 1.093448, 0.059427, 1.448675, 0.496337], [155, \"material\", \"review\", 0.662252, 0.768212, 0.556291, 0.84, 1.093448, 0.047542, 1.448675, 0.253035], [157, \"material\", \"processing\", 0.662252, 0.768212, 0.556291, 0.84, 1.093448, 0.047542, 1.448675, 0.253035], [166, \"manufacturing\", \"review\", 0.649007, 0.768212, 0.543046, 0.836735, 1.089198, 0.044472, 1.419702, 0.233318], [262, \"difficulty\", \"processing\", 0.483444, 0.768212, 0.403974, 0.835616, 1.087742, 0.032586, 1.410044, 0.156158], [171, \"word\", \"review\", 0.642384, 0.768212, 0.536424, 0.835052, 1.087007, 0.042937, 1.405215, 0.223823], [167, \"word\", \"processing\", 0.642384, 0.768212, 0.536424, 0.835052, 1.087007, 0.042937, 1.405215, 0.223823], [244, \"difficulty\", \"add\", 0.483444, 0.794702, 0.417219, 0.863014, 1.085959, 0.033025, 1.498675, 0.153236], [215, \"reasoning\", \"processing\", 0.556291, 0.768212, 0.463576, 0.833333, 1.08477, 0.036226, 1.390728, 0.176119], [134, \"express\", \"processing\", 0.708609, 0.768212, 0.589404, 0.831776, 1.082743, 0.045042, 1.377851, 0.262257], [224, \"reasoning\", \"evaluation\", 0.556291, 0.761589, 0.456954, 0.821429, 1.078571, 0.033288, 1.335099, 0.164179], [106, \"evaluation\", \"processing\", 0.761589, 0.768212, 0.629139, 0.826087, 1.075337, 0.044077, 1.332781, 0.29386], [107, \"processing\", \"evaluation\", 0.768212, 0.761589, 0.629139, 0.818966, 1.075337, 0.044077, 1.316935, 0.302256], [113, \"detailed\", \"add\", 0.728477, 0.794702, 0.622517, 0.854545, 1.075303, 0.043595, 1.411424, 0.257914], [258, \"inference\", \"access\", 0.417219, 0.900662, 0.403974, 0.968254, 1.075047, 0.028201, 3.129139, 0.119784], [173, \"creation\", \"add\", 0.629139, 0.794702, 0.536424, 0.852632, 1.072895, 0.036446, 1.393094, 0.183201], [253, \"failure\", \"evaluation\", 0.503311, 0.761589, 0.410596, 0.815789, 1.071167, 0.02728, 1.294229, 0.133763], [211, \"training\", \"database\", 0.523179, 0.827815, 0.463576, 0.886076, 1.07038, 0.030481, 1.511405, 0.137897], [201, \"ontology\", \"processing\", 0.596026, 0.768212, 0.490066, 0.822222, 1.070307, 0.032192, 1.303808, 0.162605], [198, \"training\", \"description\", 0.523179, 0.887417, 0.496689, 0.949367, 1.069809, 0.032411, 2.22351, 0.136852], [217, \"difficulty\", \"access\", 0.483444, 0.900662, 0.463576, 0.958904, 1.064666, 0.028157, 2.417219, 0.117582], [189, \"creation\", \"evaluation\", 0.629139, 0.761589, 0.509934, 0.810526, 1.064256, 0.030788, 1.258278, 0.162801], [195, \"check\", \"review\", 0.615894, 0.768212, 0.503311, 0.817204, 1.063775, 0.030174, 1.268017, 0.15608], [170, \"xml\", \"evaluation\", 0.662252, 0.761589, 0.536424, 0.81, 1.063565, 0.03206, 1.254793, 0.176955], [142, \"material\", \"database\", 0.662252, 0.827815, 0.582781, 0.88, 1.06304, 0.03456, 1.434879, 0.175579], [141, \"optimization\", \"database\", 0.662252, 0.827815, 0.582781, 0.88, 1.06304, 0.03456, 1.434879, 0.175579], [57, \"review\", \"access\", 0.768212, 0.900662, 0.735099, 0.956897, 1.062437, 0.0432, 2.304636, 0.253539], [56, \"access\", \"review\", 0.900662, 0.768212, 0.735099, 0.816176, 1.062437, 0.0432, 1.260927, 0.591592], [243, \"failure\", \"add\", 0.503311, 0.794702, 0.423841, 0.842105, 1.059649, 0.023859, 1.300221, 0.113333], [116, \"material\", \"description\", 0.662252, 0.887417, 0.622517, 0.94, 1.059254, 0.034823, 1.87638, 0.165624], [196, \"check\", \"evaluation\", 0.615894, 0.761589, 0.496689, 0.806452, 1.058906, 0.02763, 1.231788, 0.144828], [145, \"express\", \"review\", 0.708609, 0.768212, 0.576159, 0.813084, 1.058411, 0.031797, 1.240066, 0.189394], [65, \"description\", \"evaluation\", 0.887417, 0.761589, 0.715232, 0.80597, 1.058274, 0.039384, 1.228732, 0.489107], [66, \"evaluation\", \"description\", 0.761589, 0.887417, 0.715232, 0.93913, 1.058274, 0.039384, 1.849574, 0.230967], [263, \"api\", \"access\", 0.423841, 0.900662, 0.403974, 0.953125, 1.058249, 0.022236, 2.119205, 0.095534], [77, \"database\", \"add\", 0.827815, 0.794702, 0.695364, 0.84, 1.057, 0.037498, 1.283113, 0.313187], [76, \"add\", \"database\", 0.794702, 0.827815, 0.695364, 0.875, 1.057, 0.037498, 1.377483, 0.262673], [158, \"material\", \"add\", 0.662252, 0.794702, 0.556291, 0.84, 1.057, 0.029999, 1.283113, 0.159664], [185, \"word\", \"evaluation\", 0.642384, 0.761589, 0.516556, 0.804124, 1.055849, 0.027323, 1.217149, 0.147911], [161, \"creation\", \"database\", 0.629139, 0.827815, 0.549669, 0.873684, 1.055411, 0.028858, 1.363135, 0.141566], [124, \"word\", \"access\", 0.642384, 0.900662, 0.609272, 0.948454, 1.053062, 0.0307, 1.927152, 0.140902], [208, \"failure\", \"description\", 0.503311, 0.887417, 0.470199, 0.934211, 1.05273, 0.023552, 1.711258, 0.100845], [119, \"review\", \"evaluation\", 0.768212, 0.761589, 0.615894, 0.801724, 1.052699, 0.030832, 1.202419, 0.215975], [118, \"evaluation\", \"review\", 0.761589, 0.768212, 0.615894, 0.808696, 1.052699, 0.030832, 1.21162, 0.209976], [98, \"add\", \"processing\", 0.794702, 0.768212, 0.642384, 0.808333, 1.052227, 0.031885, 1.209329, 0.241769], [99, \"processing\", \"add\", 0.768212, 0.794702, 0.642384, 0.836207, 1.052227, 0.031885, 1.253398, 0.214138], [152, \"ontology\", \"description\", 0.596026, 0.887417, 0.556291, 0.933333, 1.051741, 0.027367, 1.688742, 0.12178], [232, \"training\", \"add\", 0.523179, 0.794702, 0.437086, 0.835443, 1.051266, 0.021315, 1.24758, 0.102273], [83, \"evaluation\", \"database\", 0.761589, 0.827815, 0.662252, 0.869565, 1.050435, 0.031797, 1.320088, 0.201389], [102, \"add\", \"evaluation\", 0.794702, 0.761589, 0.635762, 0.8, 1.050435, 0.030525, 1.192053, 0.233871], [179, \"optimization\", \"evaluation\", 0.662252, 0.761589, 0.529801, 0.8, 1.050435, 0.025437, 1.192053, 0.142157], [103, \"evaluation\", \"add\", 0.761589, 0.794702, 0.635762, 0.834783, 1.050435, 0.030525, 1.242593, 0.201389], [84, \"database\", \"evaluation\", 0.827815, 0.761589, 0.662252, 0.8, 1.050435, 0.031797, 1.192053, 0.278846], [214, \"acquisition\", \"access\", 0.490066, 0.900662, 0.463576, 0.945946, 1.050278, 0.022192, 1.837748, 0.093878], [231, \"failure\", \"database\", 0.503311, 0.827815, 0.437086, 0.868421, 1.049053, 0.020438, 1.308609, 0.094141], [156, \"word\", \"database\", 0.642384, 0.827815, 0.556291, 0.865979, 1.046103, 0.024516, 1.284768, 0.123236], [193, \"training\", \"step\", 0.523179, 0.92053, 0.503311, 0.962025, 1.045078, 0.02171, 2.092715, 0.090461], [248, \"difficulty\", \"database\", 0.483444, 0.827815, 0.417219, 0.863014, 1.042521, 0.017017, 1.256954, 0.078958], [148, \"check\", \"description\", 0.615894, 0.887417, 0.569536, 0.924731, 1.042048, 0.022981, 1.495743, 0.105052], [175, \"optimization\", \"review\", 0.662252, 0.768212, 0.529801, 0.8, 1.041379, 0.021052, 1.15894, 0.117647], [249, \"inference\", \"analysis\", 0.417219, 0.960265, 0.417219, 1.0, 1.041379, 0.016578, Infinity, 0.068182], [140, \"creation\", \"access\", 0.629139, 0.900662, 0.589404, 0.936842, 1.04017, 0.022762, 1.572848, 0.104133], [229, \"cad\", \"step\", 0.463576, 0.92053, 0.443709, 0.957143, 1.039774, 0.016973, 1.854305, 0.07131], [137, \"check\", \"step\", 0.615894, 0.92053, 0.589404, 0.956989, 1.039607, 0.022455, 1.847682, 0.099186], [180, \"check\", \"database\", 0.615894, 0.827815, 0.529801, 0.860215, 1.03914, 0.019955, 1.231788, 0.09806], [151, \"xml\", \"database\", 0.662252, 0.827815, 0.569536, 0.86, 1.03888, 0.021315, 1.229896, 0.110807], [74, \"detailed\", \"step\", 0.728477, 0.92053, 0.695364, 0.954545, 1.036952, 0.02478, 1.748344, 0.131243], [52, \"access\", \"add\", 0.900662, 0.794702, 0.741722, 0.823529, 1.036275, 0.025964, 1.163355, 0.352381], [51, \"add\", \"access\", 0.794702, 0.900662, 0.741722, 0.933333, 1.036275, 0.025964, 1.490066, 0.170507], [129, \"manufacturing\", \"description\", 0.649007, 0.887417, 0.596026, 0.918367, 1.034877, 0.020087, 1.379139, 0.096017], [138, \"word\", \"description\", 0.642384, 0.887417, 0.589404, 0.917526, 1.033928, 0.019341, 1.365066, 0.09176], [187, \"reasoning\", \"description\", 0.556291, 0.887417, 0.509934, 0.916667, 1.03296, 0.016271, 1.350993, 0.071913], [46, \"processing\", \"analysis\", 0.768212, 0.960265, 0.761589, 0.991379, 1.032402, 0.023902, 4.609272, 0.135404], [108, \"material\", \"step\", 0.662252, 0.92053, 0.629139, 0.95, 1.032014, 0.019517, 1.589404, 0.091847], [105, \"optimization\", \"step\", 0.662252, 0.92053, 0.629139, 0.95, 1.032014, 0.019517, 1.589404, 0.091847], [147, \"creation\", \"description\", 0.629139, 0.887417, 0.576159, 0.915789, 1.031972, 0.01785, 1.336921, 0.083539], [165, \"xml\", \"add\", 0.662252, 0.794702, 0.543046, 0.82, 1.031833, 0.016754, 1.140545, 0.091344], [239, \"cad\", \"description\", 0.463576, 0.887417, 0.423841, 0.914286, 1.030277, 0.012456, 1.313466, 0.054784], [24, \"step\", \"description\", 0.92053, 0.887417, 0.84106, 0.913669, 1.029582, 0.024166, 1.304084, 0.361549], [25, \"description\", \"step\", 0.887417, 0.92053, 0.84106, 0.947761, 1.029582, 0.024166, 1.521287, 0.255211], [159, \"reasoning\", \"analysis\", 0.556291, 0.960265, 0.549669, 0.988095, 1.028982, 0.015482, 3.337748, 0.063478], [256, \"manufacture\", \"description\", 0.450331, 0.887417, 0.410596, 0.911765, 1.027436, 0.010964, 1.275938, 0.048581], [87, \"express\", \"access\", 0.708609, 0.900662, 0.655629, 0.925234, 1.027281, 0.017412, 1.328642, 0.091139], [149, \"check\", \"access\", 0.615894, 0.900662, 0.569536, 0.924731, 1.026724, 0.014824, 1.319773, 0.067763], [182, \"word\", \"add\", 0.642384, 0.794702, 0.523179, 0.814433, 1.024828, 0.012675, 1.106328, 0.067745], [242, \"manufacture\", \"step\", 0.450331, 0.92053, 0.423841, 0.941176, 1.022429, 0.009298, 1.350993, 0.03991], [72, \"express\", \"analysis\", 0.708609, 0.960265, 0.695364, 0.981308, 1.021914, 0.014912, 2.125828, 0.073593], [97, \"express\", \"description\", 0.708609, 0.887417, 0.642384, 0.906542, 1.021551, 0.013552, 1.204636, 0.072399], [121, \"detailed\", \"database\", 0.728477, 0.827815, 0.615894, 0.845455, 1.021309, 0.01285, 1.114141, 0.076842], [111, \"xml\", \"step\", 0.662252, 0.92053, 0.622517, 0.94, 1.021151, 0.012894, 1.324503, 0.061327], [209, \"reasoning\", \"database\", 0.556291, 0.827815, 0.470199, 0.845238, 1.021048, 0.009693, 1.112583, 0.046458], [92, \"optimization\", \"analysis\", 0.662252, 0.960265, 0.649007, 0.98, 1.020552, 0.01307, 1.986755, 0.059624], [226, \"manufacture\", \"development\", 0.450331, 0.980132, 0.450331, 1.0, 1.02027, 0.008947, Infinity, 0.036145], [190, \"failure\", \"development\", 0.503311, 0.980132, 0.503311, 1.0, 1.02027, 0.01, Infinity, 0.04], [228, \"acquisition\", \"description\", 0.490066, 0.887417, 0.443709, 0.905405, 1.02027, 0.008815, 1.190161, 0.038961], [254, \"system design\", \"development\", 0.410596, 0.980132, 0.410596, 1.0, 1.02027, 0.008158, Infinity, 0.033708], [47, \"evaluation\", \"development\", 0.761589, 0.980132, 0.761589, 1.0, 1.02027, 0.015131, Infinity, 0.083333], [154, \"reasoning\", \"development\", 0.556291, 0.980132, 0.556291, 1.0, 1.02027, 0.011052, Infinity, 0.044776], [183, \"training\", \"development\", 0.523179, 0.980132, 0.523179, 1.0, 1.02027, 0.010394, Infinity, 0.041667], [246, \"inference\", \"development\", 0.417219, 0.980132, 0.417219, 1.0, 1.02027, 0.008289, Infinity, 0.034091], [200, \"acquisition\", \"development\", 0.490066, 0.980132, 0.490066, 1.0, 1.02027, 0.009736, Infinity, 0.038961], [67, \"evaluation\", \"step\", 0.761589, 0.92053, 0.715232, 0.93913, 1.020206, 0.014166, 1.305582, 0.083076], [75, \"processing\", \"description\", 0.768212, 0.887417, 0.695364, 0.905172, 1.020008, 0.01364, 1.187237, 0.084626], [73, \"review\", \"description\", 0.768212, 0.887417, 0.695364, 0.905172, 1.020008, 0.01364, 1.187237, 0.084626], [110, \"word\", \"analysis\", 0.642384, 0.960265, 0.629139, 0.979381, 1.019908, 0.01228, 1.927152, 0.054581], [122, \"creation\", \"analysis\", 0.629139, 0.960265, 0.615894, 0.978947, 1.019456, 0.011754, 1.887417, 0.051459], [82, \"detailed\", \"access\", 0.728477, 0.900662, 0.668874, 0.918182, 1.019452, 0.012763, 1.214128, 0.070273], [168, \"optimization\", \"add\", 0.662252, 0.794702, 0.536424, 0.81, 1.01925, 0.010131, 1.080516, 0.055919], [132, \"creation\", \"step\", 0.629139, 0.92053, 0.589404, 0.936842, 1.017721, 0.010263, 1.258278, 0.04695], [40, \"step\", \"database\", 0.92053, 0.827815, 0.774834, 0.841727, 1.016806, 0.012806, 1.087899, 0.207977], [41, \"database\", \"step\", 0.827815, 0.92053, 0.774834, 0.936, 1.016806, 0.012806, 1.241722, 0.095989], [30, \"database\", \"analysis\", 0.827815, 0.960265, 0.807947, 0.976, 1.016386, 0.013026, 1.655629, 0.093632], [31, \"analysis\", \"database\", 0.960265, 0.827815, 0.807947, 0.841379, 1.016386, 0.013026, 1.085517, 0.405738], [131, \"express\", \"database\", 0.708609, 0.827815, 0.596026, 0.841121, 1.016075, 0.009429, 1.083755, 0.054293], [38, \"add\", \"analysis\", 0.794702, 0.960265, 0.774834, 0.975, 1.015345, 0.01171, 1.589404, 0.073615], [39, \"analysis\", \"add\", 0.960265, 0.794702, 0.774834, 0.806897, 1.015345, 0.01171, 1.06315, 0.380342], [71, \"processing\", \"access\", 0.768212, 0.900662, 0.701987, 0.913793, 1.014579, 0.010087, 1.152318, 0.061995], [62, \"add\", \"description\", 0.794702, 0.887417, 0.715232, 0.9, 1.014179, 0.01, 1.125828, 0.0681], [130, \"optimization\", \"description\", 0.662252, 0.887417, 0.596026, 0.9, 1.014179, 0.008333, 1.125828, 0.041394], [128, \"xml\", \"description\", 0.662252, 0.887417, 0.596026, 0.9, 1.014179, 0.008333, 1.125828, 0.041394], [63, \"description\", \"add\", 0.887417, 0.794702, 0.715232, 0.80597, 1.014179, 0.01, 1.058074, 0.124183], [199, \"failure\", \"analysis\", 0.503311, 0.960265, 0.490066, 0.973684, 1.013975, 0.006754, 1.509934, 0.027748], [207, \"acquisition\", \"analysis\", 0.490066, 0.960265, 0.476821, 0.972973, 1.013234, 0.006228, 1.470199, 0.025613], [69, \"detailed\", \"analysis\", 0.728477, 0.960265, 0.708609, 0.972727, 1.012978, 0.009079, 1.456954, 0.047185], [221, \"acquisition\", \"step\", 0.490066, 0.92053, 0.456954, 0.932432, 1.01293, 0.005833, 1.176159, 0.025033], [210, \"difficulty\", \"analysis\", 0.483444, 0.960265, 0.470199, 0.972603, 1.012848, 0.005965, 1.450331, 0.024558], [19, \"description\", \"development\", 0.887417, 0.980132, 0.880795, 0.992537, 1.012656, 0.011008, 2.662252, 0.111013], [18, \"development\", \"description\", 0.980132, 0.887417, 0.880795, 0.898649, 1.012656, 0.011008, 1.110817, 0.629073], [225, \"difficulty\", \"step\", 0.483444, 0.92053, 0.450331, 0.931507, 1.011925, 0.005307, 1.160265, 0.022813], [204, \"training\", \"access\", 0.523179, 0.900662, 0.476821, 0.911392, 1.011914, 0.005614, 1.121097, 0.024691], [163, \"ontology\", \"access\", 0.596026, 0.900662, 0.543046, 0.911111, 1.011601, 0.006228, 1.11755, 0.028389], [45, \"processing\", \"development\", 0.768212, 0.980132, 0.761589, 0.991379, 1.011475, 0.00864, 2.304636, 0.048944], [44, \"review\", \"development\", 0.768212, 0.980132, 0.761589, 0.991379, 1.011475, 0.00864, 2.304636, 0.048944], [10, \"step\", \"analysis\", 0.92053, 0.960265, 0.89404, 0.971223, 1.011412, 0.010087, 1.380795, 0.141975], [61, \"processing\", \"step\", 0.768212, 0.92053, 0.715232, 0.931034, 1.011412, 0.00807, 1.152318, 0.048677], [11, \"analysis\", \"step\", 0.960265, 0.92053, 0.89404, 0.931034, 1.011412, 0.010087, 1.152318, 0.283951], [64, \"review\", \"step\", 0.768212, 0.92053, 0.715232, 0.931034, 1.011412, 0.00807, 1.152318, 0.048677], [33, \"description\", \"access\", 0.887417, 0.900662, 0.807947, 0.910448, 1.010865, 0.008684, 1.109272, 0.095468], [32, \"access\", \"description\", 0.900662, 0.887417, 0.807947, 0.897059, 1.010865, 0.008684, 1.093661, 0.108197], [233, \"manufacture\", \"analysis\", 0.450331, 0.960265, 0.437086, 0.970588, 1.010751, 0.004649, 1.350993, 0.01935], [127, \"material\", \"access\", 0.662252, 0.900662, 0.602649, 0.91, 1.010368, 0.006184, 1.103753, 0.030381], [125, \"xml\", \"access\", 0.662252, 0.900662, 0.602649, 0.91, 1.010368, 0.006184, 1.103753, 0.030381], [96, \"material\", \"analysis\", 0.662252, 0.960265, 0.642384, 0.97, 1.010138, 0.006447, 1.324503, 0.029715], [89, \"optimization\", \"development\", 0.662252, 0.980132, 0.655629, 0.99, 1.010068, 0.006535, 1.986755, 0.029511], [88, \"material\", \"development\", 0.662252, 0.980132, 0.655629, 0.99, 1.010068, 0.006535, 1.986755, 0.029511], [101, \"manufacturing\", \"development\", 0.649007, 0.980132, 0.642384, 0.989796, 1.009859, 0.006272, 1.94702, 0.027816], [104, \"word\", \"development\", 0.642384, 0.980132, 0.635762, 0.989691, 1.009752, 0.00614, 1.927152, 0.027006], [117, \"creation\", \"development\", 0.629139, 0.980132, 0.622517, 0.989474, 1.009531, 0.005877, 1.887417, 0.025456], [123, \"check\", \"development\", 0.615894, 0.980132, 0.609272, 0.989247, 1.0093, 0.005614, 1.847682, 0.023988], [251, \"api\", \"analysis\", 0.423841, 0.960265, 0.410596, 0.96875, 1.008836, 0.003596, 1.271523, 0.015202], [126, \"manufacturing\", \"step\", 0.649007, 0.92053, 0.602649, 0.928571, 1.008736, 0.005219, 1.112583, 0.024673], [220, \"failure\", \"access\", 0.503311, 0.900662, 0.456954, 0.907895, 1.00803, 0.00364, 1.078524, 0.016039], [7, \"design\", \"step\", 0.993377, 0.92053, 0.92053, 0.926667, 1.006667, 0.006096, 1.083685, 1.0], [252, \"system design\", \"design\", 0.410596, 0.993377, 0.410596, 1.0, 1.006667, 0.002719, Infinity, 0.011236], [95, \"manufacturing\", \"design\", 0.649007, 0.993377, 0.649007, 1.0, 1.006667, 0.004298, Infinity, 0.018868], [197, \"ontology\", \"database\", 0.596026, 0.827815, 0.496689, 0.833333, 1.006667, 0.003289, 1.033113, 0.016393], [100, \"word\", \"design\", 0.642384, 0.993377, 0.642384, 1.0, 1.006667, 0.004254, Infinity, 0.018519], [250, \"inference\", \"design\", 0.417219, 0.993377, 0.417219, 1.0, 1.006667, 0.002763, Infinity, 0.011364], [194, \"failure\", \"design\", 0.503311, 0.993377, 0.503311, 1.0, 1.006667, 0.003333, Infinity, 0.013333], [153, \"reasoning\", \"design\", 0.556291, 0.993377, 0.556291, 1.0, 1.006667, 0.003684, Infinity, 0.014925], [212, \"cad\", \"design\", 0.463576, 0.993377, 0.463576, 1.0, 1.006667, 0.00307, Infinity, 0.012346], [28, \"database\", \"design\", 0.827815, 0.993377, 0.827815, 1.0, 1.006667, 0.005482, Infinity, 0.038462], [227, \"manufacture\", \"design\", 0.450331, 0.993377, 0.450331, 1.0, 1.006667, 0.002982, Infinity, 0.012048], [43, \"review\", \"design\", 0.768212, 0.993377, 0.768212, 1.0, 1.006667, 0.005087, Infinity, 0.028571], [42, \"processing\", \"design\", 0.768212, 0.993377, 0.768212, 1.0, 1.006667, 0.005087, Infinity, 0.028571], [120, \"check\", \"design\", 0.615894, 0.993377, 0.615894, 1.0, 1.006667, 0.004079, Infinity, 0.017241], [184, \"training\", \"design\", 0.523179, 0.993377, 0.523179, 1.0, 1.006667, 0.003465, Infinity, 0.013889], [6, \"step\", \"design\", 0.92053, 0.993377, 0.92053, 1.0, 1.006667, 0.006096, Infinity, 0.083333], [60, \"detailed\", \"design\", 0.728477, 0.993377, 0.728477, 1.0, 1.006667, 0.004824, Infinity, 0.02439], [86, \"material\", \"design\", 0.662252, 0.993377, 0.662252, 1.0, 1.006667, 0.004386, Infinity, 0.019608], [29, \"design\", \"database\", 0.993377, 0.827815, 0.827815, 0.833333, 1.006667, 0.005482, 1.033113, 1.0], [85, \"optimization\", \"design\", 0.662252, 0.993377, 0.662252, 1.0, 1.006667, 0.004386, Infinity, 0.019608], [238, \"api\", \"design\", 0.423841, 0.993377, 0.423841, 1.0, 1.006667, 0.002807, Infinity, 0.011494], [3, \"analysis\", \"design\", 0.960265, 0.993377, 0.960265, 1.0, 1.006667, 0.006359, Infinity, 0.166667], [2, \"design\", \"analysis\", 0.993377, 0.960265, 0.960265, 0.966667, 1.006667, 0.006359, 1.192053, 1.0], [70, \"express\", \"design\", 0.708609, 0.993377, 0.708609, 1.0, 1.006667, 0.004693, Infinity, 0.022727], [146, \"ontology\", \"analysis\", 0.596026, 0.960265, 0.576159, 0.966667, 1.006667, 0.003816, 1.192053, 0.016393], [27, \"access\", \"step\", 0.900662, 0.92053, 0.834437, 0.926471, 1.006454, 0.005351, 1.080795, 0.06455], [26, \"step\", \"access\", 0.92053, 0.900662, 0.834437, 0.906475, 1.006454, 0.005351, 1.06215, 0.080688], [206, \"difficulty\", \"development\", 0.483444, 0.980132, 0.476821, 0.986301, 1.006294, 0.002982, 1.450331, 0.012108], [4, \"development\", \"analysis\", 0.980132, 0.960265, 0.94702, 0.966216, 1.006198, 0.005833, 1.176159, 0.310023], [5, \"analysis\", \"development\", 0.960265, 0.980132, 0.94702, 0.986207, 1.006198, 0.005833, 1.440397, 0.155012], [222, \"cad\", \"development\", 0.463576, 0.980132, 0.456954, 0.985714, 1.005695, 0.002588, 1.390728, 0.010556], [53, \"review\", \"analysis\", 0.768212, 0.960265, 0.741722, 0.965517, 1.00547, 0.004035, 1.152318, 0.023469], [58, \"evaluation\", \"analysis\", 0.761589, 0.960265, 0.735099, 0.965217, 1.005157, 0.003772, 1.142384, 0.021522], [59, \"add\", \"step\", 0.794702, 0.92053, 0.735099, 0.925, 1.004856, 0.003552, 1.059603, 0.02354], [191, \"reasoning\", \"access\", 0.556291, 0.900662, 0.503311, 0.904762, 1.004552, 0.002281, 1.043046, 0.010212], [93, \"detailed\", \"description\", 0.728477, 0.887417, 0.649007, 0.890909, 1.003935, 0.002544, 1.032009, 0.014435], [49, \"access\", \"database\", 0.900662, 0.827815, 0.748344, 0.830882, 1.003706, 0.002763, 1.01814, 0.037168], [50, \"database\", \"access\", 0.827815, 0.900662, 0.748344, 0.904, 1.003706, 0.002763, 1.034768, 0.021443], [234, \"difficulty\", \"description\", 0.483444, 0.887417, 0.430464, 0.890411, 1.003374, 0.001447, 1.027318, 0.006509], [37, \"add\", \"development\", 0.794702, 0.980132, 0.781457, 0.983333, 1.003266, 0.002544, 1.192053, 0.015856], [20, \"access\", \"analysis\", 0.900662, 0.960265, 0.86755, 0.963235, 1.003093, 0.002675, 1.080795, 0.031043], [21, \"analysis\", \"access\", 0.960265, 0.900662, 0.86755, 0.903448, 1.003093, 0.002675, 1.028855, 0.077608], [23, \"analysis\", \"description\", 0.960265, 0.887417, 0.854305, 0.889655, 1.002522, 0.002149, 1.020281, 0.063307], [22, \"description\", \"analysis\", 0.887417, 0.960265, 0.854305, 0.962687, 1.002522, 0.002149, 1.064901, 0.022344], [162, \"ontology\", \"step\", 0.596026, 0.92053, 0.549669, 0.922222, 1.001839, 0.001009, 1.02176, 0.004543], [192, \"training\", \"analysis\", 0.523179, 0.960265, 0.503311, 0.962025, 1.001833, 0.000921, 1.046358, 0.003838], [55, \"database\", \"description\", 0.827815, 0.887417, 0.735099, 0.888, 1.000657, 0.000482, 1.005203, 0.003812], [54, \"description\", \"database\", 0.887417, 0.827815, 0.735099, 0.828358, 1.000657, 0.000482, 1.003167, 0.005829], [213, \"failure\", \"step\", 0.503311, 0.92053, 0.463576, 0.921053, 1.000568, 0.000263, 1.006623, 0.001143], [0, \"development\", \"design\", 0.980132, 0.993377, 0.97351, 0.993243, 0.999865, -0.000132, 0.980132, -0.006757], [1, \"design\", \"development\", 0.993377, 0.980132, 0.97351, 0.98, 0.999865, -0.000132, 0.993377, -0.02], [94, \"xml\", \"development\", 0.662252, 0.980132, 0.649007, 0.98, 0.999865, -8.8e-05, 0.993377, -0.0004], [13, \"design\", \"access\", 0.993377, 0.900662, 0.89404, 0.9, 0.999265, -0.000658, 0.993377, -0.1], [12, \"access\", \"design\", 0.900662, 0.993377, 0.89404, 0.992647, 0.999265, -0.000658, 0.900662, -0.007353], [16, \"description\", \"design\", 0.887417, 0.993377, 0.880795, 0.992537, 0.999154, -0.000746, 0.887417, -0.007463], [17, \"design\", \"description\", 0.993377, 0.887417, 0.880795, 0.886667, 0.999154, -0.000746, 0.993377, -0.113333], [115, \"manufacturing\", \"analysis\", 0.649007, 0.960265, 0.622517, 0.959184, 0.998874, -0.000702, 0.97351, -0.003201], [172, \"manufacturing\", \"database\", 0.649007, 0.827815, 0.536424, 0.826531, 0.998449, -0.000833, 0.992598, -0.004406], [36, \"add\", \"design\", 0.794702, 0.993377, 0.788079, 0.991667, 0.998278, -0.00136, 0.794702, -0.008333], [8, \"step\", \"development\", 0.92053, 0.980132, 0.900662, 0.978417, 0.99825, -0.001579, 0.92053, -0.021583], [9, \"development\", \"step\", 0.980132, 0.92053, 0.900662, 0.918919, 0.99825, -0.001579, 0.980132, -0.081081], [48, \"evaluation\", \"design\", 0.761589, 0.993377, 0.754967, 0.991304, 0.997913, -0.001579, 0.761589, -0.008696], [15, \"development\", \"access\", 0.980132, 0.900662, 0.880795, 0.898649, 0.997764, -0.001974, 0.980132, -0.101351], [14, \"access\", \"development\", 0.900662, 0.980132, 0.880795, 0.977941, 0.997764, -0.001974, 0.900662, -0.022059], [143, \"ontology\", \"development\", 0.596026, 0.980132, 0.582781, 0.977778, 0.997598, -0.001403, 0.89404, -0.005926], [230, \"cad\", \"analysis\", 0.463576, 0.960265, 0.443709, 0.957143, 0.996749, -0.001447, 0.927152, -0.006044], [139, \"word\", \"step\", 0.642384, 0.92053, 0.589404, 0.917526, 0.996737, -0.00193, 0.963576, -0.009072], [90, \"xml\", \"design\", 0.662252, 0.993377, 0.655629, 0.99, 0.9966, -0.002237, 0.662252, -0.01], [136, \"check\", \"analysis\", 0.615894, 0.960265, 0.589404, 0.956989, 0.996589, -0.002017, 0.923841, -0.008833], [114, \"creation\", \"design\", 0.629139, 0.993377, 0.622517, 0.989474, 0.99607, -0.002456, 0.629139, -0.010526], [261, \"manufacture\", \"access\", 0.450331, 0.900662, 0.403974, 0.897059, 0.995999, -0.001623, 0.964995, -0.007255], [188, \"reasoning\", \"step\", 0.556291, 0.92053, 0.509934, 0.916667, 0.995803, -0.002149, 0.953642, -0.009409], [260, \"acquisition\", \"database\", 0.490066, 0.827815, 0.403974, 0.824324, 0.995784, -0.00171, 0.980132, -0.008235], [34, \"development\", \"database\", 0.980132, 0.827815, 0.807947, 0.824324, 0.995784, -0.003421, 0.980132, -0.175676], [35, \"database\", \"development\", 0.827815, 0.980132, 0.807947, 0.976, 0.995784, -0.003421, 0.827815, -0.024], [133, \"ontology\", \"design\", 0.596026, 0.993377, 0.589404, 0.988889, 0.995481, -0.002675, 0.596026, -0.011111], [91, \"express\", \"step\", 0.708609, 0.92053, 0.649007, 0.915888, 0.994957, -0.003289, 0.944812, -0.017096], [81, \"evaluation\", \"access\", 0.761589, 0.900662, 0.682119, 0.895652, 0.994437, -0.003816, 0.951987, -0.022925], [203, \"acquisition\", \"design\", 0.490066, 0.993377, 0.483444, 0.986486, 0.993063, -0.003377, 0.490066, -0.013514], [205, \"difficulty\", \"design\", 0.483444, 0.993377, 0.476821, 0.986301, 0.992877, -0.003421, 0.483444, -0.013699], [68, \"detailed\", \"development\", 0.728477, 0.980132, 0.708609, 0.972727, 0.992445, -0.005395, 0.728477, -0.027273], [80, \"express\", \"development\", 0.708609, 0.980132, 0.688742, 0.971963, 0.991665, -0.005789, 0.708609, -0.028037], [109, \"review\", \"database\", 0.768212, 0.827815, 0.629139, 0.818966, 0.98931, -0.006798, 0.95112, -0.04454], [255, \"api\", \"development\", 0.423841, 0.980132, 0.410596, 0.96875, 0.988387, -0.004824, 0.635762, -0.019985], [135, \"optimization\", \"access\", 0.662252, 0.900662, 0.589404, 0.89, 0.988162, -0.007061, 0.90307, -0.034255], [112, \"xml\", \"analysis\", 0.662252, 0.960265, 0.622517, 0.94, 0.978897, -0.01342, 0.662252, -0.06], [150, \"manufacturing\", \"access\", 0.649007, 0.900662, 0.569536, 0.877551, 0.97434, -0.014999, 0.811258, -0.069796]];\n",
       "\n",
       "        // Define the dt_args\n",
       "        let dt_args = {\"layout\": {\"topStart\": \"pageLength\", \"topEnd\": \"search\", \"bottomStart\": \"info\", \"bottomEnd\": \"paging\"}, \"order\": []};\n",
       "        dt_args[\"data\"] = data;\n",
       "\n",
       "        \n",
       "        new DataTable(table, dt_args);\n",
       "    });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rules\n",
    "process_dataframe(config, rules, \"rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_cross_type_rules(rules, director:Director):\n",
    "    cross_type = [False] * len(rules)\n",
    "\n",
    "    for i, antecentent in enumerate(rules.antecedents):\n",
    "        if not isinstance(antecentent, str):\n",
    "            (antecentent,) = antecentent\n",
    "        consequent = rules.iloc[i].consequents\n",
    "        if not isinstance(consequent, str):\n",
    "            (consequent,) = consequent\n",
    "        type1, type2 = None, None\n",
    "        type1 = director.instances.get(antecentent, {}).get(\"instance_of\", [None])[0]\n",
    "        type2 = director.instances.get(consequent, {}).get(\"instance_of\", [None])[0]\n",
    "        # for type in director.classes:\n",
    "        #     if antecentent in instance_types_dicts[type]:\n",
    "        #         type1 = type\n",
    "        #     if consequent in instance_types_dicts[type]:\n",
    "        #         type2 = type\n",
    "        #     if type1 and type2:\n",
    "        #         break\n",
    "        if type1 and type2 and type1 != type2:\n",
    "            cross_type[i] = True\n",
    "            # print(rules.iloc[i])\n",
    "\n",
    "    # create a copy for all rules that are cross type\n",
    "    rules_cross_type = rules[cross_type].copy()\n",
    "    return rules_cross_type\n",
    "\n",
    "\n",
    "rules_cross_type = identify_cross_type_rules(rules, director)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table id=\"itables_3c441b74_2199_44cf_9635_a1ec0c867d9d\" class=\"display nowrap\" data-quarto-disable-processing=\"true\" style=\"table-layout:auto;width:auto;margin:auto;caption-side:bottom\">\n",
       "<thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead><tbody><tr>\n",
       "<td style=\"vertical-align:middle; text-align:left\">\n",
       "<div style=\"float:left; margin-right: 10px;\">\n",
       "<a href=https://mwouts.github.io/itables/><svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n",
       "width=\"64\" viewBox=\"0 0 500 400\" style=\"font-family: 'Droid Sans', sans-serif;\">\n",
       "    <g style=\"fill:#d9d7fc\">\n",
       "        <path d=\"M100,400H500V357H100Z\" />\n",
       "        <path d=\"M100,300H400V257H100Z\" />\n",
       "        <path d=\"M0,200H400V157H0Z\" />\n",
       "        <path d=\"M100,100H500V57H100Z\" />\n",
       "        <path d=\"M100,350H500V307H100Z\" />\n",
       "        <path d=\"M100,250H400V207H100Z\" />\n",
       "        <path d=\"M0,150H400V107H0Z\" />\n",
       "        <path d=\"M100,50H500V7H100Z\" />\n",
       "    </g>\n",
       "    <g style=\"fill:#1a1366;stroke:#1a1366;\">\n",
       "   <rect x=\"100\" y=\"7\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "      <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;500\"\n",
       "      dur=\"5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"0\" y=\"107\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"3.5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "    <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"0;0;400\"\n",
       "      dur=\"3.5s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"100\" y=\"207\" width=\"300\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;300;0\"\n",
       "      dur=\"3s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "    <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;400\"\n",
       "      dur=\"3s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <rect x=\"100\" y=\"307\" width=\"400\" height=\"43\">\n",
       "    <animate\n",
       "      attributeName=\"width\"\n",
       "      values=\"0;400;0\"\n",
       "      dur=\"4s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "      <animate\n",
       "      attributeName=\"x\"\n",
       "      values=\"100;100;500\"\n",
       "      dur=\"4s\"\n",
       "      repeatCount=\"indefinite\" />\n",
       "  </rect>\n",
       "        <g style=\"fill:transparent;stroke-width:8; stroke-linejoin:round\" rx=\"5\">\n",
       "            <g transform=\"translate(45 50) rotate(-45)\">\n",
       "                <circle r=\"33\" cx=\"0\" cy=\"0\" />\n",
       "                <rect x=\"-8\" y=\"32\" width=\"16\" height=\"30\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(450 152)\">\n",
       "                <polyline points=\"-15,-20 -35,-20 -35,40 25,40 25,20\" />\n",
       "                <rect x=\"-15\" y=\"-40\" width=\"60\" height=\"60\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(50 352)\">\n",
       "                <polygon points=\"-35,-5 0,-40 35,-5\" />\n",
       "                <polygon points=\"-35,10 0,45 35,10\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(75 250)\">\n",
       "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
       "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
       "            </g>\n",
       "\n",
       "            <g transform=\"translate(425 250) rotate(180)\">\n",
       "                <polyline points=\"-30,30 -60,0 -30,-30\" />\n",
       "                <polyline points=\"0,30 -30,0 0,-30\" />\n",
       "            </g>\n",
       "        </g>\n",
       "    </g>\n",
       "</svg>\n",
       "</a>\n",
       "</div>\n",
       "<div>\n",
       "Loading ITables v2.1.1 from the internet...\n",
       "(need <a href=https://mwouts.github.io/itables/troubleshooting.html>help</a>?)</td>\n",
       "</div>\n",
       "</tr></tbody>\n",
       "\n",
       "</table>\n",
       "<link href=\"https://www.unpkg.com/dt_for_itables@2.0.10/dt_bundle.css\" rel=\"stylesheet\">\n",
       "<script type=\"module\">\n",
       "    import {DataTable, jQuery as $} from 'https://www.unpkg.com/dt_for_itables@2.0.10/dt_bundle.js';\n",
       "\n",
       "    document.querySelectorAll(\"#itables_3c441b74_2199_44cf_9635_a1ec0c867d9d:not(.dataTable)\").forEach(table => {\n",
       "        // Define the table data\n",
       "        const data = [[223, \"reasoning\", \"ontology\", 0.556291, 0.596026, 0.456954, 0.821429, 1.378175, 0.125389, 2.262252, 0.61843], [257, \"cad\", \"manufacturing\", 0.463576, 0.649007, 0.403974, 0.871429, 1.342711, 0.10311, 2.729948, 0.475815], [177, \"manufacturing\", \"material\", 0.649007, 0.662252, 0.529801, 0.816327, 1.232653, 0.099996, 1.838852, 0.537736], [176, \"material\", \"manufacturing\", 0.662252, 0.649007, 0.529801, 0.8, 1.232653, 0.099996, 1.754967, 0.558824], [181, \"word\", \"express\", 0.642384, 0.708609, 0.523179, 0.814433, 1.14934, 0.067979, 1.570272, 0.363338], [202, \"ontology\", \"express\", 0.596026, 0.708609, 0.483444, 0.811111, 1.144652, 0.061094, 1.542657, 0.312823], [241, \"training\", \"express\", 0.523179, 0.708609, 0.423841, 0.810127, 1.143263, 0.053112, 1.534658, 0.262804], [144, \"material\", \"evaluation\", 0.662252, 0.761589, 0.576159, 0.87, 1.142348, 0.071795, 1.833928, 0.368943], [247, \"difficulty\", \"review\", 0.483444, 0.768212, 0.417219, 0.863014, 1.123406, 0.045831, 1.692053, 0.212658], [164, \"check\", \"add\", 0.615894, 0.794702, 0.543046, 0.88172, 1.109498, 0.053594, 1.735701, 0.256939], [174, \"material\", \"detailed\", 0.662252, 0.728477, 0.529801, 0.8, 1.098182, 0.047366, 1.357616, 0.264706], [79, \"processing\", \"database\", 0.768212, 0.827815, 0.695364, 0.905172, 1.093448, 0.059427, 1.815774, 0.368707], [78, \"database\", \"processing\", 0.827815, 0.768212, 0.695364, 0.84, 1.093448, 0.059427, 1.448675, 0.496337], [155, \"material\", \"review\", 0.662252, 0.768212, 0.556291, 0.84, 1.093448, 0.047542, 1.448675, 0.253035], [157, \"material\", \"processing\", 0.662252, 0.768212, 0.556291, 0.84, 1.093448, 0.047542, 1.448675, 0.253035], [262, \"difficulty\", \"processing\", 0.483444, 0.768212, 0.403974, 0.835616, 1.087742, 0.032586, 1.410044, 0.156158], [171, \"word\", \"review\", 0.642384, 0.768212, 0.536424, 0.835052, 1.087007, 0.042937, 1.405215, 0.223823], [167, \"word\", \"processing\", 0.642384, 0.768212, 0.536424, 0.835052, 1.087007, 0.042937, 1.405215, 0.223823], [244, \"difficulty\", \"add\", 0.483444, 0.794702, 0.417219, 0.863014, 1.085959, 0.033025, 1.498675, 0.153236], [134, \"express\", \"processing\", 0.708609, 0.768212, 0.589404, 0.831776, 1.082743, 0.045042, 1.377851, 0.262257], [113, \"detailed\", \"add\", 0.728477, 0.794702, 0.622517, 0.854545, 1.075303, 0.043595, 1.411424, 0.257914], [258, \"inference\", \"access\", 0.417219, 0.900662, 0.403974, 0.968254, 1.075047, 0.028201, 3.129139, 0.119784], [173, \"creation\", \"add\", 0.629139, 0.794702, 0.536424, 0.852632, 1.072895, 0.036446, 1.393094, 0.183201], [211, \"training\", \"database\", 0.523179, 0.827815, 0.463576, 0.886076, 1.07038, 0.030481, 1.511405, 0.137897], [201, \"ontology\", \"processing\", 0.596026, 0.768212, 0.490066, 0.822222, 1.070307, 0.032192, 1.303808, 0.162605], [198, \"training\", \"description\", 0.523179, 0.887417, 0.496689, 0.949367, 1.069809, 0.032411, 2.22351, 0.136852], [217, \"difficulty\", \"access\", 0.483444, 0.900662, 0.463576, 0.958904, 1.064666, 0.028157, 2.417219, 0.117582], [170, \"xml\", \"evaluation\", 0.662252, 0.761589, 0.536424, 0.81, 1.063565, 0.03206, 1.254793, 0.176955], [142, \"material\", \"database\", 0.662252, 0.827815, 0.582781, 0.88, 1.06304, 0.03456, 1.434879, 0.175579], [141, \"optimization\", \"database\", 0.662252, 0.827815, 0.582781, 0.88, 1.06304, 0.03456, 1.434879, 0.175579], [57, \"review\", \"access\", 0.768212, 0.900662, 0.735099, 0.956897, 1.062437, 0.0432, 2.304636, 0.253539], [56, \"access\", \"review\", 0.900662, 0.768212, 0.735099, 0.816176, 1.062437, 0.0432, 1.260927, 0.591592], [243, \"failure\", \"add\", 0.503311, 0.794702, 0.423841, 0.842105, 1.059649, 0.023859, 1.300221, 0.113333], [145, \"express\", \"review\", 0.708609, 0.768212, 0.576159, 0.813084, 1.058411, 0.031797, 1.240066, 0.189394], [65, \"description\", \"evaluation\", 0.887417, 0.761589, 0.715232, 0.80597, 1.058274, 0.039384, 1.228732, 0.489107], [66, \"evaluation\", \"description\", 0.761589, 0.887417, 0.715232, 0.93913, 1.058274, 0.039384, 1.849574, 0.230967], [158, \"material\", \"add\", 0.662252, 0.794702, 0.556291, 0.84, 1.057, 0.029999, 1.283113, 0.159664], [185, \"word\", \"evaluation\", 0.642384, 0.761589, 0.516556, 0.804124, 1.055849, 0.027323, 1.217149, 0.147911], [161, \"creation\", \"database\", 0.629139, 0.827815, 0.549669, 0.873684, 1.055411, 0.028858, 1.363135, 0.141566], [208, \"failure\", \"description\", 0.503311, 0.887417, 0.470199, 0.934211, 1.05273, 0.023552, 1.711258, 0.100845], [98, \"add\", \"processing\", 0.794702, 0.768212, 0.642384, 0.808333, 1.052227, 0.031885, 1.209329, 0.241769], [99, \"processing\", \"add\", 0.768212, 0.794702, 0.642384, 0.836207, 1.052227, 0.031885, 1.253398, 0.214138], [232, \"training\", \"add\", 0.523179, 0.794702, 0.437086, 0.835443, 1.051266, 0.021315, 1.24758, 0.102273], [83, \"evaluation\", \"database\", 0.761589, 0.827815, 0.662252, 0.869565, 1.050435, 0.031797, 1.320088, 0.201389], [102, \"add\", \"evaluation\", 0.794702, 0.761589, 0.635762, 0.8, 1.050435, 0.030525, 1.192053, 0.233871], [103, \"evaluation\", \"add\", 0.761589, 0.794702, 0.635762, 0.834783, 1.050435, 0.030525, 1.242593, 0.201389], [84, \"database\", \"evaluation\", 0.827815, 0.761589, 0.662252, 0.8, 1.050435, 0.031797, 1.192053, 0.278846], [214, \"acquisition\", \"access\", 0.490066, 0.900662, 0.463576, 0.945946, 1.050278, 0.022192, 1.837748, 0.093878], [231, \"failure\", \"database\", 0.503311, 0.827815, 0.437086, 0.868421, 1.049053, 0.020438, 1.308609, 0.094141], [156, \"word\", \"database\", 0.642384, 0.827815, 0.556291, 0.865979, 1.046103, 0.024516, 1.284768, 0.123236], [193, \"training\", \"step\", 0.523179, 0.92053, 0.503311, 0.962025, 1.045078, 0.02171, 2.092715, 0.090461], [248, \"difficulty\", \"database\", 0.483444, 0.827815, 0.417219, 0.863014, 1.042521, 0.017017, 1.256954, 0.078958], [148, \"check\", \"description\", 0.615894, 0.887417, 0.569536, 0.924731, 1.042048, 0.022981, 1.495743, 0.105052], [140, \"creation\", \"access\", 0.629139, 0.900662, 0.589404, 0.936842, 1.04017, 0.022762, 1.572848, 0.104133], [229, \"cad\", \"step\", 0.463576, 0.92053, 0.443709, 0.957143, 1.039774, 0.016973, 1.854305, 0.07131], [137, \"check\", \"step\", 0.615894, 0.92053, 0.589404, 0.956989, 1.039607, 0.022455, 1.847682, 0.099186], [180, \"check\", \"database\", 0.615894, 0.827815, 0.529801, 0.860215, 1.03914, 0.019955, 1.231788, 0.09806], [74, \"detailed\", \"step\", 0.728477, 0.92053, 0.695364, 0.954545, 1.036952, 0.02478, 1.748344, 0.131243], [52, \"access\", \"add\", 0.900662, 0.794702, 0.741722, 0.823529, 1.036275, 0.025964, 1.163355, 0.352381], [51, \"add\", \"access\", 0.794702, 0.900662, 0.741722, 0.933333, 1.036275, 0.025964, 1.490066, 0.170507], [129, \"manufacturing\", \"description\", 0.649007, 0.887417, 0.596026, 0.918367, 1.034877, 0.020087, 1.379139, 0.096017], [138, \"word\", \"description\", 0.642384, 0.887417, 0.589404, 0.917526, 1.033928, 0.019341, 1.365066, 0.09176], [187, \"reasoning\", \"description\", 0.556291, 0.887417, 0.509934, 0.916667, 1.03296, 0.016271, 1.350993, 0.071913], [108, \"material\", \"step\", 0.662252, 0.92053, 0.629139, 0.95, 1.032014, 0.019517, 1.589404, 0.091847], [105, \"optimization\", \"step\", 0.662252, 0.92053, 0.629139, 0.95, 1.032014, 0.019517, 1.589404, 0.091847], [147, \"creation\", \"description\", 0.629139, 0.887417, 0.576159, 0.915789, 1.031972, 0.01785, 1.336921, 0.083539], [24, \"step\", \"description\", 0.92053, 0.887417, 0.84106, 0.913669, 1.029582, 0.024166, 1.304084, 0.361549], [25, \"description\", \"step\", 0.887417, 0.92053, 0.84106, 0.947761, 1.029582, 0.024166, 1.521287, 0.255211], [256, \"manufacture\", \"description\", 0.450331, 0.887417, 0.410596, 0.911765, 1.027436, 0.010964, 1.275938, 0.048581], [87, \"express\", \"access\", 0.708609, 0.900662, 0.655629, 0.925234, 1.027281, 0.017412, 1.328642, 0.091139], [149, \"check\", \"access\", 0.615894, 0.900662, 0.569536, 0.924731, 1.026724, 0.014824, 1.319773, 0.067763], [182, \"word\", \"add\", 0.642384, 0.794702, 0.523179, 0.814433, 1.024828, 0.012675, 1.106328, 0.067745], [242, \"manufacture\", \"step\", 0.450331, 0.92053, 0.423841, 0.941176, 1.022429, 0.009298, 1.350993, 0.03991], [72, \"express\", \"analysis\", 0.708609, 0.960265, 0.695364, 0.981308, 1.021914, 0.014912, 2.125828, 0.073593], [97, \"express\", \"description\", 0.708609, 0.887417, 0.642384, 0.906542, 1.021551, 0.013552, 1.204636, 0.072399], [121, \"detailed\", \"database\", 0.728477, 0.827815, 0.615894, 0.845455, 1.021309, 0.01285, 1.114141, 0.076842], [209, \"reasoning\", \"database\", 0.556291, 0.827815, 0.470199, 0.845238, 1.021048, 0.009693, 1.112583, 0.046458], [228, \"acquisition\", \"description\", 0.490066, 0.887417, 0.443709, 0.905405, 1.02027, 0.008815, 1.190161, 0.038961], [67, \"evaluation\", \"step\", 0.761589, 0.92053, 0.715232, 0.93913, 1.020206, 0.014166, 1.305582, 0.083076], [75, \"processing\", \"description\", 0.768212, 0.887417, 0.695364, 0.905172, 1.020008, 0.01364, 1.187237, 0.084626], [73, \"review\", \"description\", 0.768212, 0.887417, 0.695364, 0.905172, 1.020008, 0.01364, 1.187237, 0.084626], [110, \"word\", \"analysis\", 0.642384, 0.960265, 0.629139, 0.979381, 1.019908, 0.01228, 1.927152, 0.054581], [82, \"detailed\", \"access\", 0.728477, 0.900662, 0.668874, 0.918182, 1.019452, 0.012763, 1.214128, 0.070273], [168, \"optimization\", \"add\", 0.662252, 0.794702, 0.536424, 0.81, 1.01925, 0.010131, 1.080516, 0.055919], [132, \"creation\", \"step\", 0.629139, 0.92053, 0.589404, 0.936842, 1.017721, 0.010263, 1.258278, 0.04695], [30, \"database\", \"analysis\", 0.827815, 0.960265, 0.807947, 0.976, 1.016386, 0.013026, 1.655629, 0.093632], [31, \"analysis\", \"database\", 0.960265, 0.827815, 0.807947, 0.841379, 1.016386, 0.013026, 1.085517, 0.405738], [131, \"express\", \"database\", 0.708609, 0.827815, 0.596026, 0.841121, 1.016075, 0.009429, 1.083755, 0.054293], [38, \"add\", \"analysis\", 0.794702, 0.960265, 0.774834, 0.975, 1.015345, 0.01171, 1.589404, 0.073615], [39, \"analysis\", \"add\", 0.960265, 0.794702, 0.774834, 0.806897, 1.015345, 0.01171, 1.06315, 0.380342], [71, \"processing\", \"access\", 0.768212, 0.900662, 0.701987, 0.913793, 1.014579, 0.010087, 1.152318, 0.061995], [62, \"add\", \"description\", 0.794702, 0.887417, 0.715232, 0.9, 1.014179, 0.01, 1.125828, 0.0681], [130, \"optimization\", \"description\", 0.662252, 0.887417, 0.596026, 0.9, 1.014179, 0.008333, 1.125828, 0.041394], [128, \"xml\", \"description\", 0.662252, 0.887417, 0.596026, 0.9, 1.014179, 0.008333, 1.125828, 0.041394], [63, \"description\", \"add\", 0.887417, 0.794702, 0.715232, 0.80597, 1.014179, 0.01, 1.058074, 0.124183], [221, \"acquisition\", \"step\", 0.490066, 0.92053, 0.456954, 0.932432, 1.01293, 0.005833, 1.176159, 0.025033], [210, \"difficulty\", \"analysis\", 0.483444, 0.960265, 0.470199, 0.972603, 1.012848, 0.005965, 1.450331, 0.024558], [19, \"description\", \"development\", 0.887417, 0.980132, 0.880795, 0.992537, 1.012656, 0.011008, 2.662252, 0.111013], [18, \"development\", \"description\", 0.980132, 0.887417, 0.880795, 0.898649, 1.012656, 0.011008, 1.110817, 0.629073], [225, \"difficulty\", \"step\", 0.483444, 0.92053, 0.450331, 0.931507, 1.011925, 0.005307, 1.160265, 0.022813], [204, \"training\", \"access\", 0.523179, 0.900662, 0.476821, 0.911392, 1.011914, 0.005614, 1.121097, 0.024691], [163, \"ontology\", \"access\", 0.596026, 0.900662, 0.543046, 0.911111, 1.011601, 0.006228, 1.11755, 0.028389], [10, \"step\", \"analysis\", 0.92053, 0.960265, 0.89404, 0.971223, 1.011412, 0.010087, 1.380795, 0.141975], [61, \"processing\", \"step\", 0.768212, 0.92053, 0.715232, 0.931034, 1.011412, 0.00807, 1.152318, 0.048677], [11, \"analysis\", \"step\", 0.960265, 0.92053, 0.89404, 0.931034, 1.011412, 0.010087, 1.152318, 0.283951], [64, \"review\", \"step\", 0.768212, 0.92053, 0.715232, 0.931034, 1.011412, 0.00807, 1.152318, 0.048677], [33, \"description\", \"access\", 0.887417, 0.900662, 0.807947, 0.910448, 1.010865, 0.008684, 1.109272, 0.095468], [32, \"access\", \"description\", 0.900662, 0.887417, 0.807947, 0.897059, 1.010865, 0.008684, 1.093661, 0.108197], [127, \"material\", \"access\", 0.662252, 0.900662, 0.602649, 0.91, 1.010368, 0.006184, 1.103753, 0.030381], [125, \"xml\", \"access\", 0.662252, 0.900662, 0.602649, 0.91, 1.010368, 0.006184, 1.103753, 0.030381], [96, \"material\", \"analysis\", 0.662252, 0.960265, 0.642384, 0.97, 1.010138, 0.006447, 1.324503, 0.029715], [88, \"material\", \"development\", 0.662252, 0.980132, 0.655629, 0.99, 1.010068, 0.006535, 1.986755, 0.029511], [104, \"word\", \"development\", 0.642384, 0.980132, 0.635762, 0.989691, 1.009752, 0.00614, 1.927152, 0.027006], [251, \"api\", \"analysis\", 0.423841, 0.960265, 0.410596, 0.96875, 1.008836, 0.003596, 1.271523, 0.015202], [126, \"manufacturing\", \"step\", 0.649007, 0.92053, 0.602649, 0.928571, 1.008736, 0.005219, 1.112583, 0.024673], [220, \"failure\", \"access\", 0.503311, 0.900662, 0.456954, 0.907895, 1.00803, 0.00364, 1.078524, 0.016039], [7, \"design\", \"step\", 0.993377, 0.92053, 0.92053, 0.926667, 1.006667, 0.006096, 1.083685, 1.0], [197, \"ontology\", \"database\", 0.596026, 0.827815, 0.496689, 0.833333, 1.006667, 0.003289, 1.033113, 0.016393], [100, \"word\", \"design\", 0.642384, 0.993377, 0.642384, 1.0, 1.006667, 0.004254, Infinity, 0.018519], [212, \"cad\", \"design\", 0.463576, 0.993377, 0.463576, 1.0, 1.006667, 0.00307, Infinity, 0.012346], [28, \"database\", \"design\", 0.827815, 0.993377, 0.827815, 1.0, 1.006667, 0.005482, Infinity, 0.038462], [6, \"step\", \"design\", 0.92053, 0.993377, 0.92053, 1.0, 1.006667, 0.006096, Infinity, 0.083333], [86, \"material\", \"design\", 0.662252, 0.993377, 0.662252, 1.0, 1.006667, 0.004386, Infinity, 0.019608], [29, \"design\", \"database\", 0.993377, 0.827815, 0.827815, 0.833333, 1.006667, 0.005482, 1.033113, 1.0], [238, \"api\", \"design\", 0.423841, 0.993377, 0.423841, 1.0, 1.006667, 0.002807, Infinity, 0.011494], [70, \"express\", \"design\", 0.708609, 0.993377, 0.708609, 1.0, 1.006667, 0.004693, Infinity, 0.022727], [146, \"ontology\", \"analysis\", 0.596026, 0.960265, 0.576159, 0.966667, 1.006667, 0.003816, 1.192053, 0.016393], [27, \"access\", \"step\", 0.900662, 0.92053, 0.834437, 0.926471, 1.006454, 0.005351, 1.080795, 0.06455], [26, \"step\", \"access\", 0.92053, 0.900662, 0.834437, 0.906475, 1.006454, 0.005351, 1.06215, 0.080688], [206, \"difficulty\", \"development\", 0.483444, 0.980132, 0.476821, 0.986301, 1.006294, 0.002982, 1.450331, 0.012108], [222, \"cad\", \"development\", 0.463576, 0.980132, 0.456954, 0.985714, 1.005695, 0.002588, 1.390728, 0.010556], [191, \"reasoning\", \"access\", 0.556291, 0.900662, 0.503311, 0.904762, 1.004552, 0.002281, 1.043046, 0.010212], [93, \"detailed\", \"description\", 0.728477, 0.887417, 0.649007, 0.890909, 1.003935, 0.002544, 1.032009, 0.014435], [49, \"access\", \"database\", 0.900662, 0.827815, 0.748344, 0.830882, 1.003706, 0.002763, 1.01814, 0.037168], [50, \"database\", \"access\", 0.827815, 0.900662, 0.748344, 0.904, 1.003706, 0.002763, 1.034768, 0.021443], [37, \"add\", \"development\", 0.794702, 0.980132, 0.781457, 0.983333, 1.003266, 0.002544, 1.192053, 0.015856], [20, \"access\", \"analysis\", 0.900662, 0.960265, 0.86755, 0.963235, 1.003093, 0.002675, 1.080795, 0.031043], [21, \"analysis\", \"access\", 0.960265, 0.900662, 0.86755, 0.903448, 1.003093, 0.002675, 1.028855, 0.077608], [23, \"analysis\", \"description\", 0.960265, 0.887417, 0.854305, 0.889655, 1.002522, 0.002149, 1.020281, 0.063307], [22, \"description\", \"analysis\", 0.887417, 0.960265, 0.854305, 0.962687, 1.002522, 0.002149, 1.064901, 0.022344], [162, \"ontology\", \"step\", 0.596026, 0.92053, 0.549669, 0.922222, 1.001839, 0.001009, 1.02176, 0.004543], [55, \"database\", \"description\", 0.827815, 0.887417, 0.735099, 0.888, 1.000657, 0.000482, 1.005203, 0.003812], [54, \"description\", \"database\", 0.887417, 0.827815, 0.735099, 0.828358, 1.000657, 0.000482, 1.003167, 0.005829], [213, \"failure\", \"step\", 0.503311, 0.92053, 0.463576, 0.921053, 1.000568, 0.000263, 1.006623, 0.001143], [94, \"xml\", \"development\", 0.662252, 0.980132, 0.649007, 0.98, 0.999865, -8.8e-05, 0.993377, -0.0004], [13, \"design\", \"access\", 0.993377, 0.900662, 0.89404, 0.9, 0.999265, -0.000658, 0.993377, -0.1], [12, \"access\", \"design\", 0.900662, 0.993377, 0.89404, 0.992647, 0.999265, -0.000658, 0.900662, -0.007353], [16, \"description\", \"design\", 0.887417, 0.993377, 0.880795, 0.992537, 0.999154, -0.000746, 0.887417, -0.007463], [17, \"design\", \"description\", 0.993377, 0.887417, 0.880795, 0.886667, 0.999154, -0.000746, 0.993377, -0.113333], [172, \"manufacturing\", \"database\", 0.649007, 0.827815, 0.536424, 0.826531, 0.998449, -0.000833, 0.992598, -0.004406], [36, \"add\", \"design\", 0.794702, 0.993377, 0.788079, 0.991667, 0.998278, -0.00136, 0.794702, -0.008333], [8, \"step\", \"development\", 0.92053, 0.980132, 0.900662, 0.978417, 0.99825, -0.001579, 0.92053, -0.021583], [9, \"development\", \"step\", 0.980132, 0.92053, 0.900662, 0.918919, 0.99825, -0.001579, 0.980132, -0.081081], [15, \"development\", \"access\", 0.980132, 0.900662, 0.880795, 0.898649, 0.997764, -0.001974, 0.980132, -0.101351], [14, \"access\", \"development\", 0.900662, 0.980132, 0.880795, 0.977941, 0.997764, -0.001974, 0.900662, -0.022059], [143, \"ontology\", \"development\", 0.596026, 0.980132, 0.582781, 0.977778, 0.997598, -0.001403, 0.89404, -0.005926], [230, \"cad\", \"analysis\", 0.463576, 0.960265, 0.443709, 0.957143, 0.996749, -0.001447, 0.927152, -0.006044], [139, \"word\", \"step\", 0.642384, 0.92053, 0.589404, 0.917526, 0.996737, -0.00193, 0.963576, -0.009072], [90, \"xml\", \"design\", 0.662252, 0.993377, 0.655629, 0.99, 0.9966, -0.002237, 0.662252, -0.01], [261, \"manufacture\", \"access\", 0.450331, 0.900662, 0.403974, 0.897059, 0.995999, -0.001623, 0.964995, -0.007255], [188, \"reasoning\", \"step\", 0.556291, 0.92053, 0.509934, 0.916667, 0.995803, -0.002149, 0.953642, -0.009409], [260, \"acquisition\", \"database\", 0.490066, 0.827815, 0.403974, 0.824324, 0.995784, -0.00171, 0.980132, -0.008235], [34, \"development\", \"database\", 0.980132, 0.827815, 0.807947, 0.824324, 0.995784, -0.003421, 0.980132, -0.175676], [35, \"database\", \"development\", 0.827815, 0.980132, 0.807947, 0.976, 0.995784, -0.003421, 0.827815, -0.024], [133, \"ontology\", \"design\", 0.596026, 0.993377, 0.589404, 0.988889, 0.995481, -0.002675, 0.596026, -0.011111], [91, \"express\", \"step\", 0.708609, 0.92053, 0.649007, 0.915888, 0.994957, -0.003289, 0.944812, -0.017096], [81, \"evaluation\", \"access\", 0.761589, 0.900662, 0.682119, 0.895652, 0.994437, -0.003816, 0.951987, -0.022925], [205, \"difficulty\", \"design\", 0.483444, 0.993377, 0.476821, 0.986301, 0.992877, -0.003421, 0.483444, -0.013699], [80, \"express\", \"development\", 0.708609, 0.980132, 0.688742, 0.971963, 0.991665, -0.005789, 0.708609, -0.028037], [109, \"review\", \"database\", 0.768212, 0.827815, 0.629139, 0.818966, 0.98931, -0.006798, 0.95112, -0.04454], [255, \"api\", \"development\", 0.423841, 0.980132, 0.410596, 0.96875, 0.988387, -0.004824, 0.635762, -0.019985], [135, \"optimization\", \"access\", 0.662252, 0.900662, 0.589404, 0.89, 0.988162, -0.007061, 0.90307, -0.034255], [112, \"xml\", \"analysis\", 0.662252, 0.960265, 0.622517, 0.94, 0.978897, -0.01342, 0.662252, -0.06], [150, \"manufacturing\", \"access\", 0.649007, 0.900662, 0.569536, 0.877551, 0.97434, -0.014999, 0.811258, -0.069796]];\n",
       "\n",
       "        // Define the dt_args\n",
       "        let dt_args = {\"layout\": {\"topStart\": \"pageLength\", \"topEnd\": \"search\", \"bottomStart\": \"info\", \"bottomEnd\": \"paging\"}, \"order\": []};\n",
       "        dt_args[\"data\"] = data;\n",
       "\n",
       "        \n",
       "        new DataTable(table, dt_args);\n",
       "    });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def process_dataframe(config:Config, input_df, name = \"some_df\", path=None):\n",
    "#     if path is None:\n",
    "#         path = config.get_output_path()\n",
    "#     filepath = os.path.join(path, name)\n",
    "\n",
    "#     # convert all froensets to strings\n",
    "#     for col in input_df.columns:\n",
    "#         if isinstance(col[0], frozenset):\n",
    "#             # input_df[col] = input_df[col].apply(lambda x: \"_\".join(x))\n",
    "#             # input_df[col] = input_df[col].apply(lambda x: \"_\".join(x))\n",
    "#             input_df[col] = input_df[col].apply(lambda x: x + \"_HI!\")\n",
    "#             pass\n",
    "\n",
    "#     input_df.to_csv(filepath + '.csv', sep=config.csv_separator, decimal=config.csv_decimal)\n",
    "#     show(input_df)\n",
    "\n",
    "# rules_cross_type = identify_cross_type_rules(rules)\n",
    "\n",
    "process_dataframe(config, rules_cross_type, \"rules_cross_type\")\n",
    "# cross_type_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_done = False\n",
    "\n",
    "\n",
    "def print_kg_dict(config: Config, kg_dict, header):\n",
    "    filepath = os.path.join(config.get_output_path(), \"instance_relations.csv\")\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(header + \"\\n\")\n",
    "        total_comma = len(kg_dict) - 1\n",
    "        for pos1, type1 in enumerate(kg_dict):\n",
    "            preamble = \",\" * pos1\n",
    "            for pos2, type2 in enumerate(kg_dict[type1]):\n",
    "                intermediate = \",\" * (pos2 + 1)\n",
    "                rest_comma = \",\" * (total_comma - pos1 - pos2)\n",
    "                for i1, i2 in kg_dict[type1][type2]:\n",
    "                    f.write(preamble + i1 + intermediate + i2 + rest_comma + \"\\n\")\n",
    "\n",
    "\n",
    "def knowledge_graph_population_cross_type_rules(\n",
    "    config: Config, rules: association_rules, instance_types_dicts\n",
    "):\n",
    "    header = config.csv_separator.join(instance_types_dicts.keys())\n",
    "    # Triangular dict\n",
    "    dummy_dict = {}\n",
    "    for instance_type in instance_types_dicts:\n",
    "        dummy_dict[instance_type] = {}\n",
    "        for type in instance_types_dicts:\n",
    "            if type not in dummy_dict:\n",
    "                dummy_dict[instance_type][type] = []\n",
    "    for i, antecentent in enumerate(rules.antecedents):\n",
    "        (antecentent,) = antecentent\n",
    "        (consequent,) = rules.iloc[i].consequents\n",
    "        first_type = None\n",
    "        second_type = None\n",
    "        for type in instance_types_dicts:\n",
    "            if antecentent in instance_types_dicts[type]:\n",
    "                # type1 = type\n",
    "                if not first_type:\n",
    "                    first_type = type\n",
    "                    first_instance = antecentent\n",
    "                else:\n",
    "                    second_type = type\n",
    "                    second_instance = antecentent\n",
    "            if consequent in instance_types_dicts[type]:\n",
    "                if not first_type:\n",
    "                    first_type = type\n",
    "                    first_instance = consequent\n",
    "                else:\n",
    "                    second_type = type\n",
    "                    second_instance = consequent\n",
    "            if first_type and second_type:\n",
    "                break\n",
    "        if first_type != second_type:\n",
    "            dummy_dict[first_type][second_type].append(\n",
    "                (first_instance, second_instance)\n",
    "            )\n",
    "\n",
    "    print_kg_dict(config, dummy_dict, header)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "## Disabled. likely not needed anymore\n",
    "# try:\n",
    "#     kg_done = knowledge_graph_population_cross_type_rules(\n",
    "#         config, rules_cross_type, director\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     if config.debug:\n",
    "#         raise e\n",
    "#     else:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare csv file again\n",
    "# process,software,data item,data model,data format specification,interchange format,data visualization,data validation,inference,source\n",
    "\n",
    "\n",
    "@time_function\n",
    "def knowledge_graph_population(\n",
    "    config: Config,\n",
    "    instance_types_dicts,\n",
    "    property_types_dicts,\n",
    "    instance_instance_proximity_matrix,\n",
    "    proximity_instances,\n",
    "):\n",
    "    columns = list(instance_types_dicts.keys())\n",
    "    # columns += list(property_types_dicts.keys())\n",
    "    # columns = ['process', 'software', 'data item', 'data model', 'data format specification', 'data visualization', 'data validation', 'inference']\n",
    "\n",
    "    rows = []\n",
    "    for c_ID, column in enumerate(columns):\n",
    "        for instance in instance_types_dicts[column]:\n",
    "            # add the instance to the csv with each of their relations\n",
    "            if instance not in proximity_instances:\n",
    "                continue\n",
    "            instance_index = proximity_instances.index(instance)\n",
    "            for oc_ID, other_column in enumerate(columns):\n",
    "                if other_column not in instance_types_dicts:\n",
    "                    if other_column in property_types_dicts:\n",
    "                        # TODO: handle properties specially\n",
    "                        continue\n",
    "                    continue\n",
    "                if other_column != column:\n",
    "                    other_column_instances = instance_types_dicts[other_column]\n",
    "                    for other_instance in other_column_instances:\n",
    "                        if other_instance not in proximity_instances:\n",
    "                            continue\n",
    "                        other_instance_index = proximity_instances.index(other_instance)\n",
    "                        if (\n",
    "                            instance_instance_proximity_matrix[instance_index][\n",
    "                                other_instance_index\n",
    "                            ]\n",
    "                            > config.proximity_min_value\n",
    "                        ):\n",
    "                            # build row column by column\n",
    "                            row = [\"\"] * len(columns)\n",
    "                            row[c_ID] = instance\n",
    "                            row[oc_ID] = other_instance\n",
    "                            rows.append(row)\n",
    "\n",
    "    # write to csv\n",
    "    filepath = os.path.join(config.get_output_path(), \"instance_relations.csv\")\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(config.csv_separator.join(columns) + \"\\n\")\n",
    "        for row in rows:\n",
    "            f.write(config.csv_separator.join(row) + \"\\n\")\n",
    "    return True\n",
    "\n",
    "## Disabled. likely not needed anymore\n",
    "# if not kg_done:\n",
    "#     kg_done = knowledge_graph_population(\n",
    "#         config,\n",
    "#         instance_types_dicts,\n",
    "#         property_types_dicts,\n",
    "#         instance_instance_proximity_matrix,\n",
    "#         proximity_instances,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: DeprecationWarning: invalid escape sequence '\\R'\n",
      "<>:14: DeprecationWarning: invalid escape sequence '\\R'\n",
      "C:\\Users\\timwi\\AppData\\Local\\Temp\\ipykernel_13328\\3497971630.py:14: DeprecationWarning: invalid escape sequence '\\R'\n",
      "  df_re = df_re.set_index(\"Domain\\Range\")\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "import types\n",
    "\n",
    "\n",
    "# process,software,data item,data model,data format specification,interchange format,data visualization,data validation,inference,source\n",
    "# process,software,data item,data model,data format specification\n",
    "\n",
    "\n",
    "def save_as_owl(config: Config, path=None):\n",
    "    onto_path = config.ontology_path\n",
    "    df_cl = pd.read_csv(os.path.join(onto_path, \"classes.csv\"))\n",
    "    df_re = pd.read_csv(os.path.join(onto_path, \"relations.csv\"))\n",
    "    df_re = df_re.set_index(\"Domain\\Range\")\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    data_path = os.path.join(path, \"instance_relations.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    # df = pd.read_csv('data.csv')\n",
    "\n",
    "    onto = get_ontology(\"http://tib.eu/slr\")\n",
    "\n",
    "    df_contributions = pd.read_csv(\n",
    "        os.path.join(path, \"paper_instance_occurrence_matrix.csv\")\n",
    "    )\n",
    "    df_rules = pd.read_csv(os.path.join(path, \"rules_cross_type.csv\"))\n",
    "\n",
    "    with open(os.path.join(path, \"instance_types_dicts.json\")) as file:\n",
    "        inst_data = json.load(file)\n",
    "\n",
    "    onto = get_ontology(\"http://tib.eu/slr\")\n",
    "\n",
    "    with onto:\n",
    "\n",
    "        # Classes\n",
    "        for ind, row in df_cl.iterrows():\n",
    "            cl = types.new_class(row[\"URI\"], (Thing,))\n",
    "            cl.label = row[\"Label\"]\n",
    "            re = types.new_class(\n",
    "                f'has{row[\"Label\"].title().replace(\" \", \"\")}', (ObjectProperty,)\n",
    "            )\n",
    "            re.label = f'has {row[\"Label\"]}'\n",
    "\n",
    "        # Instances\n",
    "        for key, value in inst_data.items():\n",
    "            cl = onto.search_one(label=key)\n",
    "            if cl:\n",
    "                for item in value:\n",
    "                    inst = cl()\n",
    "                    inst.label = item\n",
    "\n",
    "        # Statements\n",
    "        Contribution = types.new_class(\"Contribution\", (Thing,))\n",
    "        mentions = types.new_class(\"mentions\", (ObjectProperty,))\n",
    "        mentions.label = \"mentions\"\n",
    "        for ind, row in df_contributions.iterrows():\n",
    "            contrib_inst = Contribution()\n",
    "            contrib_inst.label = row[0]\n",
    "            for col in df_contributions.columns:\n",
    "                if row[col]:\n",
    "                    inst = onto.search_one(label=col)\n",
    "                    if inst:\n",
    "                        contrib_inst.mentions.append(inst)\n",
    "\n",
    "        # Rules\n",
    "        for ind, row in df_rules.iterrows():\n",
    "            subj_inst = onto.search_one(label=row[\"antecedents\"])\n",
    "            obj_inst = onto.search_one(label=row[\"consequents\"])\n",
    "            if subj_inst and obj_inst:\n",
    "                obj_cl = obj_inst.is_a[0]\n",
    "                rel_label = f\"has {str(obj_cl.label[0])}\"\n",
    "                rel = onto.search_one(label=rel_label)\n",
    "                if rel:\n",
    "                    rel[subj_inst].append(obj_inst)\n",
    "\n",
    "    output_path = os.path.join(onto_path, \"onto.owl\")\n",
    "    # onto.save('onto.owl')\n",
    "    onto.save(output_path)\n",
    "    onto.destroy()\n",
    "\n",
    "\n",
    "# save_as_owl(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for ORKG\n",
    "# header:\n",
    "# paper:title,paper:authors,paper:publication_month,paper:publication_year,paper:published_in,paper:research_field,paper:doi,paper:url,contribution:research_problem,contribution:extraction_method,Property 1,Property 2\n",
    "\n",
    "\n",
    "def flatten_nested_properties(data, pefix=\"\"):\n",
    "    res = {}\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            res.update(flatten_nested_properties(value, f\"{pefix}{key}:\"))\n",
    "        else:\n",
    "            res[f\"{pefix}{key}\"] = value\n",
    "    return res\n",
    "\n",
    "\n",
    "class Paper:\n",
    "    order = [\n",
    "        \"title\",\n",
    "        \"authors\",\n",
    "        \"publication_month\",\n",
    "        \"publication_year\",\n",
    "        \"published_in\",\n",
    "        \"research_field\",\n",
    "        \"doi\",\n",
    "        \"url\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, paperID, data={}):\n",
    "        self.paperID: str = paperID\n",
    "        self.title: str = data.get(\"title\", \"\")\n",
    "        ## now handled later\n",
    "        # if self.title:\n",
    "        #     self.title = '\"' + self.title + '\"'\n",
    "        if self.title and \"{\" in self.title or \"}\" in self.title:\n",
    "            self.title = self.title.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "        self.authors: list[str] = data.get(\"author\", \"\")\n",
    "        if isinstance(self.authors, str):\n",
    "            authors = self.authors.split(\"and \")\n",
    "            if authors:\n",
    "                for i, author in enumerate(authors):\n",
    "                    name = author.split(\",\")\n",
    "                    if len(name) > 1:\n",
    "                        name = f\"{name[1].strip()} {name[0].strip()}\"\n",
    "                    else:\n",
    "                        name = name[0].strip()\n",
    "                    authors[i] = name\n",
    "            self.authors = \"; \".join(authors)\n",
    "        self.publication_month: int = data.get(\"publication_month\", \"\")\n",
    "        self.publication_year: int = data.get(\"year\", \"\")\n",
    "        self.published_in = \"\"\n",
    "        for key in [\"journal\", \"conference\", \"journal\"]:\n",
    "            if key in data:\n",
    "                self.published_in = data[key]\n",
    "                break\n",
    "        self.research_field: str = data.get(\"research_field\", \"\")\n",
    "        if not self.research_field:\n",
    "            # TODO: find a way to get the research field\n",
    "            self.research_field = \"R195\"\n",
    "        self.doi: str = data.get(\"doi\", \"\")\n",
    "        self.url: str = data.get(\"url\", \"\")\n",
    "\n",
    "\n",
    "class Contribution:\n",
    "    def __init__(self, paperID, properties={}):\n",
    "        self.paperID: str = paperID\n",
    "        self.properties: dict = flatten_nested_properties(properties)\n",
    "\n",
    "\n",
    "class ORKGComparison:\n",
    "    def __init__(self):\n",
    "        self.papers = {}  # paperID:paper data\n",
    "        self.contibutions: dict[str:Contribution] = {}  # paperID:contribution data\n",
    "        self.properties = {}\n",
    "\n",
    "    def populate(\n",
    "        self,\n",
    "        config: Config,\n",
    "        papers,\n",
    "        instances,\n",
    "        instance_types_dicts,\n",
    "        paper_instance_occurrence_matrix,\n",
    "        papers_metadata,\n",
    "    ):\n",
    "        # Create a dictionary to hold the count of existing values for each property\n",
    "        property_ranges = {\n",
    "            property: sum(\n",
    "                value in instances for value in values\n",
    "            )  # Count how many values exist in 'instances'\n",
    "            for property, values in instance_types_dicts.items()  # Iterate over each property and its values\n",
    "        }\n",
    "        # floor = 0\n",
    "        # for prop, value in property_ranges.items():\n",
    "        #     property_ranges[prop] += floor\n",
    "        #     floor += value\n",
    "\n",
    "        for paperID, paper in enumerate(papers):\n",
    "            paper_data = papers_metadata.get(paper, {})\n",
    "            self.papers[paperID] = Paper(paper, paper_data)\n",
    "            properties = {prop: [] for prop in property_ranges}\n",
    "            floor = 0\n",
    "            for prop, prop_range in property_ranges.items():\n",
    "                for i in range(floor, floor + prop_range):\n",
    "                    if paper_instance_occurrence_matrix[paperID][i] == 1:\n",
    "                        properties[prop].append(instances[i])\n",
    "                floor += prop_range\n",
    "            self.contibutions[paperID] = Contribution(paper, properties)\n",
    "        return self\n",
    "\n",
    "    def populate_properties(self):\n",
    "        for contribution in self.contibutions.values():\n",
    "            for prop, value in contribution.properties.items():\n",
    "                len_values = len(value) if isinstance(value, list) else 1\n",
    "                if prop not in self.properties or self.properties[prop] < len_values:\n",
    "                    self.properties[prop] = len_values\n",
    "        return self.properties\n",
    "\n",
    "    def get(self, key):\n",
    "        if key == \"properties\" and not self.properties:\n",
    "            self.populate_properties()\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def save(self, config: Config, path=None, name=\"orkg_comparison\"):\n",
    "        if path is None:\n",
    "            path = config.orkg_path\n",
    "        filepath = os.path.join(path, name)\n",
    "        if not filepath.endswith(\".csv\"):\n",
    "            filepath += \".csv\"\n",
    "\n",
    "        rows = []\n",
    "        row = [\"paper:\" + prop for prop in Paper.order]\n",
    "        for prop, count in self.get(\"properties\").items():\n",
    "            # row += [f\"contribution:{prop}\"] * count\n",
    "            row += [prop] * count\n",
    "        rows.append(row)\n",
    "\n",
    "        for paperID, contribution in self.contibutions.items():\n",
    "            paper = self.papers[paperID]\n",
    "            row = [getattr(paper, key, \"\") for key in Paper.order]\n",
    "            for prop, count in self.properties.items():\n",
    "                value = contribution.properties.get(prop, \"\")\n",
    "                if not isinstance(value, list):\n",
    "                    value = [value]\n",
    "                len_taken = len(value)\n",
    "                if len_taken < count:\n",
    "                    value += [\"\"] * (count - len_taken)\n",
    "                row += value\n",
    "            rows.append(row)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            for row in rows:\n",
    "                for id, item in enumerate(row):\n",
    "                    if config.csv_separator in item:\n",
    "                        if item.startswith('\"') and item.endswith('\"'):\n",
    "                            continue\n",
    "                        row[id] = '\"' + item + '\"'\n",
    "                f.write(config.csv_separator.join(row) + \"\\n\")\n",
    "\n",
    "\n",
    "# orkg_comparison = ORKGComparison()\n",
    "# orkg_comparison.populate(\n",
    "#     config,\n",
    "#     papers,\n",
    "#     instances,\n",
    "#     instance_types_dicts,\n",
    "#     paper_instance_occurrence_matrix,\n",
    "#     papers_metadata,\n",
    "# )\n",
    "# orkg_comparison.save(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.visualize = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_list(config, instances, \"instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Dicts: instance_types_dicts, papers_metadata, instance_piece_gap\n",
    "# process_dict(config, instance_types_dicts, \"instance_types_dicts\")\n",
    "# process_dict(config, papers_metadata, \"papers_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     paper_instance_occurrence_matrix,\n",
    "#     rows=papers,\n",
    "#     columns=instances,\n",
    "#     name=\"paper_instance_occurrence_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     error_matrix,\n",
    "#     rows=error_papers,\n",
    "#     columns=error_instances,\n",
    "#     name=\"error_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     instance_instance_co_occurrence_matrix,\n",
    "#     rows=instances,\n",
    "#     columns=instances,\n",
    "#     name=\"instance_instance_co_occurrence_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     instance_instance_relative_co_occurrence_matrix,\n",
    "#     rows=instances,\n",
    "#     columns=instances,\n",
    "#     name=\"instance_instance_relative_co_occurrence_matrix\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     instance_instance_proximity_matrix,\n",
    "#     rows=proximity_instances,\n",
    "#     columns=proximity_instances,\n",
    "#     name=\"instance_instance_proximity_matrix\",\n",
    "#     instance_types_dicts=instance_types_dicts,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "## Pre-Processing\n",
    "Using Completion Rating in %\n",
    "\n",
    "### 80 %: Full Text extraction\n",
    "* lacking noise removal (Headings, page numbers, ...)\n",
    "* lacking line-break mending\n",
    "\n",
    "### 100 %: Bag of Words\n",
    "* The problem with BoW that the words are looked at seperatly and correlation is not really clear.\n",
    "\n",
    "\n",
    "### 99 %: TF-IDF\n",
    "* tf-idf only on terms\n",
    "\n",
    "### ? %: Part Of Speech (POS) Tagging, Named Entity Recognition (NER) \n",
    "* ready, but not used currently\n",
    "\n",
    "## Visualize\n",
    "\n",
    "### 85 % Matrix\n",
    "* CSV and Dataframe dumps work fine\n",
    "* Visualization as PNG or SVG are extremely large.\n",
    "  * DPI regulation works to somewhat keep this in check, but images still reach 20 MB\n",
    "* An interactive matrix would be preferred.\n",
    "  * If you hover on a cell, it shows you the x and y label and it's value.\n",
    "\n",
    "### 100 % Timeline\n",
    "* arrange the papers on a timeline and identify the flow of:\n",
    "  * Processes\n",
    "  * File formats\n",
    "  * software\n",
    "  * ...\n",
    "* Additional ideas:\n",
    "  * Compare this to goolge trends\n",
    "\n",
    "### GraphDB\n",
    "* Visualize\n",
    "\n",
    "## Future Work\n",
    "Using Difficulty ranked (DR) solutions:\n",
    "\n",
    "### Step 0: Look it up\n",
    "\n",
    "#### Wikidata linking & more\n",
    "* https://openrefine.org/\n",
    "\n",
    "#### More visualization\n",
    "* https://github.com/JasonKessler/scattertext \n",
    "* https://pypi.org/project/yellowbrick/\n",
    "\n",
    "#### NLP Pipelines:\n",
    "https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "\n",
    "#### BLAST: Basic Local Alignment Search Tool\n",
    "  * starting point: https://academic.oup.com/bioinformatics/article/39/12/btad716/7450067\n",
    "\n",
    "#### AMIE 3\n",
    "  * https://luisgalarraga.de/docs/amie3.pdf\n",
    "  * https://github.com/dig-team/amie\n",
    "\n",
    "### Step 1: Low hanging fruits\n",
    "\n",
    "#### 1/5 DR: multi-word detection (n-gram)\n",
    "Tools:  nltk, spaCy, etc.\n",
    "\n",
    "### Step 2: Not-to-tricky follow-up\n",
    "\n",
    "#### 3/5 DR: Acronym Expansion\n",
    "Tools: spaCy - https://spacy.io/universe/project/neuralcoref\n",
    "\n",
    "#### 3/5 DR: CoReference resolution\n",
    "Tools: spaCy - https://spacy.io/universe/project/neuralcoref or https://huggingface.co/coref/ (you can use the model out of the box)\n",
    "\n",
    "### Step 3: Vector-magic\n",
    "\n",
    "#### 2-4/5 DR: Word embedding\n",
    "* Find out, that jpeg and png are similar\n",
    "\n",
    "(depending on your needs) - Tools: gensim - https://www.analyticsvidhya.com/blog/2023/07/step-by-step-guide-to-word2vec-with-gensim/\n",
    "\n",
    "#### 3/5 DR: document embedding\n",
    "Tools: gensim - https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "I would also check graph embeddings, sentence embeddings, and recently there is LLM2Vec\n",
    "\n",
    "### Step 3.1: Reaping the vector-rewards\n",
    "\n",
    "#### 1/5 DR: clustering\n",
    "Tools: sklearn\n",
    "\n",
    "Requirements: Need to have data as numbers first. This is quite possible after generating embeddings\n",
    "\n",
    "### Step 9: Won't be happening in this paper\n",
    "* Paper classes\n",
    "* Subclasses of paper classes\n",
    "* model which process is a subprocess of another process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
