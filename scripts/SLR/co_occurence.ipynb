{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and modify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: find occurrences of instances in bag of words of papers\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def csv_to_dict_of_sets(csv_file):\n",
    "    dict_of_sets = {}\n",
    "    # try:\n",
    "    #     df = pd.read_csv(csv_file)\n",
    "    # except pd.errors.ParserError:\n",
    "    #     print(\"Error parsing CSV file. Trying again with 'error_bad_lines=False'\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, on_bad_lines='warn', delimiter=\";\",  encoding=\"utf8\")\n",
    "    except:\n",
    "        print(\"Error parsing CSV file. Trying again with 'encoding=ISO-8859-1'\")\n",
    "        df = pd.read_csv(csv_file, on_bad_lines='warn', delimiter=\";\", encoding='ISO-8859-1')\n",
    "    for column in df.columns:\n",
    "        dict_of_sets[column] = set(df[column].str.lower())\n",
    "    # saved_column = df['process'] #you can also use df['column_name']\n",
    "    # delete all that exists in two or more columns\n",
    "    for key in dict_of_sets:\n",
    "        for other_key in dict_of_sets:\n",
    "            if key != other_key:\n",
    "                dict_of_sets[key] = dict_of_sets[key].difference(dict_of_sets[other_key])\n",
    "    return dict_of_sets\n",
    "\n",
    "def count_occurrences(papers, instances):\n",
    "    occurrences = np.zeros((len(papers), len(instances)), dtype=int)\n",
    "\n",
    "    for p, paperpath in enumerate(papers.values()):\n",
    "        with open(paperpath, 'r', encoding=\"utf8\") as f:\n",
    "            paper = json.load(f)\n",
    "            for i, instance in enumerate(instances):\n",
    "                present = True\n",
    "                pieces = instance.split(' ')\n",
    "                for piece in pieces:\n",
    "                    if piece.lower() not in paper['bag_of_words']:\n",
    "                        present = False\n",
    "                        break\n",
    "                    \n",
    "                # if instance == \"system integration\":\n",
    "                #     if \"Liu und Hu - 2013 - A reuse oriented representation model for capturin\" in paperpath:\n",
    "                #         print(present)\n",
    "                if present:\n",
    "                    occurrences[p][i] = 1\n",
    "    return occurrences\n",
    "\n",
    "# ---------------------- Variables ----------------------\n",
    "\n",
    "## instances: A list of all instances, regardless of their type\n",
    "# first all type 1, then all type 2, etc.\n",
    "# if possible, instance sare ordered by their occurrence\n",
    "instances = []\n",
    "\n",
    "## instances_dicts: A dictionary of all different types (columns) of instances\n",
    "#\n",
    "# types:\n",
    "#  - process\n",
    "#  - software\n",
    "#  - data item\n",
    "#  - data model\n",
    "#  - data format specification\n",
    "#  - interchange format\n",
    "#  - source\n",
    "#\n",
    "# instances_dicts['process']: A set of all instances of the type 'process'\n",
    "#\n",
    "instance_types_dicts = {}\n",
    "\n",
    "## paper_nlp_dict: A dictionary of all papers and their NLP data (as dict)\n",
    "paper_nlp_paths = {}\n",
    "\n",
    "## occurrences: A matrix of binary occurrences of instances in papers\n",
    "#\n",
    "# rows: papers\n",
    "# columns: instances\n",
    "# cells: 1 if instance is present in paper, 0 otherwise\n",
    "#\n",
    "paper_instance_occurrence_matrix = np.zeros((), dtype=int)\n",
    "\n",
    "csv_file = 'C:/workspace/borgnetzwerk/tools/scripts/SLR/data.csv'\n",
    "paperspath = 'G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/02_nlp'\n",
    "\n",
    "# ---------------------- Main ----------------------\n",
    "\n",
    "# Usage example\n",
    "\n",
    "instance_types_dicts = csv_to_dict_of_sets(csv_file)\n",
    "\n",
    "# delete sources from instances_dicts\n",
    "if 'source' in instance_types_dicts:\n",
    "    instance_types_dicts.pop('source')\n",
    "\n",
    "# merge \"interchange format\" into \"data format specification\"\n",
    "if 'interchange format' in instance_types_dicts:\n",
    "    instance_types_dicts['data format specification'].update(instance_types_dicts['interchange format'])\n",
    "    instance_types_dicts.pop('interchange format')\n",
    "\n",
    "# merge all sets into one set\n",
    "for instance_type in instance_types_dicts:\n",
    "    instances += (instance_types_dicts[instance_type])\n",
    "\n",
    "# drop all non-text instances\n",
    "if np.nan in instances:\n",
    "    instances.remove(np.nan)\n",
    "# print(result)\n",
    "\n",
    "paper_nlp_paths = {}\n",
    "for file in os.listdir(paperspath):\n",
    "    if file.endswith(\".json\"):\n",
    "        paper_nlp_paths[file[:-5]] = os.path.join(paperspath, file)\n",
    "\n",
    "papers = list(paper_nlp_paths.keys())\n",
    "\n",
    "paper_instance_occurrence_matrix = count_occurrences(paper_nlp_paths, instances)\n",
    "\n",
    "\n",
    "# free unneeded memory\n",
    "del csv_file, file, instance_type, paperspath, paper_nlp_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Paper Metadata\n",
    "from bnw_tools.extract import util_zotero\n",
    "\n",
    "papers_metadata = {}\n",
    "\n",
    "bib_resources = util_zotero.BibResources('G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR')\n",
    "\n",
    "for paper in papers:\n",
    "    for entry in bib_resources.entries:\n",
    "        if hasattr(bib_resources.entries[entry], 'file') and paper in bib_resources.entries[entry].file:\n",
    "            papers_metadata[paper] = bib_resources.entries[entry].get_dict()\n",
    "            del bib_resources.entries[entry]\n",
    "            break\n",
    "\n",
    "\n",
    "print(f\"{len(papers_metadata)} out of {len(papers)} papers have metadata.\")\n",
    "\n",
    "# free unneeded memory\n",
    "del bib_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_instances(matrix, instances, instance_type_dict=None, dump = False):\n",
    "    # total occurrences of each instance\n",
    "    instance_occurrences = {}\n",
    "    for i, instance in enumerate(instances):\n",
    "        instance_occurrences[instance] = matrix[:, i].sum()\n",
    "    instance_occurrences = {k: float(v) for k, v in sorted(instance_occurrences.items(), key=lambda item: item[1], reverse=True) if v > 0}\n",
    "\n",
    "    if dump:\n",
    "        with open('instance_occurrences.json', 'w', encoding=\"utf-8\") as f:\n",
    "            json.dump(instance_occurrences, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    if instance_type_dict is not None:\n",
    "        # Instances should be sorted by their type\n",
    "        type_lists = [[] for _ in range(len(instance_type_dict))]\n",
    "        for instance in instance_occurrences:\n",
    "            for type_ID, instance_type in enumerate(instance_type_dict):\n",
    "                if instance in instance_type_dict[instance_type]:\n",
    "                    type_lists[type_ID].append(instance)\n",
    "        instances = [item for sublist in type_lists for item in sublist]\n",
    "    return instances\n",
    "\n",
    "def remove_zeros(matrix, columns=True, rows=True, row_lists=None, column_lists=None):\n",
    "    # remove all columns that are all zeros\n",
    "    if columns:\n",
    "        deleted_columns = np.all(matrix == 0, axis=0)\n",
    "        matrix = matrix[:, ~np.all(matrix == 0, axis=0)]\n",
    "\n",
    "    # remove all rows that are all zeros\n",
    "    if rows:\n",
    "        deleted_rows = np.all(matrix == 0, axis=1)\n",
    "        matrix = matrix[~np.all(matrix == 0, axis=1)]\n",
    "\n",
    "    \n",
    "    return matrix, [deleted_columns, deleted_rows]\n",
    "\n",
    "def update_instances(matrix, instances, instance_type_dict=None, dump = False):\n",
    "    instances = sort_instances(matrix, instances, instance_type_dict, dump)\n",
    "    matrix, deletions = remove_zeros(matrix)\n",
    "    return matrix, instances, deletions\n",
    "\n",
    "paper_instance_occurrence_matrix, instances, deletions = update_instances(paper_instance_occurrence_matrix, instances, instance_types_dicts)\n",
    "\n",
    "def handle_deletions(input, deletions, rows = True):\n",
    "    \"\"\"\n",
    "    input: list, dict or np.ndarray\n",
    "    deletions: list of bools\n",
    "    rows: if True, deletions[1] is used, else deletions[0]\n",
    "    \"\"\"\n",
    "    delID = 1 if rows else 0\n",
    "\n",
    "    if deletions[delID].any():\n",
    "        # rows were deleted, in this case: papers\n",
    "        if isinstance(input, list):\n",
    "            input = [item for i, item in enumerate(input) if not deletions[delID][i]]\n",
    "        elif isinstance(input, dict):\n",
    "            input = {key: item for i, (key, item) in enumerate(input.items()) if not deletions[delID][i]}\n",
    "        elif isinstance(input, np.ndarray):\n",
    "            input = input[~deletions[delID]]\n",
    "    return input\n",
    "\n",
    "papers = handle_deletions(papers, deletions)\n",
    "# free unneeded memory\n",
    "del deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: find occurrences of instances in full text of papers\n",
    "GAP_TOO_LARGE_THRESHOLD = 1000\n",
    "\n",
    "# get all text files\n",
    "def get_paper_full_text(directory):\n",
    "    paper_full_text = {}\n",
    "    for folder in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    paper_full_text[file[:-4]] = file_path\n",
    "                    break\n",
    "\n",
    "    return paper_full_text\n",
    "\n",
    "paper_full_text = get_paper_full_text('G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/00_PDFs')\n",
    "\n",
    "def find_pos_in_paper(papers, paper_full_text, instances, paper_instance_occurrence_matrix):\n",
    "    # find all occurrences of instances in text files\n",
    "    pos_in_paper = {}\n",
    "\n",
    "    for paperID, paper in enumerate(papers):\n",
    "        if paperID % 100 == 0:\n",
    "            # print(f\"Processing paper {paperID} of {len(papers)}\")\n",
    "            continue\n",
    "        if paper in paper_full_text:\n",
    "            # Full text of paper is available\n",
    "            pos_in_paper[paper] = {}\n",
    "            with open(paper_full_text[paper], 'r', encoding=\"utf8\") as f:\n",
    "                text = f.read().lower()\n",
    "                for i, instance in enumerate(instances):\n",
    "                    # if this instance is not in this document, move on.\n",
    "                    if not paper_instance_occurrence_matrix[paperID][i]:\n",
    "                        continue\n",
    "\n",
    "                    pieces = instance.split(' ')\n",
    "                    for piece in pieces:\n",
    "                        piece = piece.lower()\n",
    "                        if piece not in pos_in_paper[paper]:\n",
    "                            pos_in_paper[paper][piece] = []\n",
    "                            pos = 1\n",
    "                            while pos > 0:\n",
    "                                pos = text.find(piece, pos)\n",
    "                                if pos != -1:\n",
    "                                    pos_in_paper[paper][piece].append(pos)\n",
    "                                    # make sure this instance cannot be found again\n",
    "                                    pos += 1\n",
    "                                    # Idea: store the sentence in which the instance was found\n",
    "    return pos_in_paper\n",
    "\n",
    "pos_in_paper = find_pos_in_paper(papers, paper_full_text, instances, paper_instance_occurrence_matrix)\n",
    "\n",
    "# free unneeded memory\n",
    "# del paper_full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: find the gap between the pieces of an instance\n",
    "import sys\n",
    "\n",
    "def find_min_distance(lists):\n",
    "    #TODO: currently, this does not consider stemmed words\n",
    "\n",
    "    # Initialize pointers for each of the lists\n",
    "    pointers = [0] * len(lists)\n",
    "    min_distance = sys.maxsize\n",
    "    for list in lists:\n",
    "        if not list:\n",
    "            # There are cases where e.g. \"system integration\" is not found in full text\n",
    "            # This happens when NLP converts e.g. \"integrated\" to \"integration\"\n",
    "            # example:\n",
    "            # \"Liu und Hu - 2013 - A reuse oriented representation model for capturin\"\n",
    "            # \"system integration\" -> \"integration\" is not found in the full text\n",
    "            return -1\n",
    "    while True:\n",
    "        # Get the current elements from the lists\n",
    "        current_elements = [lists[i][pointers[i]] for i in range(len(lists))]\n",
    "        \n",
    "        # Calculate the current distance\n",
    "        current_min = min(current_elements)\n",
    "        current_max = max(current_elements)\n",
    "        current_distance = current_max - current_min\n",
    "        \n",
    "        # Update the minimum distance\n",
    "        if current_distance < min_distance:\n",
    "            min_distance = current_distance\n",
    "            \n",
    "        # Check if we can move forward in the list containing the minimum element\n",
    "        min_index = current_elements.index(current_min)\n",
    "        \n",
    "        # If the pointer exceeds its list length, exit the loop\n",
    "        for i in range(len(lists)):\n",
    "            if pointers[i] < len(lists[i]) - 1:\n",
    "                break\n",
    "        if pointers[min_index] + 1 >= len(lists[min_index]):\n",
    "            break\n",
    "        \n",
    "        # Otherwise, increment the pointer\n",
    "        pointers[min_index] += 1\n",
    "    \n",
    "    return min_distance\n",
    "\n",
    "# # Test the function with the given lists\n",
    "# lists = [[1, 2, 3, 2, 1000], [50, 1001], [100, 1002, 10000]]\n",
    "# print(find_min_distance(lists))\n",
    "\n",
    "def split_string(string, delimiters = [\" \", \"-\"]):\n",
    "    for delimiter in delimiters:\n",
    "        string = \" \".join(string.split(delimiter))\n",
    "    return string.split()\n",
    "\n",
    "def find_instance_piece_gap(papers, paper_full_text, instances, paper_instance_occurrence_matrix, pos_in_paper):\n",
    "    error_matrix = np.zeros(paper_instance_occurrence_matrix.shape, dtype=int)\n",
    "    instance_piece_gap = {}\n",
    "    for paperID, paper in enumerate(papers):\n",
    "        if paperID % 100 == 0:\n",
    "            # print(f\"Processing paper {paperID} of {len(papers)}\")\n",
    "            continue\n",
    "        if paper in paper_full_text:\n",
    "            for i, instance in enumerate(instances):\n",
    "                # if this instance is not in this document, move on.\n",
    "                #TODO This does not work\n",
    "                if not paper_instance_occurrence_matrix[paperID][i]:\n",
    "                    continue\n",
    "\n",
    "                pieces = split_string(instance)\n",
    "\n",
    "                if len(pieces) > 1:\n",
    "                    # print(f\"Processing {instance} in {paper}\")\n",
    "                    candidate_postions = []\n",
    "                    for piece in pieces:\n",
    "                        candidate_postions.append(pos_in_paper[paper][piece])\n",
    "                    min_distance = find_min_distance(candidate_postions)\n",
    "\n",
    "                    # min_distance_nested = find_min_distance_nested(candidate_postions)\n",
    "                    # print(f\"{instance}: {min_distance} vs {min_distance_nested}\")\n",
    "                    # if min_distance != min_distance_nested:\n",
    "                    #     print(f\"Error: {min_distance} != {min_distance_nested}\")\n",
    "\n",
    "                    # Pieces to far apart are not counted\n",
    "                    if min_distance > GAP_TOO_LARGE_THRESHOLD:\n",
    "                        # print(f\"Gap for {instance} in {paper} ({min_distance} > {GAP_TOO_LARGE_THRESHOLD})\")\n",
    "                        paper_instance_occurrence_matrix[paperID][i] = 0\n",
    "                        error_matrix[paperID][i] = min_distance\n",
    "                    \n",
    "                    # Some pieces may not be found in the full text\n",
    "                    if min_distance == -1:\n",
    "                        # print(f\"{instance} not found in {paper} at all\")\n",
    "                        paper_instance_occurrence_matrix[paperID][i] = 0\n",
    "                        error_matrix[paperID][i] = min_distance\n",
    "                        # for these, we do not store the gap                    \n",
    "                        continue\n",
    "\n",
    "                    if instance not in instance_piece_gap:\n",
    "                        instance_piece_gap[instance] = {}\n",
    "                    instance_piece_gap[instance][paper] = min_distance\n",
    "    return instance_piece_gap, error_matrix\n",
    "\n",
    "instance_piece_gap, error_matrix = find_instance_piece_gap(papers, paper_full_text, instances, paper_instance_occurrence_matrix, pos_in_paper)\n",
    "\n",
    "error_matrix, has_error = remove_zeros(error_matrix)\n",
    "error_papers = handle_deletions(papers, has_error)\n",
    "error_instances = handle_deletions(instances, has_error, rows = False)\n",
    "\n",
    "paper_instance_occurrence_matrix, instances, deletions = update_instances(paper_instance_occurrence_matrix, instances, instance_types_dicts)\n",
    "\n",
    "papers = handle_deletions(papers, deletions)\n",
    "pos_in_paper = handle_deletions(pos_in_paper, deletions)\n",
    "\n",
    "instance_instance_co_occurrence_matrix = np.dot(paper_instance_occurrence_matrix.T, paper_instance_occurrence_matrix)\n",
    "\n",
    "# free unneeded memory\n",
    "del deletions, has_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Complete\n",
    "\n",
    "We now have:\n",
    "\n",
    "| Variable                          | Type    | Size         | Comments |\n",
    "|-----------------------------------|---------|--------------|----------|\n",
    "| error_instances                   | list    | 165          | Comments |\n",
    "| error_matrix                      | ndarray | (999, 165)   | Comments |\n",
    "| error_papers                      | list    | 999          | Comments |\n",
    "| gap_too_large_threshold           | int     | n.a.         | Comments |\n",
    "| instance_piece_gap                | dict    | 151          | Comments |\n",
    "| instance_types_dicts              | dict    | 5            | Comments |\n",
    "| instances                         | list    | 315          | Comments |\n",
    "| paper_full_text                   | dict    | 1029         | Comments |\n",
    "| paper_instance_occurrence_matrix  | ndarray | (1003, 315)  | Comments |\n",
    "| papers                            | list    | 1003         | Comments |\n",
    "| pos_in_paper                      | dict    | 1003         | Comments |\n",
    "\n",
    "Consisting of:\n",
    "* The paper_instance_occurrence_matrix, binary listing if a term (instance) is present in a paper\n",
    "  * papers x instances\n",
    "* The error_matrix, of all instances that were dropped from the paper_instance_occurrence_matrix\n",
    "  * error_papers x error_instances\n",
    "\n",
    "And some leftover variables:\n",
    "* instance_types_dicts, listing all instance types (\"process\", \"software\", ...) and their respective instance sets (\"Curation\", \"Knowledge Work\", ...)\n",
    "* paper_full_text, containing each papers full text\n",
    "  * pos_in_paper, listing for each paper: for each instance: each position of that instance in that papers full text.\n",
    "* instance_piece_gap, a dict listing all instances made up from compound words (e.g. \"Knowledge Work\", and their minimum distance in each papers full text)\n",
    "  * gap_too_large_threshold, defining how far appart a finding of \"Knowledge\" and \"Work\" would qualify as \"Knowledge Work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare proximity of all instances with one antoher\n",
    "def calculate_proximity_matrix(pos_in_paper, instances):\n",
    "\n",
    "    # create a np zeros matrix of size instances x instances\n",
    "    instance_instance_proximity_matrix = np.zeros((len(instances), len(instances)), dtype=float)\n",
    "\n",
    "    mode = \"sqrt\"\n",
    "    # alternatives are:\n",
    "    # \"sqrt\" - 1 / (square root of the distance)\n",
    "    # \"linear\" - 1 / distance\n",
    "    # \"binary\" - 1 if distance < MAX_GAP_THRESHOLD, 0 otherwise\n",
    "    # \"log\" - 1 / log(distance) \n",
    "\n",
    "    for paper in pos_in_paper:\n",
    "        for id1, instance1 in enumerate(instances):\n",
    "            for id2, instance2 in enumerate(instances):\n",
    "                if instance1 != instance2:\n",
    "                    if instance1 not in pos_in_paper[paper]:\n",
    "                        continue\n",
    "                    if instance2 not in pos_in_paper[paper]:\n",
    "                        continue\n",
    "                    positions1 = pos_in_paper[paper][instance1]\n",
    "                    positions2 = pos_in_paper[paper][instance2]\n",
    "                    if positions1 and positions2:\n",
    "                        distance = find_min_distance([positions1, positions2])\n",
    "                        if distance < 0:\n",
    "                            # print(f\"Error: {instance1} and {instance2} not found in {paper}\")\n",
    "                            continue\n",
    "                        \n",
    "                        result = 0.0\n",
    "                        if distance == 0:\n",
    "                            result = 1\n",
    "                        elif mode == \"sqrt\":\n",
    "                            result = 1 / np.sqrt(distance)\n",
    "                        elif mode == \"linear\":\n",
    "                            result = 1 / distance\n",
    "                        elif mode == \"binary\":\n",
    "                            result = 1 if distance < GAP_TOO_LARGE_THRESHOLD else 0\n",
    "                        elif mode == \"log\":\n",
    "                            result = 1 / np.log(distance)\n",
    "                        else:\n",
    "                            print(\"Error: unknown mode\")\n",
    "                            break\n",
    "                        if result > 0.0:\n",
    "                            instance_instance_proximity_matrix[id1][id2] += result\n",
    "\n",
    "    #TODO rest doesnt seem to work, short fix implemented:\n",
    "    # create a copy of labels that only contains instances that are in the proximity matrix\n",
    "\n",
    "    instance_instance_proximity_matrix, deletions = remove_zeros(instance_instance_proximity_matrix)\n",
    "    proximity_instances = handle_deletions(instances, deletions, rows=False)\n",
    "    \n",
    "    return instance_instance_proximity_matrix, proximity_instances\n",
    "\n",
    "instance_instance_proximity_matrix, proximity_instances = calculate_proximity_matrix(pos_in_paper, instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo for quantitative analysis\n",
    "* tf-idf only on terms\n",
    "* arrange the papers on a timeline and identify the flow of:\n",
    "  * Processes\n",
    "  * File formats\n",
    "  * software\n",
    "  * ...\n",
    "  * Compare this to goolge trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "# AttributeError: 'numpy.ndarray' object has no attribute 'dtypes'\n",
    "dataframe = pd.DataFrame(paper_instance_occurrence_matrix, columns=instances).astype(bool)\n",
    "\n",
    "# for each process:\n",
    "# create one res\n",
    "\n",
    "res = apriori(dataframe, min_support=0.4, use_colnames=True, max_len=2)\n",
    "\n",
    "# visualize res\n",
    "res = res.sort_values(by='support', ascending=False)\n",
    "res = res.reset_index(drop=True)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "rules = association_rules(res)\n",
    "# sort rules by confidence\n",
    "rules = rules.sort_values(by='confidence', ascending=False)\n",
    "# rules = rules.sort_values(by='lift', ascending=False) # (propably most important)\n",
    "# rules = rules.sort_values(by='leverage', ascending=False)\n",
    "# export rules to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_type = [False] * len(rules)\n",
    "\n",
    "for i, antecentent in enumerate(rules.antecedents):\n",
    "    antecentent, = antecentent\n",
    "    consequent, = rules.iloc[i].consequents\n",
    "    type1, type2 = None, None\n",
    "    for type in instance_types_dicts:\n",
    "        if antecentent in instance_types_dicts[type]:\n",
    "            type1 = type\n",
    "        if consequent in instance_types_dicts[type]:\n",
    "            type2 = type\n",
    "        if type1 and type2:\n",
    "            break\n",
    "    if type1 != type2:\n",
    "        cross_type[i] = True\n",
    "        # print(rules.iloc[i])\n",
    "\n",
    "# create a copy for all rules that are cross type\n",
    "cross_type_rules = rules[cross_type].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = True\n",
    "visualize = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represent a dict\n",
    "import csv\n",
    "import os\n",
    "from itables import init_notebook_mode, show\n",
    "\n",
    "# better represent dataframes\n",
    "init_notebook_mode(all_interactive=True)\n",
    "\n",
    "def get_output_path(path = None):\n",
    "    if path is not None:\n",
    "        return path\n",
    "    else:\n",
    "        # return 'C:/workspace/borgnetzwerk/tools/scripts/SLR/'\n",
    "        return ''\n",
    "    \n",
    "def process_dict(input_dict, filename=\"some_dict\", path=None):\n",
    "    # convert all sets to lists\n",
    "    for key in input_dict:\n",
    "        if isinstance(input_dict[key], set):\n",
    "            input_dict[key] = list(input_dict[key])\n",
    "\n",
    "    if path is None:\n",
    "        path = get_output_path()\n",
    "\n",
    "    with open(filename + '.json', 'w', encoding=\"utf-8\") as f:\n",
    "        json.dump(input_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        \n",
    "    container = [\n",
    "        [\"Instance\", \"Min\", \"Max\", \"Mean\", \"Median\", \"Std\"]\n",
    "    ]\n",
    "\n",
    "    for instance, papers in instance_piece_gap.items():\n",
    "\n",
    "        # print(f\"Instance: {instance}\")\n",
    "        gaps = papers.values()\n",
    "        # generate all kinds of statistical values\n",
    "        min_gap = min(gaps)\n",
    "        max_gap = max(gaps)\n",
    "        mean_gap = sum(gaps) / len(gaps)\n",
    "        median_gap = np.median(list(gaps))\n",
    "        std_gap = np.std(list(gaps))\n",
    "        container.append([instance, min_gap, max_gap, mean_gap, median_gap, std_gap])\n",
    "\n",
    "    filepath = os.path.join(path, filename)\n",
    "\n",
    "    # write to csv\n",
    "    with open(filepath + \".csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=';')\n",
    "        writer.writerows(container)   \n",
    "\n",
    "def process_dataframe(input_df, name = \"some_df\", path=None):\n",
    "    if path is None:\n",
    "        path = get_output_path()\n",
    "    show(input_df)\n",
    "    filepath = os.path.join(path, name)\n",
    "    input_df.to_csv(filepath + '.csv', sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize co-occurrences\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import math\n",
    "\n",
    "def visualize_matrix(matrix: np.ndarray, rows: list[str], columns: list[str] = None, name: str = 'some_matrix', format = '.png') -> None:\n",
    "    \"\"\"\n",
    "    Visualizes a matrix as a heatmap.\n",
    "    matrix: The matrix to visualize\n",
    "    rows: The labels for the rows\n",
    "    columns: The labels for the columns\n",
    "    name: The name of the file to save\n",
    "    format: The format of the file to save (default: '.png', also accepts '.svg' and '.pdf', also accepts a list of formats)\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = rows\n",
    "\n",
    "    ## Calculate the maximum size of the plot\n",
    "    dpi = 300\n",
    "    max_pixel = 2**16  # Maximum size in any direction\n",
    "    max_size = max_pixel / dpi  # Maximum size in any direction\n",
    "    max_size_total = max_size * max_size # Maximum size in total\n",
    "    max_size_total *= 0.05 # produce smaller files\n",
    "\n",
    "    # Experience value of space required per cell\n",
    "    factor = 0.18\n",
    "    size_x: float = 2 + len(columns) * factor\n",
    "    size_y: float = 2 + len(rows) * factor\n",
    "\n",
    "    while size_x * size_y < max_size_total and dpi < 600:\n",
    "        dpi /= 0.95 \n",
    "        max_size_total *= 0.95\n",
    "\n",
    "    if dpi > 600:\n",
    "        dpi = 600\n",
    "\n",
    "    while size_x * size_y > max_size_total:\n",
    "        dpi *= 0.95 \n",
    "        max_size_total /= 0.95\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(size_x, size_y), dpi=dpi)\n",
    "\n",
    "    cax = ax.matshow(matrix, cmap='viridis')\n",
    "\n",
    "    # use labels from instance_occurrences\n",
    "    ax.set_xticks(range(len(columns)))\n",
    "    ax.set_xticklabels(list(columns), fontsize=10, rotation=90)\n",
    "    ax.set_yticks(range(len(rows)))\n",
    "    ax.set_yticklabels(list(rows), fontsize=10)\n",
    "\n",
    "    # # adjust the spacing between the labels\n",
    "    # plt.gca().tick_params(axis='x', which='major', pad=15)\n",
    "    # plt.gca().tick_params(axis='y', which='major', pad=15)\n",
    "\n",
    "    # show the number of co-occurrences in each cell, if greater than 0\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(len(matrix[i])):\n",
    "            if matrix[i, j] == 0:\n",
    "                continue\n",
    "            # if co_occurrences[i, j] > 100:\n",
    "            #     continue\n",
    "            plt.text(j, i, round(matrix[i, j], 2), ha='center', va='center', color='white', fontsize=4)\n",
    "\n",
    "    # plt.show()\n",
    "    fig.tight_layout()\n",
    "    if isinstance(format, list):\n",
    "        for f in format:\n",
    "            if f[0] != '.':\n",
    "                f = '.' + f\n",
    "            fig.savefig(name + f)\n",
    "    else:\n",
    "        if format[0] != '.':\n",
    "            format = '.' + format\n",
    "        fig.savefig(name + format)\n",
    "\n",
    "def visualize_matrix_graph(matrix, instances, instance_types_dicts, name='some_matrix_graph', path=None):\n",
    "    path = get_output_path(path)\n",
    "\n",
    "    SEED = 17\n",
    "    K_SPRRING = 18\n",
    "\n",
    "    scale = len(instances) * .12\n",
    "    # Create a new figure\n",
    "    x = scale / 10 * 16\n",
    "    y = scale / 10 * 9\n",
    "    fig = plt.figure(figsize=(x, y))\n",
    "\n",
    "    # normalize the proximity matrix\n",
    "    matrix = matrix / matrix.max()\n",
    "\n",
    "    mode = \"sqrt\"\n",
    "\n",
    "    # alternatives are:\n",
    "    # \"linear\" - take proximity as is\n",
    "    # \"sqrt\" - sqrt(proximity)\n",
    "    # \"log\" - log(proximity)\n",
    "    if mode == \"log\":\n",
    "        nodesize_map = [np.log(matrix[:, i].sum() + 1) for i in range(len(instances))]\n",
    "    elif mode == \"sqrt\":\n",
    "        nodesize_map = [np.sqrt(matrix[:, i].sum()) for i in range(len(instances))]\n",
    "    elif mode == \"linear\":\n",
    "        nodesize_map = [matrix[:, i].sum()for i in range(len(instances))]\n",
    "    else:\n",
    "        nodesize_map = [matrix[:, i].sum() for i in range(len(instances))]\n",
    "        \n",
    "    # print(max(nodesize_map))\n",
    "    # print(min(nodesize_map))\n",
    "\n",
    "    nodesize_map = np.array(nodesize_map) / max(nodesize_map) * 1000\n",
    "\n",
    "    # print(max(nodesize_map))\n",
    "    # print(min(nodesize_map))\n",
    "\n",
    "    # take the root of the proximity matrix\n",
    "    while np.min(matrix[np.nonzero(matrix)]) < 1/10:\n",
    "        matrix = np.sqrt(matrix)\n",
    "\n",
    "    # Create a graph from the proximity matrix\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "\n",
    "    # Specify the layout\n",
    "    pos = nx.spring_layout(G, seed=SEED, k=K_SPRRING/math.sqrt(G.order()))  # Seed for reproducibility\n",
    "\n",
    "    color_map = []\n",
    "\n",
    "    color = {\n",
    "        \"process\": \"#1f77b4\",  # muted blue\n",
    "        \"software\": \"#ff7f0e\",  # safety orange\n",
    "        \"data item\": \"#2ca02c\",  # cooked asparagus green\n",
    "        \"data model\": \"#d62728\",  # brick red\n",
    "        \"data format specification\": \"#9467bd\",  # muted purple\n",
    "        \"interchange format\": \"#8c564b\",  # chestnut brown\n",
    "        # \"source\": \"#e377c2\",  # raspberry yogurt pink\n",
    "    }\n",
    "\n",
    "    for instance in instances:\n",
    "        added = False\n",
    "        for instance_type in instance_types_dicts:\n",
    "            if instance in instance_types_dicts[instance_type]:\n",
    "                color_map.append(color[instance_type])\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            color_map.append(\"grey\")\n",
    "\n",
    "    # Draw the graph\n",
    "    options = {\n",
    "        \"edge_color\": \"grey\",\n",
    "        \"linewidths\": 0.5,\n",
    "        \"width\": 0.5,\n",
    "        \"with_labels\": True,  # This will add labels to the nodes\n",
    "        \"labels\": {i: label for i, label in enumerate(instances)},\n",
    "        \"node_color\": color_map,\n",
    "        \"node_size\": nodesize_map,\n",
    "        # \"edge_color\": \"white\",\n",
    "        # \"alpha\": 0.9,\n",
    "    }\n",
    "\n",
    "    # print(nx.is_weighted(G))\n",
    "\n",
    "\n",
    "    # nx.set_edge_attributes(G, values = 1, name = 'weight')\n",
    "\n",
    "    nx.draw(G, pos, **options, ax=fig.add_subplot(111))\n",
    "\n",
    "    # Make the graph more spacious\n",
    "    fig.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.9)\n",
    "\n",
    "    # Create a patch for each color\n",
    "    patches = [mpatches.Patch(color=color[key], label=key) for key in color]\n",
    "\n",
    "    # Add the legend to the graph\n",
    "    plt.legend(handles=patches, loc='upper right', fontsize='x-large')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # save plot to file\n",
    "    fig.savefig(path + name + '.png')\n",
    "    fig.savefig(path + name + '.svg')\n",
    "\n",
    "    # nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "def sankey(matrix, instances, instance_types_dicts, name='some_sankey', path = None):\n",
    "    #TODO: Implement a method to create one graph per Process\n",
    "    path = get_output_path(path)\n",
    "    # Convert the proximity matrix into a list of source nodes, target nodes, and values\n",
    "    sources = []\n",
    "    targets = []\n",
    "    values = []\n",
    "\n",
    "    x_pos=[0] * len(instances)\n",
    "    y_pos=[0] * len(instances)\n",
    "    color_map=[0] * len(instances)\n",
    "\n",
    "    max_types = len(instance_types_dicts)\n",
    "    type_positions = [0.1 + (i / max_types) * 0.8 for i in range(max_types)]\n",
    "\n",
    "    color = {\n",
    "        \"process\": \"#1f77b4\",  # muted blue\n",
    "        \"software\": \"#ff7f0e\",  # safety orange\n",
    "        \"data item\": \"#2ca02c\",  # cooked asparagus green\n",
    "        \"data model\": \"#d62728\",  # brick red\n",
    "        \"data format specification\": \"#9467bd\",  # muted purple\n",
    "        \"interchange format\": \"#8c564b\",  # chestnut brown\n",
    "        # \"source\": \"#e377c2\",  # raspberry yogurt pink\n",
    "    }\n",
    "    color = list(color.values())\n",
    "\n",
    "    space = {}\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        source_type = None\n",
    "\n",
    "        for j in range(matrix.shape[1]):\n",
    "            target_type = None\n",
    "            \n",
    "            for type_depth, type in enumerate(instance_types_dicts):\n",
    "                if instances[i] in instance_types_dicts[type]:\n",
    "                    source_type = type_depth\n",
    "                if proximity_instances[j] in instance_types_dicts[type]:\n",
    "                    target_type = type_depth\n",
    "\n",
    "            # only keep directly forward moving connections\n",
    "            if target_type - source_type != 1:\n",
    "                continue\n",
    "\n",
    "            # only keep forward moving connections\n",
    "            if target_type - source_type <= 0:\n",
    "                continue\n",
    "\n",
    "            if source_type not in space:\n",
    "                space[source_type] = {}\n",
    "            if i not in space[source_type]:\n",
    "                space[source_type][i] = 0\n",
    "            space[source_type][i] += matrix[i][j]\n",
    "            \n",
    "            if target_type not in space:\n",
    "                space[target_type] = {}\n",
    "            if j not in space[target_type]:\n",
    "                space[target_type][j] = 0\n",
    "            space[target_type][j] += matrix[i][j]\n",
    "\n",
    "            x_pos[i] = type_positions[source_type]\n",
    "            x_pos[j] = type_positions[target_type]\n",
    "            color_map[i] = color[source_type]\n",
    "            color_map[j] = color[target_type]\n",
    "            if matrix[i][j] > 0.0:  # Ignore zero values\n",
    "                sources.append(i)\n",
    "                targets.append(j)\n",
    "                values.append(matrix[i][j])\n",
    "\n",
    "    for type in space:\n",
    "        sum_values = sum(space[type].values())\n",
    "        space[type] = {k: v/sum_values for k, v in sorted(space[type].items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    # assign each instance a proper y position\n",
    "    for type in space:\n",
    "        bottom = 0.1\n",
    "        for i, instance in enumerate(space[type]):\n",
    "            y_pos[instance] = bottom\n",
    "            bottom += space[type][instance] * 0.8\n",
    "\n",
    "    nodes = dict(\n",
    "        # pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=proximity_instances,  # Use your labels here\n",
    "        color=color_map,\n",
    "        x=x_pos,\n",
    "        y=y_pos,\n",
    "        align=\"right\",\n",
    "    )\n",
    "\n",
    "    # Create a Sankey diagram\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=nodes,\n",
    "        link=dict(\n",
    "            source=sources,\n",
    "            target=targets,\n",
    "            value=values\n",
    "        )\n",
    "    )])\n",
    "\n",
    "\n",
    "    fig.update_layout(width=1920, height=1080)\n",
    "\n",
    "\n",
    "    fig.update_layout(title_text=\"Sankey Diagram\", font_size=10)\n",
    "    # fig.show()\n",
    "    fig.write_image(path + name + '.png')\n",
    "    fig.write_image(path + name + '.svg')\n",
    "    fig.write_html(path + name + 'sankey.html')\n",
    "\n",
    "# Represent a matrix\n",
    "def matrix_processing(matrix, rows, columns=None, name = 'some_matrix', visualize = True, path = None, instance_types_dicts = None):\n",
    "    if columns is None:\n",
    "        columns = rows\n",
    "    if path is None:\n",
    "        path = get_output_path()\n",
    "    df = pd.DataFrame(matrix, columns=columns, index=rows)\n",
    "    df.to_csv(name + '.csv', sep=';')\n",
    "    if visualize:\n",
    "        if instance_types_dicts:\n",
    "            sankey(matrix, rows, instance_types_dicts, name)\n",
    "            visualize_matrix_graph(matrix, rows, instance_types_dicts, name)\n",
    "        visualize_matrix(matrix, rows, columns, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Dicts: instance_types_dicts, papers_metadata, instance_piece_gap\n",
    "process_dict(instance_types_dicts, 'instance_types_dicts')\n",
    "process_dict(papers_metadata, 'papers_metadata')\n",
    "process_dict(instance_piece_gap, 'instance_piece_gaps')\n",
    "\n",
    "process_dataframe(rules, 'rules')\n",
    "process_dataframe(cross_type_rules, 'cross_type_rules')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_processing(paper_instance_occurrence_matrix, rows=papers, columns=instances, name='paper_instance_occurrence_matrix', visualize=visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_processing(error_matrix, rows=error_papers, columns=error_instances, name='error_matrix', visualize=visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_processing(instance_instance_co_occurrence_matrix, rows=instances, columns=instances, name='instance_instance_co_occurrence_matrix', visualize=visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_processing(instance_instance_proximity_matrix, rows=proximity_instances, columns=proximity_instances, name='proximity', visualize=visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Later\n",
    "* Word Embeedding\n",
    "  * Find out, that jpeg and png are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Much Later\n",
    "Maybe, just maybe, make \n",
    "* Paper classes\n",
    "* Subclasses of paper classes\n",
    "* model which process is a subprocess of another process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
