{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Instance occurrence in Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnw_tools.SLR.config import Config\n",
    "\n",
    "config = {\n",
    "    \"for_git\": True,\n",
    "    \"visualize\": True,\n",
    "    \"csv_separator\": \",\",\n",
    "    \"csv_decimal\": \".\",\n",
    "    ## Should only accepted papers be used for the analysis?\n",
    "    \"only_included_papers\": False,\n",
    "    ## Which instance columns actually indicate properties?\n",
    "    \"properties\": [\"source\"],\n",
    "    \"proximity_mode\": \"sqrt\",\n",
    "    ## Paths\n",
    "    \"base_path\": \"data/\",\n",
    "    \"subset_path\": \"data_subset/\",\n",
    "    \"visualization_path\": \"visualization/\",\n",
    "    \"ontology_path\": \"ontology/\",\n",
    "    \"orkg_path\": \"ORKG/\",\n",
    "    \"folder_path\": \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR\",\n",
    "    \"papers_path\": \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/02_nlp\",\n",
    "    \"review_path\": \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/03_notes\",\n",
    "    \"csv_file\": \"C:/workspace/borgnetzwerk/tools/scripts/SLR/data.csv\",\n",
    "    \"obsidian_path\": \"ontology/obsidian/\",\n",
    "    ## Position in Paper settings\n",
    "    \"gap_too_large_threshold\": 100,\n",
    "    \"savetime_on_fulltext\": False,\n",
    "    \"try_to_save_time\": False,\n",
    "    \"recalculate_pos_in_paper\": False,\n",
    "    \"debug\": True,\n",
    "    ## Wikidata settings\n",
    "    \"wikidata_query_limit\": 20,\n",
    "    ## Graph settings\n",
    "    \"proximity_seed\": 17,\n",
    "    \"proximity_k_spring\": 18,\n",
    "    \"proximity_min_value\": 0.1,\n",
    "}\n",
    "\n",
    "\n",
    "# config = Config(**config)\n",
    "config = Config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnw_tools.SLR.builder import *\n",
    "obsidian_folder = ObsidianFolder(config=config)\n",
    "\n",
    "classes = {label: _class for label, _class in obsidian_folder.classes.items()}\n",
    "instances = {label: instance for label, instance in obsidian_folder.instances.items()}\n",
    "papers = {label: paper for label, paper in obsidian_folder.papers.items()}\n",
    "\n",
    "order = [\"process\", \"software\", \"data item\", \"data model\", \"data format specification\"]\n",
    "pos_in_order = {o: i for i, o in enumerate(order)}\n",
    "instances_by_class = {o: {} for o in order}\n",
    "\n",
    "for label, instance in instances.items():\n",
    "    instance_of = instance.get(\"instance_of\", [])\n",
    "    if not instance_of:\n",
    "        continue\n",
    "    positions = [pos_in_order[i] for i in instance_of if i in pos_in_order]\n",
    "    if not positions:\n",
    "        print(f\"Instance {label} has no class other than {instance_of}\")\n",
    "        continue\n",
    "    class_ = order[min(positions)]\n",
    "    instances_by_class[class_][label] = instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "\n",
    "# Load the OWL file\n",
    "path = \"C:/workspace/borgnetzwerk/tools/scripts/SLR/ontology/slr.owl\"\n",
    "onto = get_ontology(path).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_class_mapping = {\n",
    "    \"data format specification\": 'http://purl.obolibrary.org/obo/IAO_0000098',\n",
    "    \"data item\": 'http://purl.obolibrary.org/obo/IAO_0000027',\n",
    "    \"data model\":'https://www.wikidata.org/wiki/Q1172480',\n",
    "    \"process\": 'http://purl.obolibrary.org/obo/BFO_0000015',\n",
    "    \"software\": 'http://www.ebi.ac.uk/swo/SWO_0000001',\n",
    "}\n",
    "ontology_class_mapping_inv = dict(zip(ontology_class_mapping.values(), ontology_class_mapping.keys()))\n",
    "\n",
    "\n",
    "\n",
    "data_format_specification = onto['http://purl.obolibrary.org/obo/IAO_0000098']\n",
    "data_item = onto['http://purl.obolibrary.org/obo/IAO_0000027']\n",
    "data_model = onto['https://www.wikidata.org/wiki/Q1172480']\n",
    "process = onto['http://purl.obolibrary.org/obo/BFO_0000015']\n",
    "software = onto['http://www.ebi.ac.uk/swo/SWO_0000001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "instances_new = {individual.name: individual for individual in onto.individuals() if onto['Contribution'] not in individual.is_a}\n",
    "\n",
    "instances_new_formalized = {}\n",
    "instance_label_dict = {}\n",
    "for instance_name, instance_new in instances_new.items():\n",
    "    label = instances_new[instance_name].label[0]\n",
    "    # if label not in instances:\n",
    "    #     continue\n",
    "    # instance_old = instances[label]\n",
    "\n",
    "    formal_instance = Instance()\n",
    "    formal_instance.label = label\n",
    "    formal_instance.source = getattr(instance_new, \"source\", [])\n",
    "    # for eID, entity in enumerate(formal_instance.source):\n",
    "    #     if str(entity) in ontology_class_mapping_inv:\n",
    "    #         formal_instance.source[eID] = ontology_class_mapping_inv[entity]\n",
    "\n",
    "\n",
    "    formal_instance.instance_of = getattr(instance_new, \"is_instance_of\", [])\n",
    "    new_list = []\n",
    "    for eID, entity in enumerate(formal_instance.instance_of):\n",
    "        if hasattr(entity, 'name') and entity.name in ontology_class_mapping_inv:\n",
    "            new_list.append(ontology_class_mapping_inv[entity.name])\n",
    "        else:\n",
    "            if isinstance(entity, str):\n",
    "                new_list.append(entity)\n",
    "            else:  \n",
    "                warnings.warn(formal_instance.label + \" has an unknown instance_of:\\n\" + str(entity))\n",
    "    formal_instance.instance_of = new_list\n",
    "\n",
    "    formal_instance.aliases = getattr(instance_new, \"aliase\", [])\n",
    "    formal_instance.wikidata_uri = getattr(instance_new, \"wikidata_uri\", [])\n",
    "    formal_instance.orkg_uri = getattr(instance_new, \"orkg_uri\", [])\n",
    "    \n",
    "\n",
    "    text = formal_instance.label\n",
    "    if formal_instance.instance_of:\n",
    "        try:\n",
    "            text = \", \".join(formal_instance.instance_of) + \": \" + formal_instance.label\n",
    "        except Exception as e:\n",
    "            print(formal_instance.label)\n",
    "            print(formal_instance.__dict__)\n",
    "            raise e\n",
    "    if formal_instance.aliases:\n",
    "        text += \" (aka \" + \" | \".join(formal_instance.aliases) + \")\"\n",
    "    if formal_instance.source:\n",
    "        text += \" [\" + \", \".join(formal_instance.source) + \"]\"\n",
    "    print(text)\n",
    "    instances_new_formalized[instance_name] = formal_instance\n",
    "    instance_label_dict[instance_name] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shifting from old to new instances\n",
    "print(\"Old instances:\", len(instances))\n",
    "print(\"New instances:\", len(instances_new_formalized))\n",
    "\n",
    "# print(\"Instances that were removed:\", len(set(instances.keys()) - set(instances_new_formalized.keys())))\n",
    "# print(\"Instances that were added:\", len(set(instances_new_formalized.keys()) - set(instances.keys())))\n",
    "# print(\"Instances that were kept:\", len(set(instances_new_formalized.keys()).intersection(set(instances.keys()))))\n",
    "\n",
    "instances_old = instances\n",
    "instances = instances_new_formalized\n",
    "\n",
    "order = [\"process\", \"software\", \"data item\", \"data model\", \"data format specification\"]\n",
    "pos_in_order = {o: i for i, o in enumerate(order)}\n",
    "instances_by_class = {o: {} for o in order}\n",
    "\n",
    "for label, instance in instances.items():\n",
    "    instance_of = instance.get(\"instance_of\", [])\n",
    "    if not instance_of:\n",
    "        continue\n",
    "    positions = [pos_in_order[i] for i in instance_of if i in pos_in_order]\n",
    "    if not positions:\n",
    "        print(f\"Instance {label} has no class other than {instance_of}\")\n",
    "        continue\n",
    "    class_ = order[min(positions)]\n",
    "    instances_by_class[class_][label] = instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce to Reviewed papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_papers(papers, reverse=False):\n",
    "    papers = {\n",
    "        x: papers[x]\n",
    "        for x in sorted(\n",
    "            papers,\n",
    "            key=lambda x: (\n",
    "                getattr(papers[x], \"year\", \"9999\")\n",
    "                if hasattr(papers[x], \"year\")\n",
    "                else \"9999\"\n",
    "            ),\n",
    "            reverse=reverse,\n",
    "        )\n",
    "    }\n",
    "    return papers\n",
    "\n",
    "def reduce_to_reviewed_papers(papers, config):\n",
    "    review_path = config.review_path\n",
    "    # todo: sort by review score + average rank\n",
    "    ## TODO: Make this a function that imports more data from the reivew files\n",
    "    included_identifier = {\n",
    "        3: \"review_score:: 3\",\n",
    "        4: \"review_score:: 4\",\n",
    "        5: \"review_score:: 5\",\n",
    "    }\n",
    "    excluded_identifier = {\n",
    "        2: \"review_score:: 2\",\n",
    "        1: \"review_score:: 1\",\n",
    "        0: \"review_score:: 0\",\n",
    "    }\n",
    "    included_papers = {}\n",
    "    excluded_papers = {}\n",
    "    for file in os.listdir(review_path):\n",
    "        if file.endswith(\".md\"):\n",
    "            paper_name = file[:-3]\n",
    "            if paper_name in papers:\n",
    "                paper = papers[paper_name]\n",
    "                if (\n",
    "                    paper_name in included_papers\n",
    "                    or paper_name in excluded_papers\n",
    "                ):\n",
    "                    continue\n",
    "                # check if file contains \"reviewed\"ArithmeticError\n",
    "                with open(\n",
    "                    os.path.join(review_path, file), \"r\", encoding=\"utf8\"\n",
    "                ) as f:\n",
    "                    content = f.read()\n",
    "                    for score, text in included_identifier.items():\n",
    "                        if text in content:\n",
    "                            paper.review_score = score\n",
    "                            included_papers[paper_name] = paper\n",
    "                            break\n",
    "                    for score, text in excluded_identifier.items():\n",
    "                        if text in content:\n",
    "                            excluded_papers[paper_name] = paper\n",
    "                            break\n",
    "    if config.only_included_papers:\n",
    "        papers = {k: v for k, v in included_papers.items()}\n",
    "    papers = sort_papers(papers)\n",
    "    if included_papers:\n",
    "        included_papers = sort_papers(included_papers)\n",
    "    if excluded_papers:\n",
    "        excluded_papers = sort_papers(excluded_papers)\n",
    "    return papers, included_papers, excluded_papers\n",
    "\n",
    "papers, included_papers, excluded_papers = reduce_to_reviewed_papers(papers, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "for name, instance in papers.items():\n",
    "    output[name] = instance.__dict__\n",
    "with open(\"papers.json\", 'w', encoding='utf8') as json_file:\n",
    "    json.dump(output, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Setup done.\n",
    "Proceeding to:\n",
    "\n",
    "## Matrix calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance Occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_occurrences( papers, instances):\n",
    "#     occurrences = np.zeros((len(papers), len(instances)), dtype=int)\n",
    "\n",
    "#     for p, paperpath in enumerate(papers.values()):\n",
    "#         if isinstance(paperpath, dict) or isinstance(paperpath, Instance):\n",
    "#             paperpath = paperpath.get(\"nlp_path\", None)\n",
    "#         with open(paperpath, \"r\", encoding=\"utf8\") as f:\n",
    "#             paper = json.load(f)\n",
    "#             for i, instance in enumerate(instances):\n",
    "#                 present = True\n",
    "#                 pieces = split_string(instance)\n",
    "#                 for piece in pieces:\n",
    "#                     if piece.lower() not in paper[\"bag_of_words\"]:\n",
    "#                         present = False\n",
    "#                         break\n",
    "\n",
    "#                 if present:\n",
    "#                     occurrences[p][i] = 1\n",
    "#     return occurrences\n",
    "\n",
    "\n",
    "def count_occurrences(papers, instances:dict[str,Instance]):\n",
    "    # Create an empty DataFrame with papers as rows and instances as columns\n",
    "    df = pd.DataFrame(0, index=papers.keys(), columns=instances.keys(), dtype=int)\n",
    "\n",
    "    for paper_id, paperpath in papers.items():\n",
    "        if isinstance(paperpath, dict) or isinstance(paperpath, Instance):\n",
    "            paperpath = paperpath.get(\"nlp_path\", None)\n",
    "        with open(paperpath, \"r\", encoding=\"utf8\") as f:\n",
    "            paper = json.load(f)\n",
    "            for instance_iri, instance in instances.items():\n",
    "                instance_label = instance.get(\"label\", instance_iri)\n",
    "                \n",
    "                ### NONE IN INSTACNES\n",
    "\n",
    "                label = instance.get(\"label\", None)\n",
    "                aliases = instance.get(\"aliases\", [])\n",
    "                candidates = set().union([label], [instance_label], aliases)\n",
    "                if None in candidates:\n",
    "                    print(\"Instance without label found:\")\n",
    "                    print(json.dumps(instance.__dict__, indent=4))\n",
    "                    raise ValueError(\"Instance without label found\")\n",
    "                candidates.discard(None)\n",
    "\n",
    "                for name in candidates:\n",
    "                    present = True\n",
    "                    pieces = split_string(name)\n",
    "                    for piece in pieces:\n",
    "                        if piece.lower() not in paper[\"bag_of_words\"]:\n",
    "                            present = False\n",
    "                            break\n",
    "                    if present:\n",
    "                        df.at[paper_id, instance_iri] = 1\n",
    "                        break\n",
    "    return df\n",
    "\n",
    "occurrence_matrix = count_occurrences(papers, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zeros(matrix:pd.DataFrame, columns=True, rows=True):\n",
    "    deleted_columns, deleted_rows = np.array([]), np.array([])\n",
    "\n",
    "    if columns:\n",
    "        deleted_columns = matrix.columns[(matrix == 0).all(axis=0)]\n",
    "        matrix = matrix.loc[:, (matrix != 0).any(axis=0)]\n",
    "\n",
    "    if rows:\n",
    "        deleted_rows = matrix.index[(matrix == 0).all(axis=1)]\n",
    "        matrix = matrix.loc[(matrix != 0).any(axis=1), :]\n",
    "\n",
    "    deletions = [deleted_columns, deleted_rows]\n",
    "    return matrix, deletions\n",
    "\n",
    "def sort_instances(instances, instances_by_class, matrix:pd.DataFrame):\n",
    "    indexed_instances = {\n",
    "        instance: i for i, instance in enumerate(instances.keys())\n",
    "    }\n",
    "\n",
    "    instance_occurrences = {instance_label:sum(matrix[instance_label]) for instance_label in instances.keys()}\n",
    "\n",
    "    sorted_instances = {\n",
    "        k: float(v)\n",
    "        for k, v in sorted(\n",
    "            instance_occurrences.items(), key=lambda item: item[1], reverse=True\n",
    "        )\n",
    "    }\n",
    "\n",
    "    filepath = os.path.join(config.get_output_path(), \"instance_occurrences\")\n",
    "    with open(filepath + \".json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sorted_instances, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    sorted_instance_list = list(sorted_instances.keys())\n",
    "\n",
    "    type_lists = [[] for _ in range(len(instances_by_class))]\n",
    "    for instance in sorted_instance_list:\n",
    "        for type_ID, instance_type in enumerate(instances_by_class):\n",
    "            if instance in instances_by_class[instance_type]:\n",
    "                type_lists[type_ID].append(instance)\n",
    "    type_sorted_instances = [item for sublist in type_lists for item in sublist]\n",
    "\n",
    "    # new_order = [0] * len(sorted_instance_list)\n",
    "    sorted_instances = {}\n",
    "    # for i, instance in enumerate(type_sorted_instances):\n",
    "    for instance in type_sorted_instances:\n",
    "        # new_order[i] = indexed_instances[instance]\n",
    "        sorted_instances[instance] = instances[instance]\n",
    "\n",
    "    # sort all matrixes accordingly\n",
    "    matrix = matrix.reindex(columns=sorted_instances)\n",
    "    \n",
    "    instances = sorted_instances\n",
    "    \n",
    "    return instances, matrix\n",
    "\n",
    "instances, occurrence_matrix = sort_instances(instances, instances_by_class, occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zeros_np(matrix, columns=True, rows=True):\n",
    "    # remove all columns that are all zeros\n",
    "    deleted_columns, deleted_rows = np.array([]), np.array([])\n",
    "\n",
    "    if columns:\n",
    "        deleted_columns = np.all(matrix == 0, axis=0)\n",
    "        matrix = matrix[:, ~np.all(matrix == 0, axis=0)]\n",
    "\n",
    "    # remove all rows that are all zeros\n",
    "    if rows:\n",
    "        deleted_rows = np.all(matrix == 0, axis=1)\n",
    "        matrix = matrix[~np.all(matrix == 0, axis=1)]\n",
    "\n",
    "    deletions = [deleted_columns, deleted_rows]\n",
    "    return matrix, deletions\n",
    "\n",
    "def reorder_matrix_np(matrix:np.array, new_order, cols=True):\n",
    "    if cols:\n",
    "        matrix = matrix[:, new_order]\n",
    "    else:\n",
    "        matrix = matrix[new_order, :]\n",
    "    remove_zeros_np(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all text files\n",
    "def get_paper_full_text(directory, papers:dict[str,Instance]=None):\n",
    "    paper_full_text = {}\n",
    "    for folder in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    paper_full_text[file[:-4]] = file_path\n",
    "                    break\n",
    "    if papers:\n",
    "        for paper_label, paper in papers.items():\n",
    "            paper.__setattr__(\"full_text_path\", paper_full_text.get(paper_label, None))\n",
    "        return papers\n",
    "    else:\n",
    "        return paper_full_text\n",
    "\n",
    "papers = get_paper_full_text(\n",
    "    \"G:/Meine Ablage/SE2A-B42-Aerospace-knowledge-SWARM-SLR/00_PDFs\", papers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_nan(df:pd.DataFrame, col, what):\n",
    "#     # if isinstance(col, list):\n",
    "#     #     for c in col:\n",
    "#     #         replace_nan(df, c, what)\n",
    "#     #     return df\n",
    "#     # nans = df[col].isnull()\n",
    "#     # df.loc[nans, col] = [what for isnan in nans.values if isnan]\n",
    "#     idx = df.isna()\n",
    "#     if idx.empty:\n",
    "#         return df\n",
    "#     df.where(idx, what, inplace=True)\n",
    "#     # df.loc[idx, col] = [what] * idx.sum()\n",
    "#     # df.iloc[idx, col] = what\n",
    "#     return df\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "def replace_nan_with_empty_list(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.map(lambda x: [] if pd.isna(x) else x)\n",
    "\n",
    "\n",
    "class PosInPaper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        papers: dict[str:Instance] = None,\n",
    "        instances: dict[str:Instance] = None,\n",
    "        save = True\n",
    "    ):\n",
    "        self.mode = getattr(config, \"search_in_text_mode\", \"lower\")\n",
    "        self.config = config\n",
    "        self.words = []\n",
    "        self.papers: dict[str:Instance] = {}\n",
    "        self.instances: dict[str:Instance] = {}\n",
    "\n",
    "        self.matches = {}\n",
    "\n",
    "        self.word_occurrences_in_papers = pd.DataFrame(\n",
    "            [[]], index=self.papers.keys(), columns=self.words, dtype=object\n",
    "        )\n",
    "\n",
    "        self.word_combinations = {}\n",
    "        self.word_combination_min_distance = pd.DataFrame(\n",
    "            [[]], index=self.papers.keys(), columns=self.word_combinations.keys(), dtype=int\n",
    "        )\n",
    "\n",
    "        self.instance_word_combinations = {k: [] for k in self.instances.keys()}\n",
    "        self.instance_min_distance_in_papers = pd.DataFrame(\n",
    "            [[]], index=self.papers.keys(), columns=self.instances.keys(), dtype=int\n",
    "        )\n",
    "\n",
    "\n",
    "        self.load()\n",
    "        changes = False\n",
    "        if papers:\n",
    "            updated = self.update_papers(papers)\n",
    "            if updated:\n",
    "                changes = True\n",
    "        if instances:\n",
    "            updated = self.update_instances(instances)\n",
    "            if updated:\n",
    "                changes = True\n",
    "            # self.update_instances(dict(list(instances.items())[0:2]))\n",
    "\n",
    "        if save:\n",
    "            self.save()\n",
    "\n",
    "    def save(self):\n",
    "        # write all data to json, except dataframes to csv\n",
    "        data = {}\n",
    "        for key, value in self.__dict__.items():\n",
    "            if key in [\"papers\", \"instances\", \"config\"]:\n",
    "                continue\n",
    "            if isinstance(value, pd.DataFrame):\n",
    "                if value.empty:\n",
    "                    continue\n",
    "                try:\n",
    "                    if isinstance(value.iloc[0, 0], list):\n",
    "                        # # data[key].to_json(key+'.json', orient='records', lines=True)\n",
    "                        # data[key].to_json(key+'.json')\n",
    "                        # # with open(f\"data/{key}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                        # #     json.dump(data[key].to_json(orient=\"split\"), f, ensure_ascii=False, indent=4)\n",
    "                        result = value.to_json(orient=\"split\")\n",
    "                        parsed = json.loads(result)\n",
    "                        # json.dumps(parsed, indent=4)\n",
    "                        path = f\"{self.config.get_output_path()}/{key}.json\"\n",
    "                        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "                            json.dump(parsed, f, ensure_ascii=False)\n",
    "                    else:\n",
    "                        path = f\"{self.config.get_output_path()}/{key}.csv\"\n",
    "                        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "                            value.to_csv(f, lineterminator='\\n')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving {key}: {e}\")\n",
    "                continue\n",
    "            elif isinstance(value, dict):\n",
    "                data[key] = {}\n",
    "                for k, v in value.items():\n",
    "                    if isinstance(v, Instance):\n",
    "                        data[key][k] = v.__dict__\n",
    "                    if isinstance(v, set):\n",
    "                        data[key][k] = list(v)\n",
    "            elif key == \"word_combinations\":\n",
    "                data[key] = [list(x) for x in value]\n",
    "            elif key == \"matches\":\n",
    "                data[key] = {k: list(v) for k, v in value.items()}\n",
    "            else:\n",
    "                data[key] = value\n",
    "        path = f\"{self.config.get_output_path()}/pos_in_paper.json\"\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def load(self):\n",
    "        # TODO: Load Dataframes, sets, Instance, dicts of anything\n",
    "        for key, value in self.__dict__.items():\n",
    "            path = f\"{self.config.get_output_path()}/{key}.\"\n",
    "            csv_path = path + \"csv\"\n",
    "            json_path = path + \"json\"\n",
    "            if isinstance(value, pd.DataFrame):\n",
    "                if value.empty:\n",
    "                    # do not overwrite\n",
    "                    continue\n",
    "                if os.path.exists(csv_path):\n",
    "                    value = pd.read_csv(csv_path, index_col=0)\n",
    "                    # data[key] = data[key].applymap(literal_eval)\n",
    "                    for col in data[key].columns:\n",
    "                        value[col] = value[col].apply(literal_eval)\n",
    "                elif os.path.exists(json_path):\n",
    "                    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        value = pd.read_json(f, orient=\"split\")\n",
    "\n",
    "            else:\n",
    "                if value:\n",
    "                    # do not overwrite\n",
    "                    continue\n",
    "                elif os.path.exists(json_path):\n",
    "                    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        data[key] = json.load(f)\n",
    "        path = f\"{self.config.get_output_path()}/pos_in_paper.json\"\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error loading data: {e}\")\n",
    "                    return\n",
    "                for key, value in data.items():\n",
    "                    if key == \"word_combinations\":\n",
    "                        value = {frozenset(x): i for i, x in enumerate(value)}\n",
    "                    current = getattr(self, key)\n",
    "                    if isinstance(current, pd.DataFrame):\n",
    "                        continue\n",
    "                    elif isinstance(current, dict):\n",
    "                        if key in [\"papers\", \"instances\"]:\n",
    "                            continue\n",
    "                        if key == \"matches\":\n",
    "                            for k, v in value.items():\n",
    "                                if k not in self.matches:\n",
    "                                    self.matches[k] = set()\n",
    "                                self.matches[k].update(v)\n",
    "                            continue\n",
    "                    if not current:\n",
    "                        setattr(self, key, value)\n",
    "\n",
    "    # def get_word_combinations_from_instances(self):\n",
    "    #     word_combinations = list(self.word_combinations.keys())\n",
    "\n",
    "    def reindex(self):\n",
    "        self.word_occurrences_in_papers = self.word_occurrences_in_papers.reindex(\n",
    "            self.papers.keys(),\n",
    "            columns=self.words\n",
    "        )\n",
    "        self.word_combination_min_distance = self.word_combination_min_distance.reindex(\n",
    "            self.papers.keys(),\n",
    "            columns=self.word_combinations.keys()\n",
    "        )\n",
    "        self.instance_min_distance_in_papers = self.instance_min_distance_in_papers.reindex(\n",
    "            self.papers.keys(),\n",
    "            columns=self.instances.keys()\n",
    "        )\n",
    "\n",
    "    def update_papers(self, papers: dict[str:Instance]):\n",
    "        changes = False\n",
    "        for paper_label, paper in papers.items():\n",
    "            if paper_label not in self.papers:\n",
    "                self.papers[paper_label] = paper\n",
    "                changes = True\n",
    "            self.papers = sort_papers(self.papers)\n",
    "            self.reindex()\n",
    "        return changes\n",
    "\n",
    "    def update_instances(self, instances: dict[str, Instance]):\n",
    "        changes = False\n",
    "        for instance_label, instance in instances.items():\n",
    "            if instance_label not in self.instances:\n",
    "                self.instances[instance_label] = instance\n",
    "                changes = True\n",
    "        if changes:\n",
    "            for instance_label, instance in instances.items():\n",
    "                # candidates = instance.get_all_names().union([instance_label])\n",
    "                candidates = instance.get_all_names()\n",
    "                candidate_words = self.update_words(candidates)\n",
    "                for words in candidate_words.values():\n",
    "                    frozenset_words = frozenset(words)\n",
    "                    pos = len(self.word_combinations)\n",
    "                    if frozenset_words not in self.word_combinations:\n",
    "                        self.word_combinations[frozenset_words] = pos\n",
    "                    else:\n",
    "                        pos = self.word_combinations[frozenset_words]\n",
    "                    if instance_label not in self.instance_word_combinations:\n",
    "                        self.instance_word_combinations[instance_label] = []\n",
    "                    if pos not in self.instance_word_combinations[instance_label]:\n",
    "                        self.instance_word_combinations[instance_label].append(pos)\n",
    "            self.reindex()\n",
    "        return changes\n",
    "\n",
    "    def update_words(self, words):\n",
    "        res = {candidate: [] for candidate in words}\n",
    "        for candidate in words:\n",
    "            for word in split_string(candidate):\n",
    "                if self.mode == \"lower\":\n",
    "                    word = word.lower()\n",
    "                remove_chars = [\"(\", \")\", \"[\", \"]\", \"{\", \"}\", \":\", \";\", \",\", \".\"]\n",
    "                remove_chars += [\"'\", '\"', \"’\", \"‘\", \"”\", \"“\", \"´\", \"`\", \"´´\", \"``\"]\n",
    "                for char in remove_chars:\n",
    "                    word = word.replace(char, \"\")\n",
    "                if word not in self.words:\n",
    "                    self.words.append(word)\n",
    "                res[candidate].append(word)\n",
    "        self.words.sort()\n",
    "        self.matches.update({word: set() for word in self.words})\n",
    "        return res\n",
    "\n",
    "    def find_occurrences_in_texts(self, save = True):\n",
    "        self.word_occurrences_in_papers = replace_nan_with_empty_list(\n",
    "            self.word_occurrences_in_papers\n",
    "        )\n",
    "        discarded_matches = {}\n",
    "        for pID, (paper_label, paper) in enumerate(self.papers.items()):\n",
    "            if pID % (len(self.papers.keys()) // 10) == 0:\n",
    "                print(f\"Progress: \" + str(pID) + \"/\" + str(len(self.papers.keys())) + \" papers\")\n",
    "            if hasattr(paper, \"full_text_path\") and paper.full_text_path:\n",
    "                with open(paper.full_text_path, \"r\", encoding=\"utf8\") as f:\n",
    "                    full_text = f.read()\n",
    "\n",
    "                    # Pre-processing:\n",
    "                    # Reduce text to only letters and numbers\n",
    "                    full_text = re.sub(r\"[^a-zA-Z0-9 ]\", \" \", full_text)\n",
    "                    # Reduce multiple whitespaces to single whitespace\n",
    "                    full_text = re.sub(r\"\\s+\", \" \", full_text)\n",
    "                    \n",
    "                    if self.mode == \"lower\":\n",
    "                        full_text = full_text.lower()\n",
    "\n",
    "                    # TODO: Work with the idea of word based lookup, not character based lookup\n",
    "                    # If we lookup words, we loose \"Wolfram&Heart\" occurences of \"&\" and \"engineer\" in \"engineers\"\n",
    "                    # If we lookup characters, we need to keep track of the length of the words to have a meaningful distance\n",
    "                    # words = split_string(full_text)\n",
    "                    # for wID, word in enumerate(words):\n",
    "                    #     if word in self.words:\n",
    "                    #         self.word_occurrences_in_papers.at[paper_label, word].append(wID)\n",
    "\n",
    "                    for word in self.words:\n",
    "                        # check exact matches with regex first:\n",
    "                        # regex_pattern = r'\\b[ _-]{}[-_ (s)(ing)(d)(ed)]+\\b'\n",
    "                        # pattern = re.compile(regex_pattern.format(re.escape(word)), re.IGNORECASE)\n",
    "                        # positions = [match.start() for match in pattern.finditer(full_text)]\n",
    "\n",
    "                        if self.word_occurrences_in_papers.at[paper_label, word]:\n",
    "                            # if self.word_occurrences_in_papers.at[paper_label, word] == positions:\n",
    "                            continue\n",
    "                        if self.mode == \"lower\":\n",
    "                            word = word.lower()\n",
    "                        pos = full_text.find(word)\n",
    "                        # if pos == -1:\n",
    "                        #     self.word_occurrences_in_papers.at[paper_label, word] = None\n",
    "                        while pos != -1:\n",
    "                            # if pos not in positions:\n",
    "                            #     if word not in discarded_matches:\n",
    "                            #         discarded_matches[word] = set()\n",
    "                            \n",
    "                            correct = True\n",
    "                            \n",
    "                            if pos != 0:\n",
    "                                if full_text[pos - 1].isalpha():\n",
    "                                    correct = False\n",
    "                            if pos + len(word) != len(full_text):\n",
    "                                if full_text[pos + len(word)].isalpha():\n",
    "                                    correct = False\n",
    "                            \n",
    "                            if correct:\n",
    "                                # match is valid\n",
    "                                self.word_occurrences_in_papers.at[\n",
    "                                    paper_label, word\n",
    "                                ].append([pos, word])\n",
    "                            else:\n",
    "                                # lower = max(pos - 20, 0)\n",
    "                                # upper = min(pos + 20 + len(word), len(full_text))\n",
    "                                # discarded_match = full_text[lower:upper]\n",
    "                                # discarded_matches[word].add(discarded_match)\n",
    "                                pass\n",
    "                            \n",
    "\n",
    "                            next = full_text.find(\" \", pos + 1)\n",
    "                            previous = full_text.rfind(\" \", 0, pos)\n",
    "                            self.matches[word].add(full_text[previous:next])\n",
    "\n",
    "                            pos = full_text.find(word, pos + 1)\n",
    "                        \n",
    "        if save:\n",
    "            self.save()\n",
    "\n",
    "    def find_min_distance_by_id(self, paper_label, word_combination):\n",
    "        distance = self.word_combination_min_distance.at[paper_label,word_combination]\n",
    "\n",
    "        if distance == -1:\n",
    "            # word combination not found in paper\n",
    "            return -1\n",
    "        if distance == -2 or pd.isna(distance):\n",
    "            # calculate distance\n",
    "            pass\n",
    "        else:\n",
    "            return distance\n",
    "\n",
    "        # list_ids = self.word_combination_lists[wcID]\n",
    "        words = list(word_combination)\n",
    "        if len(words) == 1:\n",
    "            occurrences = self.word_occurrences_in_papers.at[paper_label,words[0]]\n",
    "            if not occurrences:\n",
    "                self.word_combination_min_distance.at[paper_label,word_combination] = -1\n",
    "                return -1\n",
    "            else:\n",
    "                self.word_combination_min_distance.at[paper_label,word_combination] = 0\n",
    "                return 0\n",
    "        # since we have attached global Word IDs to the occurrences, we need to map to their local position\n",
    "        list_ids_map = {words[i]: i for i in range(len(words))}\n",
    "        # literals = [list(self.words)[i] for i in list_ids]\n",
    "\n",
    "        lit_len = [len(i) for i in words]\n",
    "\n",
    "        for word in words:\n",
    "            if not self.word_occurrences_in_papers.at[paper_label,word]:\n",
    "                self.word_combination_min_distance.at[paper_label,word_combination] = -1\n",
    "                return -1\n",
    "        # Outsourced to optimize\n",
    "        # inputs = [[(x, i) for x in self.word_occurrences_in_papers[paperID][wordID]] for i, wordID in enumerate(list_ids)]\n",
    "        inputs = [\n",
    "            self.word_occurrences_in_papers.at[paper_label,word] for word in words\n",
    "        ]\n",
    "\n",
    "        indices = [lst[0][0] for lst in inputs]\n",
    "        best = float(\"inf\")\n",
    "\n",
    "        for item in sorted(sum(inputs, [])):\n",
    "            if item[0] not in indices:\n",
    "                continue\n",
    "            # indices[list_ids_map[item[1]]] = item[0]\n",
    "            indices[list_ids_map[item[1]]] = item[0]\n",
    "            arr_min = min(indices)\n",
    "            best = min(max(indices) - arr_min - lit_len[indices.index(arr_min)], best)\n",
    "            if best <= 0:\n",
    "                best = 0\n",
    "                break\n",
    "        self.word_combination_min_distance.at[paper_label,word_combination] = best\n",
    "\n",
    "        return best\n",
    "    \n",
    "    def find_all_combinations(self, save = True):\n",
    "\n",
    "        for paper_label in self.papers:\n",
    "            for combination in self.word_combinations:\n",
    "                self.find_min_distance_by_id(paper_label, combination)\n",
    "        if save:\n",
    "            self.save()\n",
    "\n",
    "    def update_instance_min_distances(self, save = True):\n",
    "        combination_list = list(self.word_combinations.keys())\n",
    "        for paper_label in self.papers:\n",
    "            for instance_label, combinations in self.instance_word_combinations.items():\n",
    "                dist = -1\n",
    "                for combination in combinations:\n",
    "                    combination = combination_list[combination]\n",
    "                    distance = self.find_min_distance_by_id(paper_label, combination)\n",
    "                    if distance < dist or dist == -1:\n",
    "                        dist = distance\n",
    "                self.instance_min_distance_in_papers.at[paper_label,instance_label] = dist\n",
    "        if save:\n",
    "            self.save()\n",
    "pos_in_paper = PosInPaper(config, papers, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_in_paper.find_occurrences_in_texts(save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_in_paper.find_all_combinations(save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_in_paper.update_instance_min_distances(save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_in_paper.instance_min_distance_in_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_in_paper.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# director.paper_full_text = paper_full_text\n",
    "\n",
    "# director.builder[\"error_matrix_builder\"] = ErrorMatrixBuilder(director, pos_in_paper)\n",
    "# director.builder[\"error_matrix_builder\"].build()\n",
    "\n",
    "# director.builder[\"error_matrix_builder\"].save()\n",
    "# director.sort_instances()\n",
    "# director.builder[\"occurrence_matrix\"].save()\n",
    "\n",
    "\n",
    "def build_error_matrix(\n",
    "    config: Config, occurrence_matrix: pd.DataFrame, pos_in_paper: PosInPaper\n",
    "):\n",
    "    initial_occurrences = occurrence_matrix.sum().sum()\n",
    "    removed = 0\n",
    "    added = 0\n",
    "    error_matrix = np.zeros(occurrence_matrix.shape, dtype=float)\n",
    "    error_matrix = pd.DataFrame(\n",
    "        error_matrix, index=occurrence_matrix.index, columns=occurrence_matrix.columns\n",
    "    )\n",
    "    for paper_label in occurrence_matrix.index:\n",
    "        for instance_label in occurrence_matrix.columns:\n",
    "            min_distance = pos_in_paper.instance_min_distance_in_papers.at[\n",
    "                paper_label, instance_label\n",
    "            ]\n",
    "            current = occurrence_matrix.at[paper_label, instance_label]\n",
    "            if not isinstance(current, np.int32):\n",
    "                print(f\"Wrong data type ({type(current)}) at {instance_label} : {paper_label}.\")\n",
    "                print(f\"Data: {current}\")\n",
    "                raise ValueError(\"Wrong data in occurrence_matrix\")\n",
    "            if min_distance is None:\n",
    "                pass\n",
    "            if min_distance > config.gap_too_large_threshold:\n",
    "                # print(f\"Gap for {instance} in {paper} ({min_distance} > {GAP_TOO_LARGE_THRESHOLD})\")\n",
    "                if current:\n",
    "                    occurrence_matrix.at[paper_label, instance_label] = 0\n",
    "                    removed += 1\n",
    "                # get log base 10 of min distance\n",
    "                error_matrix.at[paper_label, instance_label] = round(\n",
    "                    np.log10(min_distance), 1\n",
    "                )\n",
    "\n",
    "            # Some pieces may not be found in the full text\n",
    "            elif min_distance == -1:\n",
    "                # print(f\"{instance} not found in {paper} at all\")\n",
    "                if current:\n",
    "                    occurrence_matrix.at[paper_label, instance_label] = 0\n",
    "                    removed += 1\n",
    "                error_matrix.at[paper_label, instance_label] = min_distance\n",
    "                # for these, we do not store the gap\n",
    "                continue\n",
    "            elif not current:\n",
    "                occurrence_matrix.at[paper_label, instance_label] = 1\n",
    "                added += 1\n",
    "    final_occurrences = occurrence_matrix.sum().sum()\n",
    "\n",
    "    print(f\"Corrected occurrence_matrix from {initial_occurrences} to {final_occurrences} occurrences.\")\n",
    "    print(f\"Removed {removed} and added {added} instance occurrences.\")\n",
    "\n",
    "\n",
    "    return error_matrix\n",
    "\n",
    "\n",
    "error_matrix = build_error_matrix(config, occurrence_matrix, pos_in_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reduce_to_existing(input_dict, matrix, axis=0, name=\"\"):\n",
    "#     before = len(input_dict)\n",
    "#     presumed_name = [\"columns\", \"rows\"]\n",
    "#     if not name:\n",
    "#         name = presumed_name[axis]\n",
    "\n",
    "#     if axis == 0:\n",
    "#         # columns, likely instances\n",
    "#         input_dict = {k: v for k, v in input_dict.items() if k in matrix.columns}\n",
    "#     else:\n",
    "#         # rows, likely papers\n",
    "#         input_dict = {k: v for k, v in input_dict.items() if k in matrix.index}\n",
    "#     after = len(input_dict)\n",
    "#     if before != after:\n",
    "#         print(f\"Removed {before - after} {name}\")\n",
    "#     return input_dict\n",
    "\n",
    "# instances = reduce_to_existing(instances, occurrence_matrix, axis=0, name=\"instances\")\n",
    "# papers = reduce_to_existing(papers, occurrence_matrix, axis=1, name=\"papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances, occurrence_matrix = sort_instances(instances, instances_by_class, occurrence_matrix)\n",
    "error_matrix = error_matrix.reindex(index=occurrence_matrix.index, columns=occurrence_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# occurrence_matrix, deletions = remove_zeros(occurrence_matrix)\n",
    "# error_matrix, deletions = remove_zeros(error_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance_instance Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_instance_co_occurrence_matrix = occurrence_matrix.T.dot(occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_instance_co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if nan values are present in instance_instance_co_occurrence_matrix\n",
    "def clean_nans(matrix:pd.DataFrame):\n",
    "    nans = matrix.isna().sum().sum()\n",
    "    if nans:\n",
    "        print(f\"{nans} NaN values present in matrix\")\n",
    "        matrix.fillna(0, inplace=True)\n",
    "    return matrix\n",
    "\n",
    "instance_instance_co_occurrence_matrix = clean_nans(instance_instance_co_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if nan values are present in instance_instance_co_occurrence_matrix\n",
    "def clean_nans(matrix:pd.DataFrame):\n",
    "    nans = matrix.isna().sum().sum()\n",
    "    if nans:\n",
    "        print(f\"{nans} NaN values present in matrix\")\n",
    "        matrix.fillna(0, inplace=True)\n",
    "    return matrix\n",
    "\n",
    "instance_instance_co_occurrence_matrix = clean_nans(instance_instance_co_occurrence_matrix)\n",
    "\n",
    "# Get the diagonal elements and make a copy\n",
    "diag_elements = np.diag(instance_instance_co_occurrence_matrix).copy()\n",
    "\n",
    "# Replace zero diagonal elements to avoid division by zero\n",
    "diag_elements[diag_elements == 0] = 1\n",
    "\n",
    "instance_instance_relative_co_occurrence_matrix = (\n",
    "    instance_instance_co_occurrence_matrix\n",
    "    # / np.diag(instance_instance_co_occurrence_matrix)\n",
    "    / diag_elements\n",
    ")\n",
    "\n",
    "instance_instance_relative_co_occurrence_matrix = round(instance_instance_relative_co_occurrence_matrix, 3)\n",
    "instance_instance_relative_co_occurrence_matrix = clean_nans(instance_instance_relative_co_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_instance_relative_co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_year_paper_occurrence_matrix(\n",
    "    paper_instance_occurrence_matrix: pd.DataFrame, papers, is_error_matrix=False\n",
    "):\n",
    "    year_papers = {}\n",
    "    for paper, instance in papers.items():\n",
    "        if hasattr(instance, \"year\"):\n",
    "            year = int(getattr(instance, \"year\"))\n",
    "            if year not in year_papers:\n",
    "                year_papers[year] = {}\n",
    "            year_papers[year][paper] = instance\n",
    "\n",
    "    earliest = min(year_papers)\n",
    "    latest = max(year_papers)\n",
    "    span = latest - earliest + 1\n",
    "\n",
    "    for year in range(earliest, latest):\n",
    "        if year not in year_papers:\n",
    "            year_papers[year] = []\n",
    "\n",
    "    year_papers = {\n",
    "        k: v for k, v in sorted(year_papers.items(), key=lambda item: item[0])\n",
    "    }\n",
    "\n",
    "    if is_error_matrix:\n",
    "        # convert any value != 0 to 1\n",
    "        paper_instance_occurrence_matrix = np.where(\n",
    "            paper_instance_occurrence_matrix != 0, 1, 0\n",
    "        )\n",
    "\n",
    "    # create a year_instance_occurrence matrix from the paper_instance_occurrence_matrix\n",
    "    year_instance_occurrence_matrix = np.zeros(\n",
    "        (span, paper_instance_occurrence_matrix.shape[1]), dtype=int\n",
    "    )\n",
    "\n",
    "    year_instance_occurrence_matrix = pd.DataFrame(\n",
    "        year_instance_occurrence_matrix,\n",
    "        index=list(year_papers.keys()),\n",
    "        columns=paper_instance_occurrence_matrix.columns,\n",
    "    )\n",
    "\n",
    "    for year in year_papers:\n",
    "        np_year = np.zeros(paper_instance_occurrence_matrix.shape[1], dtype=int)\n",
    "        for paper in year_papers[year]:\n",
    "            # add the instance occurrence matrix of the paper to the year matrix\n",
    "            if paper in paper_instance_occurrence_matrix.index:\n",
    "                np_year += paper_instance_occurrence_matrix.loc[paper]\n",
    "        year_instance_occurrence_matrix.loc[year] = np_year\n",
    "\n",
    "    return year_instance_occurrence_matrix, year_papers\n",
    "\n",
    "\n",
    "year_instance_occurrence_matrix, year_papers = build_year_paper_occurrence_matrix(\n",
    "    occurrence_matrix, papers\n",
    ")\n",
    "\n",
    "# year_instance_occurrence_matrix, year_papers = create_year_paper_occurrence_matrix(\n",
    "#     papers_metadata, paper_instance_occurrence_matrix, papers\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_instance_occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Complete\n",
    "\n",
    "We now have:\n",
    "\n",
    "| Variable                          | Type    | Size         | Comments |\n",
    "|-----------------------------------|---------|--------------|----------|\n",
    "| error_instances                   | list    | 165          | Comments |\n",
    "| error_matrix                      | ndarray | (999, 165)   | Comments |\n",
    "| error_papers                      | list    | 999          | Comments |\n",
    "| gap_too_large_threshold           | int     | n.a.         | Comments |\n",
    "| instance_piece_gap                | dict    | 151          | Comments |\n",
    "| instance_types_dicts              | dict    | 5            | Comments |\n",
    "| instances                         | list    | 315          | Comments |\n",
    "| paper_full_text                   | dict    | 1029         | Comments |\n",
    "| paper_instance_occurrence_matrix  | ndarray | (1003, 315)  | Comments |\n",
    "| papers                            | list    | 1003         | Comments |\n",
    "| pos_in_paper                      | dict    | 1003         | Comments |\n",
    "\n",
    "Consisting of:\n",
    "* The paper_instance_occurrence_matrix, binary listing if a term (instance) is present in a paper\n",
    "  * papers x instances\n",
    "* The error_matrix, of all instances that were dropped from the paper_instance_occurrence_matrix\n",
    "  * error_papers x error_instances\n",
    "\n",
    "And some leftover variables:\n",
    "* instance_types_dicts, listing all instance types (\"process\", \"software\", ...) and their respective instance sets (\"Curation\", \"Knowledge Work\", ...)\n",
    "* paper_full_text, containing each papers full text\n",
    "  * pos_in_paper, listing for each paper: for each instance: each position of that instance in that papers full text.\n",
    "* instance_piece_gap, a dict listing all instances made up from compound words (e.g. \"Knowledge Work\", and their minimum distance in each papers full text)\n",
    "  * gap_too_large_threshold, defining how far appart a finding of \"Knowledge\" and \"Work\" would qualify as \"Knowledge Work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~3 min | {( len(papers) * len(instances) ) / (3 * 1000) }seconds  compare proximity of all instances with one antoher\n",
    "# ~8 min right now.\n",
    "# 3 min 30 sec with 164 papers and 339 instances\n",
    "class ProximityMatrixBuilder(MatrixBuilder):\n",
    "    def __init__(self, director:Director, instances = None, papers = None, pos_in_paper = None, mode = \"sqrt\"):\n",
    "        super().__init__(director)\n",
    "\n",
    "        self.instances:dict[str,Instance] = instances or director.instances\n",
    "        self.papers:dict[str,Instance] = papers or director.papers\n",
    "        self.pos_in_paper:PosInPaper = pos_in_paper or director.pos_in_paper\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "    def build_matrix(self, instances = None, papers = None, pos_in_paper = None):\n",
    "        instances = instances or self.instances\n",
    "        papers = papers or self.papers\n",
    "        pos_in_paper = pos_in_paper or self.pos_in_paper\n",
    "\n",
    "        # self.matrix, self.proximity_instances = calculate_proximity_matrix(\n",
    "        #     self.config, pos_in_paper, instances, mode=\"sqrt\"\n",
    "        # )\n",
    "\n",
    "    def build(self):\n",
    "        self.build_matrix()\n",
    "        self.remove_zeros()\n",
    "        self.instances = self.handle_deletions(self.instances)\n",
    "\n",
    "    @time_function\n",
    "    def build_matrix(self,\n",
    "        # config: Config,\n",
    "        # pos_in_paper: PosInPaper,\n",
    "        # instances,\n",
    "        # mode=\"sqrt\",\n",
    "        try_to_save_time=False,\n",
    "    ):\n",
    "        # TODO: Optimize this function.\n",
    "        # each instance needs to have it's occurrences as pieces clustered together, so that only those below max distance are considered\n",
    "\n",
    "        # create a np zeros matrix of size instances x instances\n",
    "        indexed_instances = {instance: i for i, instance in enumerate(self.instances)}\n",
    "\n",
    "        self.matrix = np.zeros(\n",
    "            (len(self.instances), len(self.instances)), dtype=float\n",
    "        )\n",
    "\n",
    "        # alternatives are:\n",
    "        # \"sqrt\" - 1 / (square root of the distance)\n",
    "        # \"linear\" - 1 / distance\n",
    "        # \"binary\" - 1 if distance < MAX_GAP_THRESHOLD, 0 otherwise\n",
    "        # \"log\" - 1 / log(distance)\n",
    "\n",
    "        # There is a chance that pos_in_paper papers and instances are out of sync with the current papers and instances\n",
    "        paperIDs = [\n",
    "            paperID for paperID, name in enumerate(pos_in_paper.papers) if name in self.papers\n",
    "        ]\n",
    "        lID_map = {\n",
    "            indexed_instances[name]: instanceID\n",
    "            for instanceID, name in enumerate(pos_in_paper.literals)\n",
    "            if name in self.instances\n",
    "        }\n",
    "\n",
    "        for id1 in range(len(self.instances)):\n",
    "            # print (f\"Processing {id1} of {len(instances)}: {instance1}\")\n",
    "            for id2 in range(id1 + 1, len(self.instances)):\n",
    "                # FIXME: this resulted in a matrix which was not symmetric.\n",
    "                # That hints at a problem with the calclulation, [id1][id2] and [id2][id1] should be the same\n",
    "                wcID = pos_in_paper.word_combination_index_literal_literal[lID_map[id1]][\n",
    "                    lID_map[id2]\n",
    "                ]\n",
    "                for paperID in paperIDs:\n",
    "                    distance = pos_in_paper.find_min_distance_by_id(paperID, wcID)\n",
    "\n",
    "                    if distance < 0:\n",
    "                        # print(f\"Error: {instance1} and {instance2} not found in {paper}\")\n",
    "                        continue\n",
    "                    result = 0.0\n",
    "                    if distance == 0:\n",
    "                        result = 1\n",
    "                    elif distance == 1:\n",
    "                        result = 1\n",
    "                    elif self.mode == \"sqrt\":\n",
    "                        result = 1 / np.sqrt(distance)\n",
    "                    elif self.mode == \"linear\":\n",
    "                        result = 1 / distance\n",
    "                    elif self.mode == \"binary\":\n",
    "                        result = 1 if distance < config.gap_too_large_threshold else 0\n",
    "                    elif self.mode == \"log\":\n",
    "                        result = 1 / np.log(distance)\n",
    "                    else:\n",
    "                        print(\"Error: unknown mode\")\n",
    "                        break\n",
    "                    if result > 0.0:\n",
    "                        self.matrix[id1][id2] += result\n",
    "                        self.matrix[id2][id1] += result\n",
    "\n",
    "        # TODO rest doesnt seem to work, short fix implemented:\n",
    "        # create a copy of labels that only contains instances that are in the proximity matrix\n",
    "\n",
    "        # instance_instance_proximity_matrix, deletions = remove_zeros(\n",
    "        #     instance_instance_proximity_matrix\n",
    "        # )\n",
    "        # proximity_instances = handle_deletions(instances, deletions, rows=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# director.pos_in_paper = pos_in_paper\n",
    "# director.builder['proximity_matrix'] = ProximityMatrixBuilder(director)\n",
    "# director.builder['proximity_matrix'].build()\n",
    "# # instance_instance_proximity_matrix, proximity_instances = calculate_proximity_matrix(\n",
    "# #     config, pos_in_paper, instances\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "\n",
    "def get_rules(matrix: pd.DataFrame, columns):\n",
    "    # AttributeError: 'numpy.ndarray' object has no attribute 'dtypes'\n",
    "    if not isinstance(matrix, pd.DataFrame):\n",
    "        matrix = pd.DataFrame(matrix, columns=columns)\n",
    "    dataframe = matrix.astype(bool)\n",
    "\n",
    "    # for each process:\n",
    "    # create one res\n",
    "\n",
    "    res = apriori(dataframe, min_support=0.4, use_colnames=True, max_len=2)\n",
    "\n",
    "    # visualize res\n",
    "    res = res.sort_values(by=\"support\", ascending=False)\n",
    "    res = res.reset_index(drop=True)\n",
    "    # res\n",
    "\n",
    "    rules = association_rules(res)\n",
    "    # sort rules by confidence\n",
    "    # rules = rules.sort_values(by='confidence', ascending=False)\n",
    "    rules = rules.sort_values(by=\"lift\", ascending=False)  # (propably most important)\n",
    "    # rules = rules.sort_values(by='leverage', ascending=False)\n",
    "    # export rules to csv\n",
    "    return rules\n",
    "\n",
    "\n",
    "rules = get_rules(occurrence_matrix, list(instances.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnw_tools.SLR.export import process_dataframe\n",
    "process_dataframe(config, rules, \"rules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_cross_type_rules(rules, instances: dict[str, Instance]):\n",
    "    cross_type = [False] * len(rules)\n",
    "\n",
    "    for i, antecentent in enumerate(rules.antecedents):\n",
    "        if not isinstance(antecentent, str):\n",
    "            (antecentent,) = antecentent\n",
    "        consequent = rules.iloc[i].consequents\n",
    "        if not isinstance(consequent, str):\n",
    "            (consequent,) = consequent\n",
    "        type1, type2 = None, None\n",
    "        type1 = instances.get(antecentent, {}).get(\"instance_of\", [None])[0]\n",
    "        type2 = instances.get(consequent, {}).get(\"instance_of\", [None])[0]\n",
    "        if type1 and type2 and type1 != type2:\n",
    "            cross_type[i] = True\n",
    "\n",
    "    # create a copy for all rules that are cross type\n",
    "    rules_cross_type = rules[cross_type].copy()\n",
    "    return rules_cross_type\n",
    "\n",
    "\n",
    "rules_cross_type = identify_cross_type_rules(rules, instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataframe(config, rules_cross_type, \"rules_cross_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, builder in director.builder.items():\n",
    "#     if not hasattr(builder, \"matrix\"):\n",
    "#         continue\n",
    "#     if name in [\"year_instance_occurrence_matrix\"]:\n",
    "#         continue\n",
    "#     rows = []\n",
    "#     candidates = [\"papers\", \"instances\"]\n",
    "#     for candidate in candidates:\n",
    "#         if hasattr(builder, candidate):\n",
    "#             rows = getattr(builder, candidate)\n",
    "#             break\n",
    "#     if not rows:\n",
    "#         raise Exception(f\"Could not find rows for {name}\")\n",
    "#     if isinstance(rows, dict):\n",
    "#         rows = list(rows.keys())\n",
    "    \n",
    "#     cols = []\n",
    "#     candidates = [\"instances\", \"literals\"]\n",
    "#     for candidate in candidates:\n",
    "#         if hasattr(builder, candidate):\n",
    "#             cols = getattr(builder, candidate)\n",
    "#             break\n",
    "#     if not cols:\n",
    "#         raise Exception(f\"Could not find cols for {name}\")\n",
    "#     if isinstance(cols, dict):\n",
    "#         cols = list(cols.keys())\n",
    "    \n",
    "#     builder.save()\n",
    "#     # process_matrix(director.config, builder.matrix, rows, cols, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_csv(\n",
    "    config: Config, matrix: pd.DataFrame, name: str = \"some_matrix\", path=None\n",
    ") -> None:\n",
    "    if path is None:\n",
    "        path = config.get_output_path()\n",
    "    filepath = os.path.join(path, name)\n",
    "    matrix.to_csv(\n",
    "        filepath + \".csv\",\n",
    "        sep=config.csv_separator,\n",
    "        decimal=config.csv_decimal,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_figsize(\n",
    "    config: Config,\n",
    "    size_x: float = None,\n",
    "    size_y: float = None,\n",
    "    column_len: int = None,\n",
    "    row_len: int = None,\n",
    ") -> tuple[float, float]:\n",
    "    if size_x and size_y:\n",
    "        return size_x, size_y\n",
    "    ## Calculate the maximum size of the plot\n",
    "    dpi = 300\n",
    "    max_dpi = 600\n",
    "    if config.for_git:\n",
    "        dpi = 96\n",
    "        max_dpi = 200\n",
    "    max_pixel = 2**16  # Maximum size in any direction\n",
    "    max_size = max_pixel / dpi  # Maximum size in any direction\n",
    "    max_size_total = max_size * max_size  # Maximum size in total\n",
    "    max_size_total *= 0.05  # produce smaller files\n",
    "\n",
    "    # Experience value of space required per cell\n",
    "    factor = 0.18\n",
    "    size_x: float = 2 + column_len * factor\n",
    "    size_y: float = 3 + row_len * 0.8 * factor\n",
    "\n",
    "    while size_x * size_y < max_size_total and dpi < max_dpi:\n",
    "        dpi /= 0.95\n",
    "        max_size_total *= 0.95\n",
    "\n",
    "    if dpi > max_dpi:\n",
    "        dpi = max_dpi\n",
    "\n",
    "    while size_x * size_y > max_size_total:\n",
    "        dpi *= 0.95\n",
    "        max_size_total /= 0.95\n",
    "\n",
    "    size_x = size_x\n",
    "    size_y = size_y\n",
    "    dpi = dpi\n",
    "\n",
    "    return [size_x, size_y], dpi\n",
    "\n",
    "\n",
    "def visualize_matrix(\n",
    "    config: Config,\n",
    "    matrix: pd.DataFrame,\n",
    "    name: str = \"some_matrix\",\n",
    "    format=\".png\",\n",
    "    path=None,\n",
    "    instance_label_dict: dict[str, str] = None,\n",
    ") -> None:\n",
    "    if not path:\n",
    "        path = config.get_output_path(path, visualization=True)\n",
    "    # Ensure all data in the DataFrame is numeric\n",
    "\n",
    "    # keep only entries > 0\n",
    "    matrix = matrix.loc[(matrix.sum(axis=1) != 0), (matrix.sum(axis=0) != 0)]\n",
    "\n",
    "    rows = list(matrix.index)\n",
    "    columns = list(matrix.columns)\n",
    "    \n",
    "    # #rename labels to instance labels\n",
    "    if instance_label_dict:\n",
    "        rows = [instance_label_dict.get(row, row) for row in rows]\n",
    "        columns = [instance_label_dict.get(col, col) for col in columns]\n",
    "\n",
    "    row_sums = list(matrix.sum(axis=1))\n",
    "    ## Removed the summation of the rows/cols since they overcomplicated the visualization\n",
    "    ## rows = [f\"{row} ({row_sums[i]})\" for i, row in enumerate(rows)]\n",
    "    col_sums = list(matrix.sum(axis=0))\n",
    "    ## columns = [f\"({col_sums[i]}) {col}\" for i, col in enumerate(columns)]\n",
    "\n",
    "    # matrix = matrix.apply(pd.to_numeric, errors='coerce')\n",
    "    matrix = matrix.values\n",
    "    figsize, dpi = get_figsize(config, column_len=len(columns), row_len=len(rows))\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "\n",
    "    cax = ax.matshow(matrix, cmap=\"viridis\")\n",
    "\n",
    "    # use labels from instance_occurrences\n",
    "    ax.set_xticks(range(len(columns)))\n",
    "    ax.set_xticklabels(list(columns), fontsize=10, rotation=90)\n",
    "    ax.set_yticks(range(len(rows)))\n",
    "    ax.set_yticklabels(list(rows), fontsize=10)\n",
    "\n",
    "    # # adjust the spacing between the labels\n",
    "    # plt.gca().tick_params(axis='x', which='major', pad=15)\n",
    "    # plt.gca().tick_params(axis='y', which='major', pad=15)\n",
    "\n",
    "    # show the number of co-occurrences in each cell, if greater than 0\n",
    "    force_cell_values = True\n",
    "\n",
    "    if getattr(config, \"visualize_cell_values\", False) or force_cell_values:\n",
    "        # for i, row in matrix.iterrows():\n",
    "        for i, row in enumerate(matrix):\n",
    "            # for j, value in row.items():\n",
    "            for j, value in enumerate(row):\n",
    "                if value == 0:\n",
    "                    continue\n",
    "                # if co_occurrences[i, j] > 100:\n",
    "                #     continue\n",
    "\n",
    "                # make sure the text is at most 3 digits and a dot\n",
    "                decimals = 2\n",
    "                if value > 99:\n",
    "                    decimals = 0\n",
    "                elif value > 9:\n",
    "                    decimals = 1\n",
    "                cell_text = round(value, decimals)\n",
    "                if decimals == 0:\n",
    "                    cell_text = int(cell_text)\n",
    "                plt.text(\n",
    "                    j, i, cell_text, ha=\"center\", va=\"center\", color=\"white\", fontsize=4\n",
    "                )\n",
    "\n",
    "    # plt.show()\n",
    "    try:\n",
    "        fig.tight_layout()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during tight_layout: {e}\")\n",
    "\n",
    "    # title\n",
    "    plt.title(name)\n",
    "\n",
    "    if isinstance(format, list):\n",
    "        for f in format:\n",
    "            if f[0] != \".\":\n",
    "                f = \".\" + f\n",
    "            filepath = os.path.join(path, name + f)\n",
    "            fig.savefig(filepath)\n",
    "            fig.savefig(filepath[:-4] + \".pdf\")\n",
    "    else:\n",
    "        if format[0] != \".\":\n",
    "            format = \".\" + format\n",
    "        filepath = os.path.join(path, name + format)\n",
    "        fig.savefig(filepath)\n",
    "        fig.savefig(filepath[:-4] + \".pdf\")\n",
    "\n",
    "\n",
    "def visualize_matrix_graph(\n",
    "    config: Config,\n",
    "    matrix,\n",
    "    instance_types_dicts,\n",
    "    instances: list[str] = None,\n",
    "    name=\"some_matrix_graph\",\n",
    "    path=None,\n",
    "    node_size_mode=\"sqrt\",\n",
    "    raise_mode=\"prune\",\n",
    "    instance_label_dict: dict[str, str] = None,\n",
    "):\n",
    "    if isinstance(matrix, pd.DataFrame):\n",
    "        # drop all rows and columns that are all 0\n",
    "        instances = list(matrix.columns)\n",
    "        matrix = matrix.values\n",
    "    if not instances:\n",
    "        return\n",
    "\n",
    "    SEED = config.proximity_seed or 17\n",
    "    K_SPRRING = config.proximity_k_spring or 18\n",
    "    MIN_VALUE = config.proximity_min_value or 0.01\n",
    "\n",
    "    matrix = matrix.copy()\n",
    "    \n",
    "    np.fill_diagonal(matrix, 0)\n",
    "\n",
    "    # normalize the proximity matrix\n",
    "    matrix = matrix / matrix.max()\n",
    "\n",
    "    # Make sure the matrix is not completely stretched out\n",
    "    if matrix.min() < MIN_VALUE:\n",
    "        if raise_mode == \"prune\":\n",
    "            # remove every value that is below MIN_VALUE\n",
    "            matrix = np.where(matrix < MIN_VALUE, 0, matrix)\n",
    "            # matrix = matrix.applymap(lambda x: 0 if x < MIN_VALUE else x)\n",
    "\n",
    "        elif raise_mode == \"sqrt\":\n",
    "            # while matrix[matrix > 0].min().min() < MIN_VALUE:\n",
    "            while np.min(matrix[np.nonzero(matrix)]) < MIN_VALUE:\n",
    "                matrix = np.sqrt(matrix)\n",
    "                # matrix = matrix.applymap(np.sqrt)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown raise mode\")\n",
    "\n",
    "    # alternatives are:\n",
    "    # \"linear\" - take proximity as is\n",
    "    # \"sqrt\" - sqrt(proximity)\n",
    "    # \"log\" - log(proximity)\n",
    "    if node_size_mode == \"log\":\n",
    "        # TODO: see how this works with log(1)\n",
    "        nodesize_map = [np.log(matrix[:, i].sum() + 1) for i in range(len(instances))]\n",
    "    elif node_size_mode == \"sqrt\":\n",
    "        nodesize_map = [np.sqrt(matrix[:, i].sum()) for i in range(len(instances))]\n",
    "    elif node_size_mode == \"linear\":\n",
    "        nodesize_map = [matrix[:, i].sum() for i in range(len(instances))]\n",
    "    else:\n",
    "        nodesize_map = [matrix[:, i].sum() for i in range(len(instances))]\n",
    "\n",
    "    # print(max(nodesize_map))\n",
    "    # print(min(nodesize_map))\n",
    "\n",
    "    # drop all nodes that have no connections\n",
    "    drop_map = [xID for xID, x in enumerate(nodesize_map) if x == 0]\n",
    "    for i in reversed(drop_map):\n",
    "        matrix = np.delete(matrix, i, 0)\n",
    "        matrix = np.delete(matrix, i, 1)\n",
    "        nodesize_map.pop(i)\n",
    "        instances.pop(i)\n",
    "\n",
    "    nodesize_map = np.array(nodesize_map) / max(nodesize_map) * 1000\n",
    "\n",
    "    # print(max(nodesize_map))\n",
    "    # print(min(nodesize_map))\n",
    "\n",
    "    # scale = len(instances) * 0.12\n",
    "    scale = len(instances) * 0.15\n",
    "    # Create a new figure\n",
    "    x = scale / 10 * 16\n",
    "    y = scale / 10 * 9\n",
    "    fig = plt.figure(figsize=(x, y))\n",
    "\n",
    "    # Create a graph from the proximity matrix\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "\n",
    "    # Specify the layout\n",
    "    pos = nx.spring_layout(\n",
    "        G, seed=SEED, k=K_SPRRING / math.sqrt(G.order())\n",
    "    )  # Seed for reproducibility\n",
    "\n",
    "    color_map = []\n",
    "\n",
    "    color = {\n",
    "        \"process\": \"#1f77b4\",  # muted blue\n",
    "        \"software\": \"#ff7f0e\",  # safety orange\n",
    "        \"data item\": \"#2ca02c\",  # cooked asparagus green\n",
    "        \"data model\": \"#d62728\",  # brick red\n",
    "        \"data format specification\": \"#9467bd\",  # muted purple\n",
    "        # \"interchange format\": \"#8c564b\",  # chestnut brown\n",
    "        # \"source\": \"#e377c2\",  # raspberry yogurt pink\n",
    "    }\n",
    "\n",
    "    for instance in instances:\n",
    "        # if drop_map and drop_map[xin]:\n",
    "        #     continue\n",
    "        added = False\n",
    "        for instance_type in instance_types_dicts:\n",
    "            if instance in instance_types_dicts[instance_type]:\n",
    "                color_map.append(color[instance_type])\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            color_map.append(\"grey\")\n",
    "\n",
    "    # Draw the graph\n",
    "    options = {\n",
    "        \"edge_color\": \"grey\",\n",
    "        \"linewidths\": 0.5,\n",
    "        \"width\": 0.5,\n",
    "        \"with_labels\": True,  # This will add labels to the nodes\n",
    "        \"labels\": {i: label for i, label in enumerate(instances)},\n",
    "        \"node_color\": color_map,\n",
    "        \"node_size\": nodesize_map,\n",
    "        # \"edge_color\": \"white\",\n",
    "        # \"alpha\": 0.9,\n",
    "    }\n",
    "\n",
    "    if instance_label_dict:\n",
    "        options['labels'] = {i: instance_label_dict.get(label, label) for i, label in enumerate(instances)}\n",
    "\n",
    "    # print(nx.is_weighted(G))\n",
    "\n",
    "    # nx.set_edge_attributes(G, values = 1, name = 'weight')\n",
    "\n",
    "    nx.draw(G, pos, **options, ax=fig.add_subplot(111))\n",
    "\n",
    "    # Make the graph more spacious\n",
    "    fig.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.9)\n",
    "\n",
    "    # Create a patch for each color\n",
    "    patches = [mpatches.Patch(color=color[key], label=key) for key in color]\n",
    "\n",
    "    # Add the legend to the graph\n",
    "    plt.legend(handles=patches, loc=\"upper right\", fontsize=\"x-large\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # save plot to file\n",
    "    filepath = os.path.join(path, name)\n",
    "    fig.savefig(filepath + \".png\")\n",
    "    fig.savefig(filepath + \".svg\")\n",
    "    fig.savefig(filepath + \".pdf\")\n",
    "\n",
    "    # nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "\n",
    "def process_matrix(\n",
    "    config: Config,\n",
    "    matrix: pd.DataFrame,\n",
    "    instances_by_class=None,\n",
    "    name=\"some_matrix\",\n",
    "    path=None,\n",
    "    instances=None,\n",
    "    mode=None,\n",
    "    instance_label_dict: dict[str, str] = None,\n",
    "):\n",
    "    if not path:\n",
    "        path = config.get_output_path(path)\n",
    "    matrix_to_csv(config, matrix, name, path)\n",
    "\n",
    "    if config.visualize:\n",
    "        path = config.get_output_path(path, visualization=True)\n",
    "        try:\n",
    "            visualize_matrix(\n",
    "                config,\n",
    "                matrix,\n",
    "                name,\n",
    "                path=path,\n",
    "                instance_label_dict=instance_label_dict,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during visualization: {e}\")\n",
    "\n",
    "        # if instances:\n",
    "        # if list(matrix.columns) == list(matrix.index):\n",
    "        # if False:\n",
    "        if instances_by_class:\n",
    "            # matrix_to_sankey(\n",
    "            #     config,\n",
    "            #     matrix,\n",
    "            #     name + \"_sankey\",\n",
    "            #     path=path,\n",
    "            # )\n",
    "            try:\n",
    "                if mode:\n",
    "                    visualize_matrix_graph(\n",
    "                        config,\n",
    "                        matrix,\n",
    "                        instances_by_class,\n",
    "                        name=name + \"_graph\",\n",
    "                        path=path,\n",
    "                        node_size_mode=config.proximity_mode,\n",
    "                        instance_label_dict=instance_label_dict,\n",
    "                    )\n",
    "                else:\n",
    "                    visualize_matrix_graph(\n",
    "                        config,\n",
    "                        matrix,\n",
    "                        instances_by_class,\n",
    "                        name=name + \"_graph\",\n",
    "                        path=path,\n",
    "                        instance_label_dict=instance_label_dict,\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print(f\"Error during graph visualization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.visualize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bnw_tools.SLR.export import process_dict, process_matrix\n",
    "\n",
    "process_matrix(\n",
    "    config,\n",
    "    occurrence_matrix,\n",
    "    name=\"paper_instance_occurrence_matrix\",\n",
    "    instance_label_dict=instance_label_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_matrix(\n",
    "    config,\n",
    "    error_matrix,\n",
    "    name=\"error_matrix\",\n",
    "    instance_label_dict=instance_label_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance x Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_matrix(\n",
    "    config,\n",
    "    instance_instance_co_occurrence_matrix,\n",
    "    instances_by_class=instances_by_class,\n",
    "    name=\"instance_instance_co_occurrence_matrix\",\n",
    "    instance_label_dict=instance_label_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_matrix(\n",
    "    config,\n",
    "    instance_instance_relative_co_occurrence_matrix,\n",
    "    instances_by_class=instances_by_class,\n",
    "    name=\"instance_instance_relative_co_occurrence_matrix\",\n",
    "    instance_label_dict=instance_label_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_matrix(\n",
    "#     config,\n",
    "#     instance_instance_proximity_matrix,\n",
    "#     name=\"instance_instance_proximity_matrix\",\n",
    "#     instance_types_dicts=instance_types_dicts,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize timeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def visualize_timeline(\n",
    "    config: Config,\n",
    "    year_instance_occurrence_matrix:pd.DataFrame,\n",
    "    year_papers,\n",
    "    # instances,\n",
    "    instance_types_dicts,\n",
    "    name=\"some_timeline\",\n",
    "    path=None,\n",
    "    recursion_depth=0,\n",
    "    start_index=0,\n",
    "    error_matrix=None,\n",
    "    error_instances=None,\n",
    "    instance_label_dict: dict[str, str] = None,\n",
    "):\n",
    "    if not path:\n",
    "        path = config.get_output_path(path, visualization=True)\n",
    "    years = list(year_papers.keys())\n",
    "    # years = list(year_instance_occurrence_matrix.index)\n",
    "    instances = list(year_instance_occurrence_matrix.columns)\n",
    "    yearly_papers = [len(year_papers[year]) for year in years]\n",
    "    # yearly_papers = year_instance_occurrence_matrix.sum(axis=1)\n",
    "    # max_papers = max([len(year_papers[year]) for year in years])\n",
    "    max_papers = max(yearly_papers)\n",
    "\n",
    "    ALPHA_ERROR_LINE = 0.3\n",
    "    ALPHA_ERROR_ZONE = 0.2\n",
    "    ALPHA_PAPER_BAR = 0.3\n",
    "\n",
    "    for type in instance_types_dicts:\n",
    "        use = [instance in instance_types_dicts[type] for instance in instances]\n",
    "        type_instances = [\n",
    "            instance for instance, use_flag in zip(instances, use) if use_flag\n",
    "        ]\n",
    "        total_occurrences = [\n",
    "            # np.sum(year_instance_occurrence_matrix[:, instances.index(instance)])\n",
    "            year_instance_occurrence_matrix[instance].sum()\n",
    "            for instance in type_instances\n",
    "        ]\n",
    "        type_instances_sorted = [\n",
    "            x \n",
    "            for occurrence, x in sorted(\n",
    "                zip(total_occurrences, type_instances),\n",
    "                key=lambda pair: pair[0],\n",
    "                reverse=True,\n",
    "            )\n",
    "            if occurrence != 0\n",
    "        ]\n",
    "\n",
    "        PARTITION_SIZE = 10\n",
    "        # if error_instances is not None:\n",
    "        #     PARTITION_SIZE = int(0.5 * PARTITION_SIZE)\n",
    "\n",
    "        # type_matrix = year_instance_occurrence_matrix[\n",
    "        #     :, [instances.index(instance) for instance in type_instances_sorted]\n",
    "        # ]\n",
    "\n",
    "        # Get the column indices for type_instances_sorted\n",
    "        column_indices = [year_instance_occurrence_matrix.columns.get_loc(instance) for instance in type_instances_sorted]\n",
    "\n",
    "        # Select the rows and the specified columns using iloc\n",
    "        type_matrix = year_instance_occurrence_matrix.iloc[:, column_indices]\n",
    "        \n",
    "\n",
    "        factor = 1\n",
    "        size_x = (2 + len(years) / 6) * factor\n",
    "        size_y = (2 + max_papers / 15) * factor\n",
    "        size_y_2 = (2 + PARTITION_SIZE / 2) * factor\n",
    "        size_y = max(size_y, size_y_2)\n",
    "        fig, ax = plt.subplots(figsize=(size_x, size_y), dpi=300)\n",
    "\n",
    "        ax.set_xticks(range(len(years)))\n",
    "        # years_labels = [year if len(year_papers[year]) > 0 else \"\" for year in years]\n",
    "        # years_labels = [year if yearly_papers[year] > 0 else \"\" for year in years]\n",
    "        years_labels = [year if yearly_papers[yID] > 0 else \"\" for yID, year in enumerate(years)]\n",
    "        ax.set_xticklabels(years_labels, fontsize=10, rotation=90)\n",
    "\n",
    "        step_size = max(1, math.ceil(max_papers / 10))\n",
    "        ax.set_yticks(np.arange(0, max_papers + 1, step=step_size))\n",
    "        ax.set_yticklabels(\n",
    "            [str(int(x)) for x in np.arange(0, max_papers + 1, step=step_size)],\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "        # set y axis label\n",
    "        ax.set_ylabel(\"absolute\", fontsize=10)\n",
    "\n",
    "        plt.bar(\n",
    "            range(len(years)),\n",
    "            yearly_papers,\n",
    "            color=\"black\",\n",
    "            alpha=ALPHA_PAPER_BAR,\n",
    "            label=f\"Total papers ({sum(yearly_papers)})\",\n",
    "            zorder=0,\n",
    "        )\n",
    "\n",
    "        line_count = 0\n",
    "        i = start_index\n",
    "        while line_count < PARTITION_SIZE and i < len(type_instances_sorted):\n",
    "            instance = type_instances_sorted[i]\n",
    "            # yearly_occurrences = type_matrix[:, i]\n",
    "            yearly_occurrences = type_matrix[instance]\n",
    "            i_total_occurrences = yearly_occurrences.sum()\n",
    "            instance_label = instance\n",
    "            if instance_label_dict:\n",
    "                instance_label = instance_label_dict.get(instance, instance)\n",
    "            label = f\"{instance_label} ({i_total_occurrences})\"\n",
    "            values = yearly_occurrences\n",
    "            line = plt.plot(range(len(years)), values, label=label, zorder=3)[0]\n",
    "            line_count += 1\n",
    "            if error_matrix is not None and instance in error_instances:\n",
    "                color = line.get_color()\n",
    "                errors = error_matrix[:, error_instances.index(instance)]\n",
    "                errors_plus = yearly_occurrences + errors\n",
    "                line.set_label(f\"{instance_label} ({i_total_occurrences}-{sum(errors_plus)})\")\n",
    "                # Plot the error as a half transparent line on top of the normal line\n",
    "                plt.plot(\n",
    "                    range(len(years)),\n",
    "                    errors_plus,\n",
    "                    color=color,\n",
    "                    alpha=ALPHA_ERROR_LINE,\n",
    "                    label=f\"{instance_label} (w/o proximity)\",\n",
    "                    zorder=2,\n",
    "                )\n",
    "                line_count += 1\n",
    "                # color in the area between the normal line and the error line\n",
    "                plt.fill_between(\n",
    "                    range(len(years)),\n",
    "                    yearly_occurrences,\n",
    "                    errors_plus,\n",
    "                    color=color,\n",
    "                    alpha=ALPHA_ERROR_ZONE,\n",
    "                    zorder=1,\n",
    "                )\n",
    "            i += 1\n",
    "\n",
    "            # plt.scatter(range(len(years)), errors, color='red', label=f\"{instance} (error)\", zorder=1)\n",
    "        stop_index = i\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        plt.title(\n",
    "            f\"Number of papers covering {type} instances (#{start_index+1} to #{stop_index} of {len(type_instances_sorted)})\"\n",
    "        )\n",
    "\n",
    "        # Inset for relative values\n",
    "        fig.canvas.draw()\n",
    "        x_lim = ax.get_xlim()  # Get the current x-axis limits from the main plot\n",
    "\n",
    "        bbox = ax.get_position()\n",
    "        bb_left, bb_bottom = bbox.x0, bbox.y0\n",
    "        bb_width, bb_height = bbox.width, bbox.height\n",
    "\n",
    "        ax_inset = plt.axes(\n",
    "            [bb_left, 0.05, bb_width, 0.15],\n",
    "            alpha=ALPHA_PAPER_BAR,\n",
    "            facecolor=\"lightgrey\",\n",
    "        )\n",
    "        for i, instance in enumerate(\n",
    "            type_instances_sorted[start_index:stop_index], start=start_index\n",
    "        ):\n",
    "            # yearly_occurrences = type_matrix[:, i]\n",
    "            yearly_occurrences = type_matrix[instance]\n",
    "            values_relative = [\n",
    "                occurrences / papers if papers > 0 else 0\n",
    "                for occurrences, papers in zip(yearly_occurrences, yearly_papers)\n",
    "            ]\n",
    "            instance_label = instance\n",
    "            if instance_label_dict:\n",
    "                instance_label = instance_label_dict.get(instance, instance)\n",
    "            line_relative = ax_inset.plot(\n",
    "                range(len(years)),\n",
    "                values_relative,\n",
    "                label=f\"{instance_label} (relative)\",\n",
    "                zorder=3,\n",
    "            )[0]\n",
    "\n",
    "            # add the error part\n",
    "            if error_matrix is not None and instance in error_instances:\n",
    "                color = line_relative.get_color()\n",
    "                errors = error_matrix[:, error_instances.index(instance)]\n",
    "                errors_plus = yearly_occurrences + errors\n",
    "                errors_relative = [\n",
    "                    error / papers if papers > 0 else 0\n",
    "                    for error, papers in zip(errors_plus, yearly_papers)\n",
    "                ]\n",
    "                if max(errors_relative) > 1:\n",
    "                    print(f\"Error: {instance_label} has a relative error > 1\")\n",
    "                    # throw an exception because this should never be the case:\n",
    "                    # raise Exception(f\"Error: relative {instance} occurrence + error > 1\")\n",
    "\n",
    "                ax_inset.plot(\n",
    "                    range(len(years)),\n",
    "                    errors_relative,\n",
    "                    alpha=ALPHA_ERROR_LINE,\n",
    "                    color=color,\n",
    "                    label=f\"{instance_label} (error, relative)\",\n",
    "                    zorder=2,\n",
    "                )\n",
    "                # color in the area between the normal line and the error line\n",
    "                ax_inset.fill_between(\n",
    "                    range(len(years)),\n",
    "                    values_relative,\n",
    "                    errors_relative,\n",
    "                    alpha=ALPHA_ERROR_ZONE,\n",
    "                    color=color,\n",
    "                    zorder=1,\n",
    "                )\n",
    "\n",
    "        ax_inset.set_xlim(x_lim)\n",
    "\n",
    "        ax_inset.set_xticks([])\n",
    "        ax_inset.set_yticks(np.arange(0, 1.1, step=0.5))\n",
    "        ax_inset.set_yticklabels(\n",
    "            [f\"{int(x*100)}%\" for x in np.arange(0, 1.1, step=0.5)], fontsize=8\n",
    "        )\n",
    "\n",
    "        # set y axis label\n",
    "        ax_inset.set_ylabel(\"relative\", fontsize=10)\n",
    "\n",
    "        plt.subplots_adjust(bottom=0.3)\n",
    "\n",
    "        start_string = f\"{start_index+1}\"\n",
    "        stop_string = f\"{stop_index}\"\n",
    "\n",
    "        # fill up with 0 to have a constant length\n",
    "        start_string = \"0\" * (3 - len(start_string)) + start_string\n",
    "        stop_string = \"0\" * (3 - len(stop_string)) + stop_string\n",
    "\n",
    "        part_appendix = f\"{start_string}_to_{stop_string}\"\n",
    "        filepath = os.path.join(path, name)\n",
    "        plt.savefig(f\"{filepath}_{type.replace(' ', '_')}_{part_appendix}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        start_index = stop_index\n",
    "        if start_index < len(type_instances_sorted):\n",
    "            # if recursion_depth > 0:\n",
    "            #     break\n",
    "            visualize_timeline(\n",
    "                config,\n",
    "                year_instance_occurrence_matrix,\n",
    "                year_papers,\n",
    "                # instances,\n",
    "                {type: instance_types_dicts[type]},\n",
    "                name,\n",
    "                path=path,\n",
    "                recursion_depth=recursion_depth + 1,\n",
    "                start_index=start_index,\n",
    "                error_matrix=error_matrix,\n",
    "                error_instances=error_instances,\n",
    "                instance_label_dict=instance_label_dict,\n",
    "            )\n",
    "        start_index = 0\n",
    "\n",
    "\n",
    "if config.visualize:\n",
    "    # yearly_error_matrix, year_error_papers = create_year_paper_occurrence_matrix(\n",
    "        # papers_metadata, error_matrix, error_papers, is_error_matrix=True\n",
    "    # )\n",
    "    visualize_timeline(\n",
    "        config,\n",
    "        year_instance_occurrence_matrix,\n",
    "        year_papers,\n",
    "        instances_by_class,\n",
    "        # instance_types_dicts,\n",
    "        name=\"year_instance_occurrence_matrix\",\n",
    "        # error_matrix=yearly_error_matrix,\n",
    "        # error_instances=error_instances,\n",
    "        instance_label_dict=instance_label_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "## Pre-Processing\n",
    "Using Completion Rating in %\n",
    "\n",
    "### 80 %: Full Text extraction\n",
    "* lacking noise removal (Headings, page numbers, ...)\n",
    "* lacking line-break mending\n",
    "\n",
    "### 100 %: Bag of Words\n",
    "* The problem with BoW that the words are looked at seperatly and correlation is not really clear.\n",
    "\n",
    "\n",
    "### 99 %: TF-IDF\n",
    "* tf-idf only on terms\n",
    "\n",
    "### ? %: Part Of Speech (POS) Tagging, Named Entity Recognition (NER) \n",
    "* ready, but not used currently\n",
    "\n",
    "## Visualize\n",
    "\n",
    "### 85 % Matrix\n",
    "* CSV and Dataframe dumps work fine\n",
    "* Visualization as PNG or SVG are extremely large.\n",
    "  * DPI regulation works to somewhat keep this in check, but images still reach 20 MB\n",
    "* An interactive matrix would be preferred.\n",
    "  * If you hover on a cell, it shows you the x and y label and it's value.\n",
    "\n",
    "### 100 % Timeline\n",
    "* arrange the papers on a timeline and identify the flow of:\n",
    "  * Processes\n",
    "  * File formats\n",
    "  * software\n",
    "  * ...\n",
    "* Additional ideas:\n",
    "  * Compare this to goolge trends\n",
    "\n",
    "### GraphDB\n",
    "* Visualize\n",
    "\n",
    "## Future Work\n",
    "Using Difficulty ranked (DR) solutions:\n",
    "\n",
    "### Step 0: Look it up\n",
    "\n",
    "#### Wikidata linking & more\n",
    "* https://openrefine.org/\n",
    "\n",
    "#### More visualization\n",
    "* https://github.com/JasonKessler/scattertext \n",
    "* https://pypi.org/project/yellowbrick/\n",
    "\n",
    "#### NLP Pipelines:\n",
    "https://spacy.io/usage/processing-pipelines\n",
    "\n",
    "\n",
    "#### BLAST: Basic Local Alignment Search Tool\n",
    "  * starting point: https://academic.oup.com/bioinformatics/article/39/12/btad716/7450067\n",
    "\n",
    "#### AMIE 3\n",
    "  * https://luisgalarraga.de/docs/amie3.pdf\n",
    "  * https://github.com/dig-team/amie\n",
    "\n",
    "### Step 1: Low hanging fruits\n",
    "\n",
    "#### 1/5 DR: multi-word detection (n-gram)\n",
    "Tools:  nltk, spaCy, etc.\n",
    "\n",
    "### Step 2: Not-to-tricky follow-up\n",
    "\n",
    "#### 3/5 DR: Acronym Expansion\n",
    "Tools: spaCy - https://spacy.io/universe/project/neuralcoref\n",
    "\n",
    "#### 3/5 DR: CoReference resolution\n",
    "Tools: spaCy - https://spacy.io/universe/project/neuralcoref or https://huggingface.co/coref/ (you can use the model out of the box)\n",
    "\n",
    "### Step 3: Vector-magic\n",
    "\n",
    "#### 2-4/5 DR: Word embedding\n",
    "* Find out, that jpeg and png are similar\n",
    "\n",
    "(depending on your needs) - Tools: gensim - https://www.analyticsvidhya.com/blog/2023/07/step-by-step-guide-to-word2vec-with-gensim/\n",
    "\n",
    "#### 3/5 DR: document embedding\n",
    "Tools: gensim - https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
    "\n",
    "I would also check graph embeddings, sentence embeddings, and recently there is LLM2Vec\n",
    "\n",
    "### Step 3.1: Reaping the vector-rewards\n",
    "\n",
    "#### 1/5 DR: clustering\n",
    "Tools: sklearn\n",
    "\n",
    "Requirements: Need to have data as numbers first. This is quite possible after generating embeddings\n",
    "\n",
    "### Step 9: Won't be happening in this paper\n",
    "* Paper classes\n",
    "* Subclasses of paper classes\n",
    "* model which process is a subprocess of another process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
