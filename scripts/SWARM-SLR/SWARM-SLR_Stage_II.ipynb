{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage II\n",
    "This stage collects, analyzes and selects literature on a bibliometric level, without technically ever needing to look into any of them. Using available indexing, metadata, as well as abstract and text extraction, screening is done based on a representation of the document instead of the document itself. This allows to more efficiently assess the literature, reducing the workload to a manageable load by not investing too much effort into later expelled documents. \n",
    "\n",
    "**Workload distribution**\n",
    "\n",
    "|Actor|Time|\n",
    "|:----|:----|\n",
    "|Researcher|15 min|\n",
    "|Machine|30 min|\n",
    "\n",
    "**Tools**\n",
    "* Search Engine (e.g. [Google Scholar](https://scholar.google.de/schhp?#d=gs_asd))\n",
    "* [Zotero](https://www.zotero.org/)\n",
    "* [Python](https://www.python.org/), including [pip](https://pip.pypa.io/en/stable/installation/) for ```pip install bnw-tools```, installing the package [bnw-tools](https://pypi.org/project/bnw-tools/) containing the SWARM-SLR code\n",
    "* Optional:\n",
    "    * Markdown file viewer (e.g. [Obsidian](https://obsidian.md/), preferably with [Dataview](https://github.com/blacksmithgu/obsidian-dataview) plugin)\n",
    "    * [Jupyter](https://jupyter.org/) for using this Jupyter notebook itself (the file you are reading right now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Search\n",
    "|Step|Result|Requirement|\n",
    ":----|:----|:----|\n",
    "|Find resources|preliminary document list|16. use reliable sources<br>17. find relevant documents<br>18. find similar documents|\n",
    "|Remove duplicates|preliminary document set|19. identify duplicate documents|\n",
    "|Find missing documents|curated document set|20. find unindexed documents<br>21. identify document set gaps|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"images/Task 3.jpg\" width=\"90%\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Find resources\n",
    "Requirements:\n",
    "* use reliable sources\n",
    "* find relevant documents\n",
    "* find similar documents\n",
    "\n",
    "1. **Reference manager**: First, a digital library setup is required. While this setup uses [Zotero](https://www.zotero.org/), others like [Citavi](https://www.citavi.com/) are equally valid, though some steps may need to be modified to fit their specific functionality. The following steps are recommended:\n",
    "    1. Install the Zotero application and browser plugin.\n",
    "    2. Sign in to your Zotero account for both.\n",
    "    3. **Zotero library**: Setup a library (/group) for this SWARM-SLR.\n",
    "    4. **Query collections**: For each search query, set up a collection (/folder) within this library.\n",
    "2. **Search engine**: While each SWARM-SLR only uses one *Reference manager*, it can and should use multiple search engines, Google Scholar, Semantic Scholar, OKMaps etc. Proceed as follows:\n",
    "    1. For each search engine:\n",
    "        1. For each query:\n",
    "            1. Create a collection (/folder) within the respective *query collection*. Select this collection with a left-click.\n",
    "            2. Open the search engine and use the query there.\n",
    "            3. Store each resulting page using the zotero plugin. While the results should be between 100 and 500, it is recommended to stop after cataloguing the first ~200 results.\n",
    "                * *Hint*: While you can open multiple tabs and increase the amount of search results per page, neither are recommended. Many search engines use temporary timeouts to sanction highly frequent access.\n",
    "            4. Then export the *query collection* as a BibTeX, for later reference.\n",
    "\n",
    "The reliability of sources is generally given for most dedicated scholarly search engines, with only minor curation required later on. While this step collects the majority of relevant documents will be collected during this step, there are various means through which the document set can be expanded later on. Examples include:\n",
    "* A highly relevant document is found and is used to query **Connected Papers**. This results in a recommendation of additional relevant papers, which can be downloaded as BibTeX and added via Zotero's \"Import from BibTeX\" functionality.\n",
    "* A (large) research project relevant to a research question is found and has it's publications catalogued. These results can be either added through BibTeX (see above), or looked up individually and added manually.\n",
    "\n",
    "Within the *Zotero library*, a new collection should be created called \"others\". Within that, each of these examples (e.g. Connected Papers dump / research project publications / ...) is stored as it's own collection. Like the *query collection*, the BibTeX representation of these collections should be exported and stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Remove duplicates\n",
    "**Requirements:**\n",
    "* identify duplicate documents\n",
    "\n",
    "*Hint*: Before any duplicates are removed, it is important to make sure all collections have their BibTeX stored. If not, it is impossible to reliably trace back the origin of a document once duplicates are removed.\n",
    "\n",
    "Most duplicates can be removed by using Zotero's \"Duplicate items\" section next to the collections. Each item there can be merged with the click of a button, removing the duplicates.\n",
    "\n",
    "Other duplicates may only be found later on, for example when calculating document similarity. This step is primarily to reduce future overhead processing the same file twice, while it presumably can't fully prevent this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Find missing documents\n",
    "Requirements:\n",
    "* find unindexed documents\n",
    "* identify document set gaps\n",
    "\n",
    "Once all queries are processed and all apparent duplicates removed, the last step of task 3 includes three phases:\n",
    "1. **Find documents**: First, using Zotero's \"Find Available PDFs\" feature, all PDFs of the library are downloaded. Select the library, all references therin and find this feature by right-clicking any of them. After this is processed, presumably not all PDFs are found. By filtering the library by attatchments, each reference without PDF can be inspected. The research can attempt to find missing PDFs of promising references through other means, e.g. from their (departments) own library, by buying the work, contacting the authors or consulting a librarian.\n",
    "2. **Identify gaps**: If works knwon to the research that are deemed relevant to the research question are not in the Zotero library, they can be added as a dedicated collection within the \"others\" collection. Each document added here should be thuroughly inspected for the following reasons:\n",
    "    * Works of a second (non-english) language are usually not picked up by research engines. This can be circumnvented by translating each query to said other language, which provides extended scope in exchange for significant overhead in data management. While not recommended, this process is not adviced against, and can be conducted if the benefit is deemed worth the investment.\n",
    "    * Potentially, the search queries have a blind spot, either being to exclusive in their keyword combination, or not including an important keyword. This indicates a) an oversight in \"Step 2.4 Refine with related literature\", as well as b) a potential to soft-reset the survey back to Step 2.5. Generally, the collected references are not invalid and can be kept, leaving only the modified and added queries to be re-run.\n",
    "        * *Hint*: Instead of adding this kind of missing keyword ```B``` as ```A OR B```, a new query should be added without the ```A```. While in hindsight the ```A OR B``` variation would have been prefered, re-running the ```A OR B``` query will return already recorded results.\n",
    "    \n",
    "    Similarly, gaps can be identified without the right documents to fill them. If such a gap is identified *via known missing keywords*, proceed with a soft reset as described above. If not, this blind spot might be a finding of the SWARM-SLR itself, noting not a gap in the search, but in the literature itself.\n",
    "3. **Export library**: Concluding task 3, the Zotero library should be exported as BibTeX, this time with \"Export Files\" checked.\n",
    "\n",
    "*Note*: Take note that this survey methodology can only process documents that have a PDF available. If the PDF is behind a paywall or otherwise unaccessible, the work will not be regarded beyond this point. This, amongst others, further supports the relevance of Open Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Select\n",
    "\n",
    "|Step|Result|Requirement|\n",
    ":----|:----|:----|\n",
    "|Extract structured data from documents|metadata|22. extract publication date<br>23. extract author(s)<br>24. extract publication venue<br>25. extract specified keywords|\n",
    "| |content|26. extract document text<br>27. differentiate chapters|\n",
    "| |bag of words|28. remove special character<br>29. identify multi-word expressions<br>30. expand acronyms<br>31. lemmatise words<br>32. normalise words<br>33. remove stopwords|\n",
    "| |contribution statements|34. identify statements|\n",
    "|Calculate relational meassurements within the document set|document representation|35. calculate \"term frequency\" (tf) and \"term frequency - inverse document frequency\" (tf-idf)<br>36. calculate wordembedding and document embedding<br>37. represent document machine-readable|\n",
    "| |similarity of documents within the document set|38. consider synonyms<br>39. consider polysems<br>40. calculate document similarity|\n",
    "|Identify documents relevant for the research questions|relevance of documents for research question|41. represent research question machine-readable<br>42. calculate document relevancy for research question|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"images/Task 4.jpg\" width=\"90%\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this task is done largely computationally, the following graphic adds insight into each step and their interaction. \n",
    "<div align=\"center\">\n",
    "<img src=\"images/Task 4 - Details.jpg\" width=\"100%\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Select\n",
    "# Setup\n",
    "from bnw_tools.extract.nlp import util_nlp\n",
    "from bnw_tools.publish import util_wordcloud\n",
    "from bnw_tools.publish.Obsidian import nlped_whispered_folder\n",
    "\n",
    "folder_path = \"D:/workspace/Zotero/SE2A-B4-2\"\n",
    "language = \"en\"\n",
    "nlptools = util_nlp.NLPTools()\n",
    "\n",
    "# Analyze Step (4.1, 4.2 und 4.3)\n",
    "folder = util_nlp.Folder(folder_path, nlptools=nlptools, language=language)\n",
    "\n",
    "# Publish (for better usability)\n",
    "util_wordcloud.folder(folder)\n",
    "nlped_whispered_folder.folder(folder, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1 Extract structures data form documents.\n",
    "**Requirements:**\n",
    "* metadata\n",
    "    * extract publication date\n",
    "    * extract author(s)\n",
    "    * extract publication venue\n",
    "    * extract specified keywords\n",
    "* content\n",
    "    * extract document text\n",
    "    * differentiate chapters\n",
    "* bag of words\n",
    "    * remove special character\n",
    "    * identify multi-wprds\n",
    "    * expand acronyms\n",
    "    * lemmatise words\n",
    "    * normalise words\n",
    "    * remove stopwords\n",
    "* contribution statements\n",
    "    * identify statements\n",
    "\n",
    "This step is broken down into extracting various structured data from documents. With this step, we begin to utilize Python, requiring setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata\n",
    "Given the BibTeX export, most of the matadata is already structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for metadata extracted from Zotero's BibTeX export:\n",
    "folder.media_resources[0].pdf.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content\n",
    "Working on a purely textual basis, this pipeline extracts all text from the PDF. This has a multitude of shortcomings, including among others:\n",
    "* PDFs that either not allow, actively hinder or somehow are incompatible with automatic mining are excluded from this approach.\n",
    "* Previously formated text looses context, such as tables, image captions, page numbers, headers and footers, etc.\n",
    "* Overlapping text, either visible oder invisible, may grain the mined text.\n",
    "\n",
    "Despite these errors, this method continues to work with this text-extraction-basis, due the nature of prior publications being of PDF-only nature at best and (scanned) print versions at worst. If more tools like [ORKG](https://orkg.org/) and [SciKGTeX](https://github.com/Christof93/SciKGTeX) are used in scientific practice, this and other step would benefit greatly.\n",
    "\n",
    "In the meantime, we proceed as follows:\n",
    "1. For each PDF\n",
    "    1. **Extract text**: If no available PDF reader can extract text from a given PDF, this step is skipped for said document. It will not be further analyzed computationally and awaits manual evaluation later on.\n",
    "    2. **Clean text**: Due to the nature of PDF, a multitude of errors can be *attempted* to clean up post extraction. These attempts are differentiated between ```always correct [++]```, ```potentially wrong [+-]``` and  ```highly situational [--]```. They include:\n",
    "        * [++] Replace known error characters: ```ﬄ``` -> ```ffl```\n",
    "        * [++] Removing line-breaks inside words: ```know-\\n ledge``` -> ```know-ledge```\n",
    "        * [+-] Removing hyphenation inside words: ```know-ledge``` -> ```knowledge```\n",
    "        * [+-] Remove PDF authors, institutions, etc.\n",
    "        * [+-] Remove headers and footers\n",
    "        * [+-] Re-position captions\n",
    "        * [--] Remove / re-position footnotes\n",
    "        * [--] Remove / re-position table content\n",
    "    3. **Structure text**: Attempt to differentiate between:\n",
    "        * Abstract\n",
    "        * Chapters / sections\n",
    "        * References\n",
    "\n",
    "Each extracted and potentially structured text is stored for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text is read from the pdf, with minimal invasive cleanup.\n",
    "# Extract text:     Completely implemented. Mostly reliable, though some files are still unreadable.\n",
    "# Clean text:       Work in progress, only known error characters implemented.\n",
    "# Structure text:   Work in progress.\n",
    "\n",
    "import inspect\n",
    "from bnw_tools.extract.util_pdf import PDFDocument\n",
    "\n",
    "print(inspect.getsource(PDFDocument.from_file))\n",
    "print(inspect.getsource(PDFDocument.clean_text))\n",
    "\n",
    "folder.media_resources[0].pdf.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of words\n",
    "Before the plain text can be converted to a bag of words (BoW), some sub-steps are required: \n",
    "* **remove special characters**: For the purpose of the BoW, special characters such as ```!\"§$%&/()=?``` are not required and are removed.\n",
    "* **identify multi-word expressions**: Words that semantically form a single unit, such as ```New York```, should be treated as such. \n",
    "* **expand acronyms**: Acronyms such as ```BoW``` and ```Bag of Words``` should not both be stored, hence each acroynme is expanded to its long form.\n",
    "* **lemmatise words**: Instance like ```New Yorks``` and ```instances``` is reduced to it's non-flexed lemma ```New York``` and ```instance```. \n",
    "* **normalise words**: Terms like ```U.S.A.``` and ```anti-discriminatory``` are assigned to their *equivalence classes* and hence match with other words like them, such as ```USA``` and ```antidiscriminatory```. Note that terms such as ```Windows``` might appear in different *equivalence classes*, where a search for ```window``` might return instances of ```Windows```, but not the other way around: (```window```, ```windows```, ```Window```, ```Windows```, ); (```Windows```).\n",
    "* **remove stopwords**: Grammatical words that carry no distinct meaning are removed, such as ```such```, ```as``` and ```and```.\n",
    "\n",
    "Each of these processes should be handled with care, since potentially important information can be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "# remove special characters:        completely implemented.\n",
    "# identify multi-word expressions:  work in progress.\n",
    "# expand acronyms:                  work in progress.\n",
    "# lemmatise words**:                completely implemented\n",
    "# normalise words**:                work in progress.\n",
    "# remove stopwords**:               completely implemented, continues to be improved.\n",
    "folder.media_resources[0].nlp_analysis.bag_of_words.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### contribution statements\n",
    "The representation of a document as statements is among the potentially most significant forms of knowledge representations. Unless pre-existing manual curation either through the authors usage of [SciKGTeX](https://github.com/Christof93/SciKGTeX), or by a fellow researchers [ORKG](https://orkg.org/) usage, potentially in a prior SWARM-SLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contribution\n",
    "# A module is in development, but due to the complexity not ready for implementation yet.\n",
    "# -> The current iteration of the SWARM-SLR includes no statement extraction module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Evaluate\n",
    "|Step|Result|Requirement|\n",
    ":----|:----|:----|\n",
    "|Remove out-of-scope document subset|in-scope document set|43. define in-/exclusion criteria<br>44. exclude documents|\n",
    "|Evaluate relevancy measurement|selection approval|45. evaluate similarity of documents<br>46. evaluate relevance of documents|\n",
    "|Classify document subsets|literature subsets|47. define thresholds<br>48. divide document set into subsets|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove out-of-scope document subset\n",
    "**Requirements:**\n",
    "* define in-/exclusion criteria\n",
    "* exclude documents\n",
    "\n",
    "While formally each exclusion criteria could be rephrased as an inclusion criteria, their nature is different:\n",
    "* **Inclusion criteria** are defined *a priori* and designate which documents *can* be significant to answer the research question. They are derived from the research question scope. They aim to increase feasability.\n",
    "    * e.g. publicated between YYYY and XXXX\n",
    "* **Exclusion criteria** are defined *a posteriori* and designate which documents *will not* be evaluated to anser the research question. They narrow down the research question's scope. They aim to increase quality.\n",
    "    * e.g. without a scientific evaluation\n",
    "\n",
    "The exclusion can be semi-automated, while certain criteria will most likely take effect during the later manual review of remaining papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is currently no automatic support for additional criteria.\n",
    "# The available information is curated in a final note, however, making the apllication of additional criteria easier.\n",
    "# Eventually, certain reliably automatable criteria, such as date ranges, can be implemented.\n",
    "folder.media_resources[0].pdf.metadata['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate relevancy measurement\n",
    "**Requirements:**\n",
    "* evaluate similarity of documents\n",
    "* evaluate relevance of documents\n",
    "\n",
    "Works can be pre-evaluated according to their allignment with the research question via the keywords. This means that the researcher can begin with works that are statistically more likely to be relevant to the research questions, and can gradually work down until all research questions are satisfied. Due to each RQ having their own set of keywords, the calculation of significance can be adjusted throughout the survey process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from bnw_tools.extract.nlp.util_nlp import Folder\n",
    "\n",
    "# Similarity of documents is simple tf-idf cosine similarity.\n",
    "print(inspect.getsource(Folder.calc_sim_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document relevancy\n",
    "Document relevance however is calculated using the term-frequency in term-based normalized keyword weighed cosine similarity:\n",
    "* Reduce tf vector to keywords -> keyword-frequency-vector $kf$\n",
    "  * sum tf scores of words fitting to the keyword (e.g. \"software\" -> \"software\", \"software-engineer\", ...)\n",
    "* Normalize $kf$ keyword by keyword -> normalized keyword-frequency-vector $kf_{n}$\n",
    "* Raise scores by using either root or log method to reduce the impact of keyword stuffing, but still reward higher frequencies.  -> raised normalized keyword-frequency-vector $kf_{nr}$\n",
    "  * SWARM-SLR currently uses the square root (green continued line) to keep the influence simple and intuitive.\n",
    "* calculate weighed cosine similarity with $kf_{nr}$ ($kf_{nr,i} \\in [0,1]$), the keyword vector ($k_{i} \\in \\{0,1\\}$) and the weight vector ($w_{i} \\in [0,1]$)\n",
    "\n",
    "As a result, the usage of the most different, highest weighed and most frequent keywords is rewarded with the highest grade.\n",
    "\n",
    "*Hint*: Without raising the score, a document mentioning a keyword 100 times would potentially be 100 times more relevant than another document of similar length that mentions the keyword only once. An example with square root:\n",
    "* A: 100 keywords in 10.000 words  -> $kf = 0.01$    ->  $kf_{n} = 1.0$  ->  $kf_{nr} = 1.0$ \n",
    "* B: 1 keyword in 10.000 words    -> $kf = 0.0001$  ->  $kf_{n} = 0.01$ ->  $kf_{nr} = 0.1$ \n",
    "* C: 10 keywords in 10.000.000 words -> $kf = 0.000001$  ->  $kf_{n} = 0.0001$ ->  $kf_{nr} = 0.01$ \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/root log plot.png\" width=\"90%\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "<img src=\"images/root.png\" width=\"45%\" />\n",
    "<img src=\"images/log.png\" width=\"45%\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from bnw_tools.extract.nlp.util_nlp import Folder\n",
    "from bnw_tools.review.similarity import compute_weighed_similarity, normalize\n",
    "\n",
    "print(inspect.getsource(Folder.calc_rq_sim))\n",
    "print(inspect.getsource(compute_weighed_similarity))\n",
    "print(inspect.getsource(normalize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(folder.sim_matrix[0])\n",
    "folder.rq_sim_mat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify document subsets\n",
    "**Requirements:**\n",
    "* define thresholds\n",
    "* divide document set into subsets\n",
    "\n",
    "With the metrics established in this stage, a final classification of the document set is possible. Similar to the in-/exclusion criteria, this classification functions as a soft criteria, specifying the conditions to evaluate a given document. By using certain **thresholds** over the document relevancy, the document set is divided into subsets that *must (M)*, *should (S)*, *could (C)* and *won't (W)* be considered. Unlike hard criteria, these are guidelines and aim as a decision support to which works to prioritise.\n",
    "\n",
    "Giving an example:\n",
    "* Documents *must* be evaluated if their relevancy score is above 75 % (M=.75), they have been published within the last year or in a Q1/A venue.\n",
    "* Documents *should* be evaluated if their relevancy score is above 50 % (S=.50) or they have been published within the last 3 years.\n",
    "* Documents *could* be evaluated if their relevancy score is above 25 % (C=.25) or below 5 % (W=.05).\n",
    "* Documents *won't* be evaluated if their relevancy score is above 5 % (W=.05).\n",
    "\n",
    "*Note*:\n",
    "* These are *soft* criteria and to be percievved as guidelines.\n",
    "* Relevancy scores of 0 and below the *W* threshold are percieved as errors and move up to *C*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step defines thresholds that inform the next steps. All data required is already present in the folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
